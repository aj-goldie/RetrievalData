I'm so thrilled to have Peter Norwich join us here today. Thank you so, so much for joining. You're not only the director of research at Google, but you also co authored the seminar textbook on AI and modern approach, together with Stuart Rutfo. It really inspired an entire generation of AI researchers to pursue the field and then also educated them in promising paths to take. And it's used by over 1500 universities and 135 continent countries. But this continent would be, I think, let's say a little bit more thinking in the long term future when we treat planets as continents. No, but I think even that number is probably outdated because it isn't from this year. So it's probably many more at this point. And you're also the head of computational science of the computational science division at NASA Ames, where you oversaw over 2000 scientists in robotics, software engineering, neuroengineering, collaborative systems research and so forth. And that must have been a really special time. You recently worked with Sebastian Trun to develop an online AI course that was taken by lots of, lots of people all over the world, and you're quite gung ho on the promises of online teaching. You will start with a brief intro on your thinking of AI before we move into more of a guided Q A. And as I said, because I will have a chance to speak to you later tonight, I will think as many of the group questions that we can get into during the virtual conversations, the better. So thank you so, so much for really, really honored to have you here. And, yeah, the stage is yours. Okay. All right. Thanks for having me, Allison, and welcome, everybody. So I'm going to structure my remarks around the history of the textbook, because that's sort of my take on what AI is like. So take you back to ancient history, 1990. I was at Berkeley as a research faculty and doc with some of the other AI faculty, and we all not very happy with the textbooks that were available at the time. And I had grown up on these textbooks, and they were great in their time, but it seemed like AI was changing in at least three ways. So we were moving from focus on logic to probability, from a focus on hand coded knowledge to machine learning, and from a focus on expert systems that tried to duplicate human thinking towards normative or optimizing system that tried to get the best answer no matter what. And there didn't seem to be any book that could do that. So we would gripe about that, and we went out to lunch and nothing happened. Then in 91, I left Berkeley. I went to Sun Microsystems. I thought it was a good time to get to change my top level domain from to. We didn't quite have the word big data back then, but I knew that's what I wanted to do. And I knew it was hard to be successful writing grant proposals to put together a big team, but that industry was happy to give you the resources you wanted. And so that's what I did. A year later, I ran into Stuart Russell at a conference and, oh, you know that textbook we were always talking about doing? You guys must be halfway towards writing it by now. And he says, nah, we never did anything. And I said, well, why don't we do it? And so even though we were in different locations and different organizations, we started working on it. And we got the first edition out in 95, and we got the fourth edition out last year. So I think some of the things have held up well. So we structured the book around a couple of ideas. One is ways of representing the world and reasoning, and I think that's still key to the field. We thought it was particularly important to pay attention to reasoning with uncertainty, and I think that's really right. Sometimes I think it's hard to distinguish what's the line between AI and just regular software engineering, because as I see it, both are trying to do the right thing to make programs that work. And to me, the main difference is, in software engineering, the main enemy is complexity, is that as systems get big, you have to deal with their interactions. And in AI, the main enemy is uncertainty. A typical software engineering program is write the software for a bank. That's hard because there's a million rules. But you know what the right answer is? You know that if you withdraw a certain amount of money from one place, it should end up in the other place, and it shouldn't be off by a penny. So complexity is the hard part there. And in AI it's uncertainty, because the data is never right, the world is always changing and so on. So that was the main part of the book. Machine learning was a main part, although it's become much more important over the years. And then interacting with the environment we thought was really important. And I think that holds true still. In doing the fourth edition, we thought about changing the whole structure of the book to be more machine learning first, rather than to follow through, talking about the environment and talking about the structure of agents and so on. But we decided one, that would be a lot more work to reorganize everything was still plenty of work to do it the way it was, and two other people were doing that. So Kevin Murphy has out now a new comprehensive textbook that takes the stem l first approach. And we figured, let him do that work and we'll stick with our point of view. And people can read both, and they should read both. So the main differences for the fourth edition from the previous editions is obviously deep learning has come a long way. And so we had a lot more to say about that, both in the learning chapters and in the natural language and computer vision chapters. We still define AI as maximizing expected utility for class of problems that require intelligence. And that's a little bit uncertain. But in the first three editions, we said, yeah, okay, we're trying to maximize expected utility, and here's a bunch of cool algorithms that can do that, and let's analyze those algorithms and look at their strengths and weaknesses. And for the fourth edition, we, you know, we're still going to tell you about the algorithms, but that's the easy part. You can just download those from GitHub and let's not worry about them so much. And the hard part really is you have to decide what is it you're going to optimize. And getting that right, it seems like now that's the key question. And then that's a big change in our focus. So all of a sudden, ethics and fairness and privacy and diversity, equity and inclusion and lethal, autonomous weapons and all those become part of AI. And, yeah, they were kind of mentioned in the previous edition. We had, at the very end, there was a chapter on philosophy, and we threw in some ethics in the philosophy chapter, but now that's all changed. And I'm happy because the philosophy chapter used to have things like Searle's chinese room, and I was never very impressed with that argument anyways. And now it's much more meaty, I think, in covering these issues of ethics and fairness. And so that was really, to me, the biggest change of all. And then I think another thing is that the audience has changed. In the previous editions, people who took an AI class that was an elective and a normal CS person didn't take that class. You had to actually choose, yes, I want to take this AI class. And we sort of felt like, well, a lot of our job is to excite some of those people so that they want to go on to grad school in AI and then become professors in AI. And now that's completely changed. So now every computer science major is going to take a class that has either AI or machine learning or data science in the title. And a lot of stem majors in other fields are also going to be taking that class. And our book maybe is appropriate for some of those class, for other classes or other books that are good for that. But the audience has certainly changed and we didn't want to dumb down the math too much. Right. So there's still integral signs and partial differential signs and so on, but we wanted to make it a little bit more accessible to a broader range of students. Part of that is trying to explain it in a way that works for them. And then another part is that there's many more applications and less algorithms. Right? So we did the first edition in 1995. You could have a homework assignment of write an algorithm to implement blah, blah, blah. You can't have that assignment anymore as a homework problem because anybody can search for those terms and in a few seconds they can download something that works. So instead it's much more kind of project based now. So it's not implement this algorithm, but explore this algorithm and apply it to this data and see the strengths and weaknesses for how it works and how it doesn't work. And that's a big change and I think that's a good one. So why don't I stop now and open it up for you guys? Questions? So Google now has AI tool that removes dog barking from the background in an audio stream. Can you tell us more about how that works, whether how to apply it in this certain circumstance? Yeah, I'm just kidding, Peter. My dogs have spent a lot of time barking around the world to various destinations, but actually those algorithms are getting impressive and getting better. And so it's an interesting, I mean, I'll start. And this is an invitation to make me stop asking questions by just asking your own. Oh, well, I already have one. Ok, I'm going to stop immediately. Jazeera, you go first. Hey. Hi, Peter. First off, thanks so much for coming. I really appreciate it. So I've heard of an argument that maybe defines artificial intelligence differently, or intelligence differently than you did, which is that there are these organisms or entities, even if they're not properly living organisms, that exist, and they're trying to maintain some kind of homeostasis or some kind of interaction, let's say, with their environment. And then there emerges this complexity, which we view as intelligence, but that assumes intentionality. And it might be better to just say emerging complexity. I'm wondering what you think about that. Yeah, let's see. I guess that's a good way of describing it, sort of from the outside, but we're trying to think of it more from the inside. Right. So if you want to understand how do we get to where we are and what is the world like, then that's probably a good thing. But if you want to say, I want to solve a problem, I want to remove arcs from the background, then I'm not sure that that definition helps, as know we're focused more practical rather than scientific or philosophical. Philosophical. Okay, next one up we have John Peter, thanks so much for this. I so resonate with your point about moving from algorithms to what to optimize. I'm on a panel to help figure out how we can make our diversity and inclusion more comprehensive and holistic at MIT. And there are so many dimensions. There are the traditional dimensions of diversity, physical and identity dimensions that have been well established, race, gender, and so forth. But there are all the cognitive dimensions and intellectual dimensions, things like concrete versus abstract thinking and short term versus long term time horizons and relationship versus transactional orientations and so forth. And then there are all the still other dimensions, like, I call them Richard Dawkins extended phenotype of household income, hobbies, organizations joined, geographical location and so forth. So anyway, this is a real world application of the challenge that you're talking about in case that could possibly be useful. Yeah, I think that's right. And that's certainly something a lot of people are grappling with now, and different organizations figure out ways to take steps towards it. I've been working with that. And we think of it, this diversity, as three parts. So there's the pipeline of how do you get more people involved? There's the hiring process of how do you reach out and find the right people and evaluate them fairly? And then there's a retention of how do you make a working environment that's going to be supportive of them? And we're able to focus on all three of them. As a big company, we feel like we can do some things to help build the pipeline. Smaller companies probably can't make a significant debt in that, and they have to focus more on the other two. And I like your comment that there are subgroups that are legally protected, and you definitely want to pay attention to that. But I think there's all sorts of differences that are important. And I remember something. So you don't normally think of physicists as being an underrepresented group, but we hire a bunch of those. And I remember being struck by what a great thing that was once when we had a reading group in machine learning and somebody who was presenting a paper and says, and this, here's the points. And then there's this theorem in the paper, and I wasn't quite sure of the proof of how it goes. And the physicist steps up and says, oh, yeah, it's hard for you because you dumb computer scientists have the wrong notation for matrices. And he says, here's how physicists write matrices. And in this notation, the theorem is just. This cancels with this, and then it immediately falls right. So it was just, if you come from a different background, you have a different way of thinking. And some problems are trivial from that way of thinking that are hard from somebody else's way of thinking. So that was an anecdote that reminded me, let's get lots of people from different backgrounds and throw them all together, and we can solve more problems that way. Thank you. The way we do college admissions today, and it's not just MIT, it's everywhere, is so archaic, in a way, because we consider just one candidate at a time, rather than the entire pool of candidates at one time. And if we had the means, through AI, to look at the entire pool, we could better optimize for whatever dimensions of diversity we are looking for, plus ensure that we have a higher quality of students overall. Yeah, I think that's right. I was just reading or actually listening to Conneman's book noise, and he talks about this, it's really hard for people to do these types of comparisons and rank things, and he says the best way to do that is to sort them into groups and then do a versus B comparisons within the groups, and that you can do, whereas if you're asked to, here's an individual applicant, give them a score from one to 100. People are terrible at that, but they can make the comparisons within similar groups. Thank you. Next one up we have Robbie. Hi. I had a question of, how do you think about the interaction, even debuting social diagnostics questions like recommendation algorithms, getting people in the filter bubbles, or. Yeah, so that's certainly a problem. You're breaking up there a little bit. I'm not sure I got everything but these issues of filter bubbles, so I think that's really important for me. At Google, I'd focus more on the search side, and we kind of feel like we're getting a bad rap from our analysis. There's not that much going on with filter bubbles, that most people get the same results for most things. And there's been research that says people who are doing the research online more actually get a wider variety of views than people who look at traditional news media and just get one of them. So we think we're actually doing pretty well there in other areas. So at Google, something like YouTube or in Facebook or Instagram and so on there, I think there's more of this potential to get sidelined into one silo. And that is a big issue and we want to deal with that. You see, it's been tough that companies are taking some stands in terms of saying, here's the content I'm going to show you or not show you, and they get criticized no matter what choices they make. Any choice you make, somebody's going to complain that you either did or didn't give the answer here. I also say I think our job is a little bit easier in Google search, right. So if we think something is questionable but not outright illegal or should be banned, we can just put it on page five of the search results, right. So we don't have to make a binary decision of is this banned or not? We can just say we're going to put it down there. And if you really want to find it, you can find it, but if you don't, you're not going to find it. And I think somebody like Facebook has a much harder choice. If I say I want to forward this message to my friend, it's a much tougher choice to not do that forwarding when they've asked to send that message. And there you have to have stricter criteria for what you're going to ban rather than just a softer criteria pushing down the stuff that you think is not as good. So it's a tough problem. Next one up, we have Rosie. Hi, Vita. Great to meet. So your co author, Stuart Russell. Nowadays, I think, focuses a lot on AI safety and the risks of increasingly capable AI systems. But from my understanding, he didn't necessarily start out with those concerns. They kind of evolved as he learned more and more about the topic. So I'm curious, if you went through a similar journey, how was your thinking evolved over time and specifically as it relates to the risks of AI and any safety concerns, what you think of as top priority issues there? Yeah. So I'm not really in the same place he is. I do think that AI safety is an important field, and I think neither of us is really that worried about robots taking over the world. And that's the science fiction trope that you always see. I guess I'm more worried about unintentional effects. And that's why I think it's great that people are investing in AI safety now. And I wish 100 years ago when the internal combustion engine was being created, people said oh, this is great. Look at all the cool stuff we're going to be able to do. And I wish there have been more people saying, yeah, but let's worry about the unintended side effects. And so I think it's good that an AI, we're starting to do that now, and I think we probably got it about right in terms of the amount of emphasis we have on being safe. I think there are a lot of things to worry about. So I'm more worried about sort of surveillance and totalitarian governments imposing strictness on their citizens in a way that's cheaper and more effective than having to do it with hedgemen. So that's something. Stewart's definitely very much involved in lethal autonomous weapons. I'm more worried about the lethal weapon part than the autonomous. You know, I figure if I'm a peasant in Pakistan and a missile is coming towards me, I don't really care if there was a pilot in that plane or if there was a remote operator in Kansas or if it was completely autonomous, I might more care that the missile is coming rather than who ordered it. And are there in particular to those risks that you're worried about more like the surveillance part? Is there anything that we can do with research in AI almost to halt those risks? Are there positive research areas that we could really develop and push more in to help combat some of those risks? Yeah, I think it's going to be primarily a social solution rather than a technical one. But the technical parts are important. Right. So you got things like there's research into how to create deep fakes, and then there's research into how to detect them. And I think a lot of these things that will be ongoing back and forth battles like that. And I've been involved in some of those battles and things like spam detection. And so getting a piece of spam isn't maybe quite as bad as some of these other effects that we're worried about, but it's an ongoing battle, and there's the good guys and the bad guys battling it out. And I think there'll have to be more regulation. I'm not sure exactly where that's going to come from. Some will come from laws, some will come from self regulation, some might come from third party certification. Right. So again, you go back deep into history, and when electrification happened, people were worried about this new technology, that it was going to kill them. And so underwriters laboratory came along and said, we're going to certify these devices as safe. And that wasn't a governmental entity, but it was independent of the companies that were producing these devices. And so maybe we'll end up with things like that in those kinds of certifications. Okay, great. We'll take perhaps, like, one more question on the short end, and then we'll move a little bit into further out topics in AI and Vahid basis down. You've had your hand up for a while. I thought Adrian had his hand up before me, but I can go first if he doesn't mind. So, Peter, I have a quick question about the definition of AI. It seems that you've maintained the same stance throughout the different editions, various editions of the textbook. I was wondering if your view on the definition of change over the years, and what do you think about the argument that the definition of computational maximization of expected utility is just too broad and leads to a not so cohesive cluster of topics? Yeah, I think that's a fair criticism. And we have to put down some definition, and this is what we've come up with. I guess, if it's forced to do a definition, I'm happy to do that, but I always feel like fields aren't really defined by their definitions, and fields are defined by social constructs, and different people can make different choices on them. So, you know, you go to some universities, and, you know, one university will have one biology department, and another university will have six different departments. They'll have biological chemistry and chemical biology and neurobiology and so on and so on. And it doesn't really mean they have a fundamentally different view of the fields. It means the politics evolve differently in their university. And how you really tell what's going on in the field is you go into one of the labs, and you look around, and you see what kind of beakers and Bunsen burners they have and what the lab smells like, and that tells you what they are. I think the same is true of AI, that we can make definitions, but what really matters is what are people doing? And there'll be one group of people that are doing robotics, and another set of people that are doing neural nets in a certain way, and so on and so on. And I think what the field really is, is determined by those communities of people and how they interact with each other, and it's not really determined by the definition we write down. Okay, great. Maybe we'll take Adrian as a final question before we move into the longer facing AI problems. And, Adrian, thank you. So, how do you address decentralization in AI, and in particular, in both business, as well as the technical or maybe even more in the business than the technical sense. Thank you. Decentralization. I'm not sure what aspect you're getting at in the sense of distributing technology to individuals and to communities of individuals that are able to control policy locally. Okay. Yeah, so I guess I haven't been that much concerned with the local policy versus global. It's certainly involved in personal control. So one of the things we have definitely been pushing is federated learning, this idea that if you want to do something like speech recognition and everybody's got a different accent, so you want to do better for individuals. But we looked at that and we said there's privacy concerns, and maybe we don't want to hold people's private conversations in our data centers because there's too much risk that would leak. So we said, instead, let's keep people's private conversations on their personal devices. We'll never hold it, but we'll give them some software to run that will analyze how they're talking and try to improve their speech recognition for them. And that's great. And that works individually, but of course it would work much better if we can share that. And so we said, can we figure out how they can share parameters of the model that they've learned without sharing any of the data that went into improving that model? And so that's been a big focus for us. So that's decentralization in terms of getting the full effects of having one person's data help somebody else without having to share any of that data, without having to trust centralized holder of that data. And I think that's been a big change for Google as a company, and then me individually, I wrote this paper on the power of data. We always saw that as an asset. But lately I was saying, well, yeah, data can be an asset, but it can also be liability, and you have to be careful about how you want to use that. And sometimes it's better not to hold something and have it decentralized. I can say I also hear a lot from people who say it's tough to compete with the big companies now, that certain of the models that are being built and the system problems that are being solved and so on require these tens of millions of dollars investment in computing power. And that makes it harder for work to be decentralized, that only a few entities have the power to do that. And I think it's true that there definitely is some of that. Some of the things that you're seeing published could only be done in a few places. On the other hand, there's still lots of other stuff that can be done, and there's lots of interest in pushing things out to the edge and doing computation on less capable, smaller devices. So there's plenty of work left to do. And I also think that in the future, these cloud providers will be trying to do even more to track people to their platforms and sharing what they have, especially with. Thank you. All right, lovely. Now that we have the first batch of burning questions out of the way, perhaps Mark, you'd like to take us a little bit more into book relevant future facing AI topic. Yeah. So, Peter, as you know, in our book, we're looking further forward into the future, to a future in which most of the cognition of our civilization, most of the cognition that's happening in the world is not human, is the descendants of artificial intelligence. This is artificial. And we're tying one hand behind our back with regard to how we try to act. Now in order to set up a world that is good in that situation. The way in which we're tying one hand behind our back is we're saying these are going to be mind architectures that are really incomprehensible to us. So we have to imagine a framework of rules. Imagine a framework of interaction, of cooperation, a constitutional framework, if you will, that's really incredibly neutral among mind architectures and can still serve to enable them to cooperate. And I'm wondering if there's any bounds on the incomprehensibility when you look forward past machine learning, past the dangers of AI, look forward to a world in which we've, let's say, succeeded at avoiding the dangers, and now we're coexisting with genuine cognition that's exceeding our own capabilities and is everywhere. Is there anything systemic that you would expect to bound the incomprehensibility that we should be designing for? Okay, yeah, that's tough questions. I guess one of the ways I look at it is to say, in many ways, we're already there, right? So we're already living in a world where a lot of what influence us is done by superhuman, non human entities, and we call them corporations and governments, and they're pretty incomprehensible, and they have more effect on us than most individuals, but we can't understand them completely, but we can have some understanding and some predictions of what they do. And I think that's also true for humans. We don't really understand each other. We don't really understand ourselves, and yet we muddle through, and psychologists can understand some things on the types of common flaws and reasoning that people have. And we can try to make sense of it with that. I think that will continue. And I think the biggest change will just be in the pace that governments now have big effects on us, but they make changes on the orders of months and years and not in the order of milliseconds. And I think that's what I'm most worried about. And so I would like to see, when you think of constitutions and so on and rules and what are you going to do? I think mostly of what can be sort of a governing effect. And I'm surprised we aren't better at that now. Right. So you see things like there are these flash crashes in the stock market. We could have stopped those. We could have said, rather than having these trades where Wall street firms try to get their computers a few meters closer so that they can save a few nanoseconds in their trades, we could have said, well, we're not going to have any trades faster than the minute level or maybe the hour level, and we're going to put a tax on every trade so that there's less of an advantage to high speed trading. So I think those types of rules just slow everything down would be one of the things that I would concentrate on. Let me just follow up a bit. One of the things we take a lot of inspiration from is that the US constitution was designed two industrial revolutions ago, or more, depending on how you count those. And it was the degree of complexity of corporations and lobby groups and all sorts of organizations, the superhuman dangers that that structure has succeeded at continuing to provide a cooperative framework among superhuman adversaries that are incredibly more complex than anything the founding fathers could have thought to anticipate. And nevertheless, they set up a structure that has continued to serve to some degree. But when they were doing that, they were looking back at the history of computer science, not computer science of political science. Sorry, looking back at the history of political science, they were very, very well informed on that. So they had a lot of things to patterns to know, to worry about, and psychology to project somewhat with regard to the nature of human institutions. I think we're facing a deeper incomprehensibility barrier with regard to the coexisting superhuman cognitions that are going to be the descendants of our current engineering work. So is there, once these things are much less constituted from humans interacting with each other, composing the superintelligences? What can you say about the bounds on the difference in character of the resulting intelligence compared to current superhuman human organizations? Yeah, I think it's still a science. And I was struck by how Al Abelson said that computer science has become a natural science. And by that he know it used to be a mathematical science that we could prove our programs correct. And for our nontrial programs, we never actually proved them, but we sort of knew which direction we would have to go if we wanted to prove them. And we proved some of the properties. And for decades we taught software engineering like that. And he says, now it's not like that anymore. You no longer write a program and think about proving it's correct. Rather, it's like being a biologist for the field manual. And you download some program and the field manual says its behavior is such and such. And then you observe its behavior and you say, oh, well, no, there's unmistakes in behavior. Didn't get it quite right. And then you update your hypotheses on how this thing actually works, and then you try it again and you never bother to prove it correct or actually understand it. You just make guesses about his operation. And I think we're stuck at that level, right? It's so complex, we're never going to be able to get down to a proof again. Rather, we're going to be like naturalists, observing what's going on and forming hypotheses about where that will lead in the future. So I think we should get better about trying to do those kinds of observations and make those kinds of theories. And you make a good point about the founders of the US Constitution. They did a pretty good job. They lasted for 200 years, a lot of changes, and they mostly did okay. We see things like saying, oh, well, maybe the electoral college is becoming more unfair and so on. There's this famous story of Kurt Goodell when he was studying for his citizen said, huh? You know, I've discovered a contradiction that could lead the US into a dictatorship. And Einstein told him, whatever you say, don't tell that to the judge. And then, so that was lost, whatever that loophole was. And we don't know if it was a serious one or not. It's hard to get these things right. I think it's also hard to balance how much change you want to allow. Right? I was talking about prohibiting too much change too fast, but you also want to allow some kind of change. So what can you allow in terms of how fast can you change these things and what sort of guidelines do you want to put in place this idea of tie me to the mass so I won't be tempted to dive in with the sirens, we need these capabilities to tie us down to stop us from doing things that seem good in the moment, but we know our law are bad in the long term. Active. Any follow ups? Okay, then, Peter, I think, had a question that's relevant maybe to this one before. Maybe I'll jump in. Yeah. So that naturalist perspective that you're quoting there, Peter, seems both true in some deep sense, but also rather know. Although we can't get robust proofs of the properties of the future systems or even the present systems that we're working with, it does seem as though it should be possible to get particular systems that have very well studied properties. And even if they're not, like, robustly proven, you can kind of say, well, on the test data set, they did this. And when we threw adversarial examples in the system, did y? And then once you've got an example might be, if you want a system that thinks about the future, you could say that you want question answering over its future plans, and you want those question answers to be truthful or something like that. And so you could imagine building systems like that and then having architectures where we compose some systems that have very well studied properties. And so I'm wondering if there should be kind of architectures like this for future cognition that we should really be intentionally laying groundwork for at the moment and whether you have any thoughts on what those might be. Oh, absolutely. Yeah. So, to me, I like the term trustworthiness. So I think we should try to build trustworthy systems other people have. Like DARPA has this initiative on explainable AI, and I don't like the term explainable because I think it's not quite enough, because anybody can come up with explanations. And every day the stock analysts will give you an explanation for why the stock market went up, but if it gone down, they would have had an explanation, too. So you sort of feel like the explanations aren't that powerful. They're kind of more post hoc. And I think a lot of our own reasoning is like that, that we sort of use our gut to come up with the answer, and then we use our head to come up with an explanation that justifies the answer that we had, but may not actually correspond to the reasoning. So, trustworthiness, I think, ties into the types of things you were talking about, that you want to be able to have a conversation with the system, and you want to be able to pose it scenarios and say what would happen in such and such a case, and why this? And what were you thinking when you did that and so on. And you do want to have some guarantees that it's not lying to you. And that's a little bit tricky, because the real answer is, well, I made this decision because I did these matrix multiplications, and then I compared this summation to that summation, and it was greater. And so that's not a very satisfying explanation. So any explanation that is satisfying is also in some sense lying in that it's not the whole truth, but you want it to be the truth, if not the whole truth, and you want to have some way to verify that what it's telling you is something that you can trust. And then you have these conversations back and forth. It's still not proof. And there will be situations that nobody had thought of where the system will misbehave because it's kind of outside the boundaries of what you expected. And we just want to try to minimize those in terms of building systems that are more robust and building better testing tools to try to limit the untested situations as much as we can. Yeah, I guess I have a follow up that somewhat pertains to this. So it's interesting. I think AI approach is subtitled, or at least sometimes called the intelligent Agent book, because it really takes this premise that intelligence is concerned with rational action, always taking the best possible action in the situation. And that definition of an agent know holds both for humans and AIs. Right. And so Jillian Hadfield, who we had on last time, was also making some comparison between. Okay, what can we actually learn between human principle and agent alignment for AI? Agent alignment. And so I'm wondering if you have more good parallels in which we can maybe already learn from human action. She was mostly talking in terms of contracting and how we fill in, often with our social context for the things that are still left unset in contracts. So do you have a hunch of how to go best about doing this for artificial agents? Yeah. So, getting back to definitions of AI, we surveyed past definitions, and we came up with this two by two matrix saying past definitions either focus on imitating humans or just optimizing results. So that's one dimension. And then the other dimension is, are you looking at valid reasoning processes, or are you looking at the decisions that come out of those reasoning processes? And so we came down in the quadrant of optimizing the decisions, but other people are in other quadrants, and that all makes sense. I think so. I like that way of looking at it. I think it's interesting that when you look at our societal and sort of legal type issues, we kind of cut across different ways of looking at things. Right? So most of our laws are focused on outcomes, but some of them are focused on intent. Right? So murder is worse than attempted murder, whereas if you're only focusing on the actions there, the intent of the murderer, they should be the same because they're both equally bad in terms of their decision. And in most cases, we only punish you for what actually happened, not for your intent. But we do punish you for drunk driving, even if you drove perfectly. So I think society as a whole has different paradigms for looking at these types of issues, and we combine them together in funny ways to make it through. Yeah, I think in one of the Lex Friedman interviews and Lucy said that I think one of our challenges for the future is to describe to our markets and to our high tech products what it is that we really want. In AI, there's a common goal of maximizing expected utility. We spend decades on the expected and maximizing parts, but very little but taking utility as a given. And until the public has the power to say what it really wants, the markets would choose poorly. So here, I guess in our book, we take a little bit more of a questioning approach to the concept of utility, especially as aggregated over multiple agents. And so I'd be really curious to see, because you were just pointing out that as a society, we would have time things, perhaps not really inconsistently, but in funny ways. So can there be something that the public as a whole really wants? And what would it mean to figure this out? And then how could we possibly communicate this to agents that may be very different to us? Yeah, so there's certainly lots of criticisms of maximizing utility approach, and some of these gantian approaches try to get at that. I think a lot of the criticisms ignore externalities, and that if you take them into account, that you get a better approach for maximizing utilities. So one of the famous examples is, well, what if it's determined that I have a certain blood type so that my organs could save five people from dying, and so therefore I should be killed and my organs harvest and give it to these five people because saving five is better than saving one? If you find that reprehensible, then you should not believe in Max. By the. My response to that is, well, the difference between these two outcomes is one, whether I get cut up or not. And yeah, that's kind of important to me. But the bigger difference is, does everybody have to live in a society where they're constantly under the threat of being cut up? And once you take that into account of the utility, then it makes sense to say, well, I can maximize expected utility by not having everybody live under that threat, and therefore it's okay under utility maximization approach to not cut up people like that. I think a lot of the paradoxes are that kind where if you take a broader view, it makes sense. Well, I think that, on the one hand, one could say this is rule utilitarianism, as we want to live under a rule in which this is true. And I think even there, it's sometimes really difficult whether in how far rule utilitarianism, how that relates to just the types of rules that evolve by us playing evolutionarily over many, many, iterated over many, many iterations with each other. But yeah, I think this is definitely more on the philosophical angle. I think we had a question, I'm watching the time, because we'll have much time to chat tonight, right. In a more structured manner. But we had one from Ravi here, which was on economics, and then bouncing off by Rosie. Ravi, if you want to go for it. Yeah, I mean, my question was basically the long run economical. Do you see AI as a complement rather than a substitute? And that's one of the things that I think was wrong with the expert systems approach of the trying to say we're going to replace a person. Rather. I think AI should be a tool that helps people get their jobs done. And sometimes that tool can operate completely autonomously, or all the time, or most of the time. But we should think of building systems where the humans are in charge and the AI is helping them achieve their goals the best they can. Okay. Rosie had a follow up that was taking a little bit more concrete, if you'd like to go. Yeah. I was specifically wondering, Peter, if you've had a chance to look into some of the recent advances in sort of automating aspects of writing code. So GitHub, Copilot and open air codex API, and just generally what your thoughts are on those sorts of advances, both on the labor market also just in general, like what the implications are for technological development. Yeah, so I think that's really interesting. I've been following it. I haven't played with it that much. I was kind of interested in this as a research area, probably about five years ago, and maybe it was just a little bit too early. I guess I'm a little bit surprised at the way these systems have come out, that at sort of just how little knowledge they're incorporated. Right. So they're really just looking at strings of tokens and then they're able to be pretty successful using that. And when I was thinking about it years ago, I was also saying, well let's try to look at, unlike with human language processing, we actually know the grammar of all these programming languages, so maybe shouldn't we take that into account? And they aren't, or they're only doing that in a latent way, which is maybe sort of a really cool hack that five years ago it definitely didn't work, but now it does, maybe just because you have enough code. And so people are looking at these issues and there certainly are problems of since it doesn't understand what it's doing, it's going to make some errors. People also worried about copyright issues and who owns this generated code and so on. So I think it's really interesting. I think we can do a lot more and I think we'll see a change pretty rapidly in terms of what someone's day to day life as a programmer is going to be. And I think we've already seen that and I notice it in working with my younger colleagues. I feel like a dinosaur sometimes, right? So we'll say, oh, we're trying to do something and here's this new package that helps. And I'll sit down and I'll start reading the documentation and 3 hours later I feel like I sort of have a pretty good understanding, I'm ready to get started and my younger colleague will come back and say, I'm done, I implemented it, it works, let's move on to the next thing. And I said, but I have all these questions, how does this work? How does that work, how does this other thing work? And they said I have no idea, but I got the answer so I'm not going to care. And sometimes my approach of saying maybe you really should understand things before you say you're finished, sometimes I'm right, but sometimes they're right. And it was a waste of my time to try to understand something when I could have just gotten the answer and been done with it and been on to something else. And I have to try to train myself to say that these are two different modes of operating and figure out when it's right to do one or the other. And that's been hard for me, to give up that level of control. Well I think you wrote a fantastic essay on this, of teaching yourself, I think, how to program in over ten years, arguing for a small, I think, understanding approach. But I think just in the interest of time, with 1 minute left we had Chukchiang Hu I think who's the way in from China, if that's correct, and with another final question, and then we'll get into more of the deeper discussions tonight. Hi Peter, thank you for your talk. And I have two questions. One is how can we exchange more effectively the ideas and results among many disciplines such as philosophy, logic, mathematics, economics, et cetera, to shape a better version of AI area. And question two is aligned AI possible from your perspective? Yeah, I think that's a great question. And we've certainly seen examples of people in AI rediscovering things that were already known. Right. So operations research had a lot of these approaches already figured out. Nobody in AI had read any of that. And then they started publishing them. And then finally somebody said, hey, this work has already been done, let's use what somebody else has. So I think we need to do a better job of know. Maybe we can have some automated tools to. There's been some work to know this literature intersects with that literature. Here's something else you should read. So I hope we can do that. I also wanted to jump in there and say in the comments, John Chisholm had this point about common law. I think that's really important. And I think part of it is it's hard to be a programmer, it's hard to write things down formally, and it's hard to write down laws formally and get them right. And maybe it's easier to write down prototypes to say not, here's the exact limits on what's legal and not, but here's one example of something that's good, and here's another example of something that's bad. So that's what we do in machine learning rather than in regular programming. And maybe the law, in our ways of understanding and dealing with technology should be driven more by these prototype examples rather than by trying to write down a law and get it exactly right. So I agree with that completely. Well, we're getting back into the point on constitutions made earlier and I think explaining how that, or exploring how that legal interacts with the AI space I think will definitely be, I think, an interesting feedback as well over the next few years. Okay, we're now 1 minute overtime. I want to be really mindful of your time, especially because you're gracious enough to grant us more of that later for an in person after show meeting. Thank you so, so much for everyone who joined virtually. We tried to get as many questions from all of you in as possible. And tonight we have a little bit more of a focused discussion with a fireside chat with Peter and a fireside chat with Mark Miller and team tribble from algorithm. I look forward to seeing a few of you there. For those of you who filled out the type form. And I got back to you. I can't wait to see you later. And thank you so, so much for taking time to join us, virtually. It was really be much appreciated. Which took a crazy walk through a bunch of different areas near and short, and hoping to focus in on a few of those tonight. Okay, thank you, everyone. I'll see many of you tonight or at the next one, virtually. Bye.