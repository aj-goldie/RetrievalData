We typically have like a list of projects, right? So when you download Hop, you get two projects by default. You get the samples project and a default project. That is not a bad thing. So you get started. The downside of that is let's start with let's suppose you install hop in uphop which is in my case a symbolic lane. So you get like a default project in your config folder. And this is what Hop uses. It uses this hop config JSON file and this is the default file, right? So it has the default project here which lives in the relative folder, project, default and sample. So this is where you will work if you just accept default settings of hop. Now obviously suppose that you want to upgrade hop. You download a new version, one, one, two, 2.0, whatever, you start from scratch again. So that's not a great way of doing things, right? So in my case, I'm not using these files. I've set a couple of hop variables and the first one to take note of is the hop config folder. And in that config folder we store the configuration file. So another hop config. So if I go to hop mat PMP config hopconfig JSON, this is where all my environment variables are stored. This is the default locale selection, some GUI properties by default credentials for Google Drive. So all the plugins have access to storing information that they find relevant, like the Google Drive VFS driver. The projects setup is also a plugin so they can write to this one as well. And as you can see, here are all the plugins. Here are all the projects that I defined in the GUI, right? So this is that list and what that is, is there's no real data in here. It's just pointers to the folders. So this is how that gets bootstrapped. So if you look at a project called hop Two for example, this is one of my many test fiddle projects, I guess. So this is the content of that. It inherits from another project base project where a lot of the central metadata is stored that I use. This is like default, right? And the configuration where this information is stored is called project config JSON. So typically you just concatenate these two things and that gives you the configuration file of the project. There are no variables defined. We usually keep these things like pretty standard. That is a first best practice. Move your hop config file somewhere where you feel it might be more resilient to change. Maybe you can check in this file into a Git repository or something like that. Maybe you can back it up during your regular backups and then you can exclude the whole binary of hop because that's never going to be relevant. You can just download another copy from the web. You don't have to back that up. And if you start messing with the install that's different, right? So that's the first thing. You might have also noticed that when I change one project to another, that Hopgui remembers where things left off the open tabs. And even if you change the zoom or something, it will remember, right? So hop two and go back. So the zoom is remember, and I think even the scroll bars or whatever, that information is stored somewhere as well. So then we can look at the second folder we can configure, and that's the audit folder. So in my case, hop audit folder. And the first one was hop config folder. And this is all documented on the website, but we thought it might be useful to just line it up. So in here you will find all the projects that I ever used and hop two, and you will find lists of files, things it remembers, and it's perfectly safe to just delete that information on a regular basis, clean it up, nothing bad is going to happen. This is just usage advice, right? The most recent files, all those sorts of things, the stuff that you search for in the past so that you don't have to retype it. That's where this bucket is being used for. All right, if you're setting environment variables and you're using GDBC drivers, you can also set a shared GDBC driver folder so that you don't have to copy like Oracle or special drivers or whatever that you need into the binary of hop. Again, it's easier that way. Then you don't have to worry about copying drivers all over the place. Hop will go pick it up in that folder and that makes it again easier to just upgrade the binaries. So those three, a third one that you can actually quite easily see in the hop scripts. So let's say hop run, for example. So all of them have hop options, all of them check hop options. So it's one option. Like hop options can be set for all scripts if you want. Usually for me, in this case, two gigs is more than enough until my new laptop arrives in a few weeks. I will not be changing that. Maybe after that I will up it a bit. But you can set like a whole set of options in the hop options, like define environment variables on the Java virtual machine, add more Java options, whatever, and those can be picked up as well. That's one I might recommend that you set as well for production use cases on a server, for example. But those four variables before you get started. And then I think it's time to look at these projects and let's take a cool one. So this is a big one I've been working on recently. This is a migration of a project from Ketle that I did for my brother in law, actually. And it's a project that went on for more than ten years. And so this has been running for a while now in production on Hop, so we can take a look at best practices. First thing is everything is stored in version control in git. So there's not even a dow. Everything needs to be version controlled. It's easier to revert files, to see changes to files. You can get basically a lot of information. There's visual differentials available, so many options, right? So make use of that. It's so easy to do. Just install git, make a free account on GitHub. They allow private repositories. There's really no question, right? Second thing to realize is that if you have a project, everything can be contained in a single folder. So even for a large project like this, everything is contained in a single folder. Even the scripts, the shell scripts around the server, everything is in a single folder, which makes it easy to just check everything in and version control and check it out on the server. The problem then is like, okay, so I'm developing locally and I need to be able to say I have a bunch of environment variables set for my local environment and these are used throughout the whole project. So I call this the name of whatever and this file is not checked into git, right? So you can also store this in a separate folder with all the environment files. For many projects you might have and check that into a private git repository somewhere else. So that's a good practice as well that you can do so that you never lose that work. So let's take a look at maybe on the environment. So if you have any project, this is a project I just got started with this week. And this is critical vulnerabilities export. This is a critical vulnerabilities file format. It's an XML file format where all software critical vulnerabilities are stored in. And Neo for J is also working with that like any other software vendor. So let's do a quick test. There's a big file with XML in it that we need to parse and load into Neo four J. First thing I do is, okay, so I need a connection to Neo for J, right? So the CVE graph model or you might be doing a new connection to a MySQL database or to Oracle or the relational database. And the first thing you want to do is, oh, I'm going to specify hostname, the database, the port, whatever. Well at that point you should have the reflex of saying I am going to create an environment and in this case I might want to call this local or whatever. You can create as many environments as you want for any given project. So I call this development, you can give it another name and, and I can even create a new file right, for local and by default it takes the parent folder of wherever you're working, right? So now I have a new one and I can say I'm going to create a new one. So this is how easy that is, right? I need a hostname, and let's get local host, maybe in this case the hostname, and so on and so on and so on. So it's very easy to create these environments. It takes all of like two minutes. And then I have an environment for maybe a local new for J database that may be running in a docker container. And then I have another one that is my default one that goes to my Mercury server here, which is sitting a few meters away from me. And if you make any changes, it reloads automatically. And it just makes sure that all the variables that you define are nicely updated, even in the metadata. And then you can just say, okay, I'm testing this. Right? So that makes it very easy to just always just need one database connection or any metadata item only once for your environment. There's never any need anymore. I'm going to test this against another database. I'm going to change the database connection or like the way it was in Kettle. I need to change the Kettle properties file with the variables, and I need to shut down the tool, restart it again. So all of that is not needed anymore. It's become a really powerful tool for managing these sometimes complicated environments. Right? This also means that you have a dynamic server. You fire it up, some address changed, just go into the environment, change it, and once you hit okay, it will ask you to reload the project. And all the variables and all the tabs and all the metadata objects that you have in your GUI will be reloaded automatically for questions. Before we go on, I just want to see whether or not I haven't missed anything glaringly obvious. Barth, do you see anything in the chat or that one is nothing yet. Okay. All right, so what I find particularly annoying is I just did a project where like seven or eight different people worked on for ten years, more than ten years. And so what you get is that the naming scheme of all the workflows and pipelines, if they get migrated from Ketle or the same will happen in Hop, right? So if you start working with a larger project, it becomes excruciating. So what I would recommend is to stick to a naming scheme, and I would recommend people not to use any spaces in their file names, because what happens if you put spaces in these file names internally? They get converted into Uri, URLs, basically, and all the spaces get converted into percentage 20. And those file names then become like percentage 20 in there. And in these tabs, you will also see the percentage 20 because it takes, like, the base file name of a VFS file name, a Uri. So stick to one particular naming scheme that you can come up with and you will see the column has come back into your project this way, right? So it's a very simple thing to do for these maybe say 150 workflows and pipelines, it took me maybe like an hour or so to go through them, rename them, check where the reference was made. If you aren't dealing with any of these migration project or these renames and you want to see where something was used, you can always do something like this, right? Or even have a string like this and just search for it. And with a bit of luck, your favorite tool will just pick that up and say, okay, so the file name here, but if there's more than one location besides just this spot, you can just go back to the find file and just change it there as well. And once you've moved to a single file name scheme that is simple and straightforward, just stick to it, reinforce it regularly, take a look at it, and just take your time, half an hour, an hour to just push it back into the right order. Another thing that I noticed, and this is more around, let me just open that up. I hope you guys can see this. Think there let's say let's call this the Daily Backup, right? So Daily Backup is a workflow that runs. So if you have a script called Daily Backup, make it call a workflow called Daily Backup. And if you put in a file, give it the same name. That makes it easy for DevOps people later on to just figure out what's going on. Or for the simple case, okay, I just want to put my log file somewhere. It's simple. And I am migrating that log file to near for J later on. But not everybody is as lucky to work in a Gui. In this particular case, the server runs through a tunnel somewhere and I don't have access to the GUI, so I have to do everything from the server and there's a hop server involved and there's a lot of moving parts. So make things as simple as possible so that you can recognize the process that they're running, where they are logging to, and which script is executing. Just make your life easy. So this is part of a naming scheme, right? So set this up from the get go, and don't log files all over the place. Don't log in 15 subfolders. Just somewhere simple, right? Another thing that I mentioned in passing is that I will not use any success actions in any of these workflows, nor will I be using any Aborts. So if you have something like this right, this is a design pattern that I see on occasion, feel free to do so. But it doesn't really do anything if this thing is failing. If any of these are failing, the workflow is already set in a failed state. It's already being flagged as an error. There's no need to do an extra board. The only thing where that might make sense is if you have two or three of these items where you might want to send an email or so. But typically it's better to just elevate this workflow to a higher level and then you could just do copy workflow as an action. Let's take a new workflow, this one, and then handle this case by sending a mail, for example, in case something goes wrong. And this obviously is cleaner, fewer errors, keeps the workflows small and tidy. So this is one of the best practice for project setups. Don't put too many workflow items in a single workflow because it makes it harder to see what's going on and it makes it harder to run the separate parts when you're developing. For these actions, we actually ask the user, do you want to rename the action to match the file name? So that is great, right? But if you have, let's say, another one, let's call it this one, typically you would have a transform called Graph output. So this is how it's called originally. But it makes so much more sense to just rename it. Do a quick rename. Again, more like a best practice item. But I feel to see how you can do projects without keeping them maintainable without that sort of best practice. So if you've done all these things that I just mentioned, you're already in the top 95th percentile of people that are doing the writing. So great job. And if not, consider doing these at a half hour. Any questions about this so far, Bart? Crystal clear, I guess. Yeah, it's quite simple, but yeah, the default setup of Hop was done for convenience and for exposing the samples. But the next thing that I want to show you is that by having the ability to set a Hop config folder variable, it also makes it very easy to run whole projects, let's say on a docker container, or for integration testing. So what we've done, for example, the Transformation Integration test. So maybe I can just go there for a second. Bear with me, right? So this is Incubator hub integration tests, transforms. This is the Transforms folder integration test list. The goal of this, just for those people that are not with us, the goal is that these integration tests run every night. And in this case, we have a whole list of transforms that we test every night. And if somebody messes something up or we make a change. Like, Sergio has been really cool in fixing issues, and then he found one in the group by transform. So he added an extra test to the group by workflow. And we can find this one group by I don't know which one, but this is the transform. And then there is the main group by one. There you go. So this is running all the tests so you can choose which test to open. So this is how this is structured. So typically we would say if you run in a docker container, well we have Hop config which allows you to point your project to a certain folder. So we have a script called Hub config. Create project, create environment, set everything right and then you can start running things. In this case it's actually quite a bit easier because we actually have a Hub config file in here. Let me just jump to the bottom here for a second. And in that Hub config file we just have one project. And the project home is the config folder, which is defined by a script that runs and it's a default project and the environment config file in here we can put stuff like, I think for database JSON def environment. Yeah, there's some that have variables for connection details. I think Neo four J might have one. The default ports is just ABCD for the docker container that we briefly start up to run the integration test with. So this makes it very easy to have this whole self contained project. There's no setup or anything. You just point your environment, you just set one variable and everything follows automatically. This is very convenient for these kinds of setups where you just, hey, I just want to point and shoot, I don't want to be developing other stuff. So again, you can consider this to do this in this one self contained project, if all that's ever going to be is a one self contained project. And as you can see, I can still open this project simply by pointing my environment to that folder. The Hop config file is never going to be changed by this project. So on the subject of migrating things and loops in these workflows, so there were a lot of old, or should I say this old design patterns where you would say, okay, so I have ten files I need to loop over, or 20 partners that this company works with or whatever, 20 FTP folders or stuff like that. In those scenarios you typically have something like, I have a bunch of export partners, transport partners, and then you had copy rows to result in the next workflow. And this, I guess after this one you would have the actual loop. It all takes about ten minutes, five minutes to change that, right? You basically replace copy rows to result with a workflow executor. You map the input fields directly to the parameters that you're bound already have if you did the right thing. And that's all it took in my case. So make sure to do that sort of work. It will make things more transparent, easier, and as a bonus, if you want to parallelize, you can do that quite easily then with this approach, right, you can say, okay, I'm going to process four partners at the same time. If there were to be a performance problem with this, but there isn't. I think if most people would do it like this, then they would do themselves a favor. There are a couple of questions now, Matt. All right, cool. Can you show your Git setup? And I think well, if it doesn't show any passwords, I think the question is, do you have any clear text Git passwords in your configuration? No, I don't use git passwords. I have a key that generated, so keygen, I think. Right. An RSA. So this public key is registered with GitHub in this case, so that if I do get status, git fetch upstream master, which is one of the things that you do to get the latest stuff from Git, I'm automatically authenticated using this public key. So this is a very easy setup. It doesn't really take that long, and it's well documented. So no git, passwords, nothing. This works for private and public repositories. Yeah, quite easy to do. I hope that someone answers the question for GitHub. You can do Matt. No, not the key part. I get that. All right. Is it in the same area as the GFS where you set those things up for Hop? So for Hop, so this you mean that it picks up the Git folder? Yeah. Where do you you don't have to do anything. So if Hop finds a Git folder in the project home folder, it will automatically start showing, enabling these menu bars. In the past in Kettle, we had like, a Git project, but that's not needed anymore because we can do this smarter. We can say, oh, there's a Git project Git file with a config folder in there, so we know everything that we need to know. And the libraries read this and the API just starts to work. Yeah. Okay. That's why I missed it, because it just did it automatically. Yeah, we probably should be doing that smarter. In this case, for example, it just checks this folder. Right. I think I just ran a Git in it in here. So it's easy to test. But yeah, that's pretty much for it transform, for example, which is also showing up. So I guess somebody fixed that issue where it didn't search in the parent folders, lo and behold. So that is pretty cool. Right. So if there are any revisions, you probably can see the changes then and stuff like that. But typically we try to look for a Git config folder. Any other questions? Yeah, parent child projects. Great. Question is about deployment. And how is one project aware of how is a child project aware of the parent project? Vice versa. And Enrico says, I found it difficult to use parent child projects with docker parent child projects. So you have, like, base project. Right. So it's quite easy. So if I'm in Project Hop two, and I have to specify that inheriting things from a project means metadata objects. Right. So you have, let's say here a local pipeline run configuration. If you have 20 projects, you don't want to copy these local JSON files all over the place, right? So you create it once and then you inherit from that base project. And if you mouse over the tab, you can see where this is stored. Right. I think there is a Jira case out there to also list this somewhere in the GUI more explicitly. But since we know where the information is coming from, you can edit this and it will update the information in the base project, if it's stored in a base project and we'll store it updated in your current loaded project. So metadata and variables that you might have defined are inherited, but it's mostly for metadata because there are certain things you don't want to recreate over and over and over again. So quite a simple setup. And this is handled internally by the multimedata repository, which you can just add more metadata sources for any given setup. And this works for hop around, for Hop GUI for all scenarios. Quite useful. So we can go to base project. I don't know if I have it even I think this is an old one. So base project, I think it's project default here. So there's a local here and then I can create a new project called what are we going to call it? Stuff hop Hangout and home folder is test stuff Hangout and I am going to the folders hop. Let's inherit from default. Not at this moment. And I can still see these locals. So they're still defined, right? Yeah, go over here and see the same. So sort of like that? That's sort of like the idea. But it's quite simple in structure. Nothing magical about it. You don't have to use it in the end. We're talking about a few JSON files of a few hundred bytes. They're quite small, I guess, right? If you have these metadata objects, what is that? A few hundred bytes? Not that big of a deal. Great question. Thanks. Any others? Not so far. Anyone or anything else unrelated? I do want to add with the because you were also mentioning the docker part in your question, enrico, I believe our default docker image has no support for, well, not out of the box for the parent child project. You register one project when running the image. So there's no way to load multiple well, configure multiple projects on that image and then link them to each other in one go. So you'll have to make a custom image to do that. That would be pretty hard to do, actually, because the multilevel well, it's not just one parent project. You can have multiple levels of parent projects and that would be pretty hard to put into one generic image. That would work for everyone, I guess. So there's a bit of customization that you may need to do. Yes, because also probably in your heat structure, you would split that information off to a separate repository. So you'd have to fetch that repository as well and place it somewhere. So it goes to the realm of custom image making to tailor your needs then. Yeah, unless someone comes up with a magical solution that works, then that would be great. Yeah. So the feedback that you have around these things is always valuable. We don't claim to have a perfect solution, but we do find that the current architecture that we have is resilient to improvement. So we can improve things without breaking stuff. We already did a lot of improvements all across the board, and that includes the GUI as well. So this is 1.1. I think one of the things that we did was, like, if a hop is disabled, you will only see enable and when it's enabled, only disables. So stuff like that. It's a small thing, but yeah, only showing the appropriate actions, smoothing things out across the board. I think it's just something that people will feel if they use the tool regularly and say, oh, well, there's a new version, things improve. That's why you would download a new version. Right. So let us know what your feedback is on any sort of topic, and you will find that the most critical people are we, the developers who are all constantly looking for things to clean up and improve, which is awesome. Community has been really on a roll lately, last couple of weeks. I don't know what it is. Maybe because it's getting darker out outside. Yeah. So this is what I had. I hope it was useful. I don't know if there are any other projects you guys want to show or showcase, but I'm pretty much done. Same question as always, I think. If there are any topics you'd like to see covered in any of the next sessions, let us know here in the chat. Just shout Twitter, matter most, whatever. Just let us know what you'd like to hear us talk about. My colleagues, David, Rodrigo and everybody, thanks for joining. Thanks for testing hop and for helping out. Yeah, welcome. Thank you for the presentation, Matt. It's always a pleasure. We have cool new Neo for J improvements coming in the pipeline for the graph output transform, the dynamic relationship selector, sulfur relationships, and node label selection selection based on input data. Those are hideously complicated, but they're worth building. I have another question. All right. Stefan asks if we can do a deep dive on a docker setup. So I think that's a separate session. Hans was saying that he really wanted to do a session on that, if I recall. I think I heard that too at some point. You heard it as well, Mark. All right. Well, he already did the using hop in docker, so it's just a small step to setting up a docker project because there's always a little bit of confusion around docker and docker hop web or incubator hop and incubator hop web. Right. So those are. The two images that we have. So maybe show the differences, how we can extend those tailored to our own needs. Maybe explain how the integration tests run on docker compose. I think we have, like, a Kubernetes integration example somewhere as well, if I recall correctly. Helm chart, maybe, something like that. What else can we drag along? Well, I think sub question between brackets, the switch from local to docker could be really interesting because, well, there aren't that many projects that well, either they start directly in docker or stay on a virtual machine or a physical server. But the switch from a local setup to a docker setup could be interesting to cover as well. What would be cool? So my colleagues have created, like, a whole project on Google Storage. So maybe we can do a simple project on Amazon and then run a docker container on the Amazon Kubernetes custom container using a project in no standard container. You can use a standard container because you don't have to expose any metadata. The metadata can be read from s three. Correct. Well, that was for Google Storage. I don't know how security works on the Amazon side. API key. Yeah, the Kubernetes host, the containers actually know the Google Cloud project or something. There's some secret sauce involved. So in theory, you can just access Google Storage because it knows which project, it knows the service account, it knows the authentication, and then you can just read from it. Right, but otherwise expose the key in some way. But those would be really cool exercises, I guess, right? Yeah, but probably more than one session, actually. Yeah, we want to keep the sessions under an hour because the video is getting a little bit long. Otherwise, I hope you found it useful. Guys, thanks for the awesome questions, and see you on the next hot hop hangout, I guess. All right, well, again, two or three weeks. Yeah, let's keep them rolling, right? Cheers. Thanks, everyone. Bye.