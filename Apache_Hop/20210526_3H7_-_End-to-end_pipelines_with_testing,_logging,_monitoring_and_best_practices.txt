Yeah, so we'll have a look at pipelines. This won't be a tutorial on how to build pipelines, but we'd rather have an end to end look at what a pipeline is, the basics of how you create a pipeline and how you run, debug, preview, pipelines and logging, monitoring, testing. So the whole set up that you need to create and run your pipelines in a robust production environment. Like I said, the topics that we'll look at today are couple of the high level overview. So what are pipelines, what are the internals, what are pipeline basics? How do you execute, preview, debug, a pipeline, logging and monitoring, error handling and then finally we'll have a quick look at best practices, how you design and build your pipelines in the best way possible. We'll keep the slides as short as possible. I think we have two slides after this one and then we'll dive in. So, pipelines, first of all, the pipelines in Hop and if you have a background in Kettle and Pentagon data integration, it's similar to what a transformation is or was there. A pipeline is where you do the heavy data lifting. Basically you read data from a variety of sources, do a number of operations on your data, could be lookups joins, enrich your data in a number of ways, do some cleanup well, manipulate your data, do some calculations, extract data, whatever. And then finally you'll write your data to a target platform. Could be a database, relational database, graph database, a data lake. There are tens, if not hundreds of possible targets that you can write from hub. A pipeline is a collection of transforms. And transforms are very small basic pieces of functionality, blocks of functionality that you chain together with Hops and Hops, pass data between transforms and the collection of transforms and the Hops that connect those transforms are what you use to build the powerful pipelines that you can build in Hop. Once you've designed a pipeline, you can run it in a number of runtime configurations. We have the native runtime configuration, so the Hop engine itself that you can run locally and remotely. So you can run a pipeline on your local machine or on a local server from within the Hop GUI from Hop run or remote on the Hop server. And then additionally we have Beam that was covered in the previous hangout session. And Beam allows you to run the same pipeline basically on Spark, Apache, Flink or Google dataflow. So those are a couple of different options that you have to run your pipeline. But the goal here is to be as close as a design once run anywhere design paradigm. Of course there are tweaks to optimize for the environment or the engine that you will run on. But basically the idea is always the same. Another important thing to note is that all transforms in a pipeline run in parallel and run whenever you start your pipeline. So if you start a pipeline, all transforms in that pipeline will run simultaneously. We'll have a look at that later on in the hands on part, but even if you do a preview, then your entire pipeline is active. And even the transforms that are after the transform that you want to preview are active and may have some unexpected side effects if you are not aware of the fact that everything runs in parallel and is active when you run a pipeline. So that is an important one. And finally, unit tests are an important new aspect in Hop that allows you to not just make sure that your pipelines run correctly, but also that the data is processed doing the way you expect it. The steps that are or that a pipeline goes through when you run it are first of all, there's a preparation phase where, well, the engine checks if you're running in save mode gets the parameters, the variables that are needed to run your pipeline checks and sets the correct logging level fetches the row size the row set size that is specified especially in the local runtime run configuration or in your game engines. Then your pipeline is initialized so the entire graph of your pipeline is built. Hop checks which transforms, need to run first, need to do some load data in memory, et cetera. Database connections are opened. Then in the execute phase, your pipeline starts. There's a threat for each and every transform and finally there's a finalizing and cleanup phase where database connections, et cetera are closed and all the timers are stopped. So that's basically the slides part that we wanted to walk through. So now it's about time to get our hands dirty. And as always, we want to make this as interactive as possible. So well, feel free to interrupt and ask questions if things are unclear or are moving too fast. So first of all, I'll walk through a simple pipeline. So if you click the plus button here, choose a new pipeline, then this is the canvas where you do all of the work, where the visual development happens. You have a header or a toolbar at the top that we'll look at in a minute. But this is where you run most of the operations and design items in your pipeline. In one of the previous sessions we had a quick walkthrough of the toolbar on the left hand side. The different perspectives that we have there, I may touch on that briefly, but let's move on in your pipeline edit if you click anywhere on the canvas, just like the image says here, you have a pop up dialog where you can choose the available transforms that we have. And I think at the moment there are about 200. Well, yeah, about 200 in different categories. The easiest way to find a transform is just by typing a part of the name of the transform or the functionality that you want to use so in this case I'll use a CSV input and well, there is an option. Look in general here to use double click that is enabled by default. I disabled this. So I now have the single click mode available and that allows you to single click on the icon of a transform to open the available actions. Or if I single click on the name of transform that opens the properties for this transform directly, then for this CSV file input, I'll select a text file that has a delimiter of Cmicon. Use get Fields to detect the field layout of the file so that fetches the field names and the detected data types from the file. I can do a quick preview here and well, that shows me the first line, 100 lines. Well, there are 100 lines in this file, but this shows you a preview of what is in your data. Then as we'll look at or as we'll see in the best practices, it's good practice to use meaningful names for your transforms as soon as possible because that makes your whole pipeline as readable, readable and self documenting as possible. While I'm at it, I'll save this. Then to add another transform, I'll use select values. Select Values is another one of those transforms. If you come from the Pentau world, from the Ketle world, then this is definitely a transform that you're familiar with. Now that we have two transforms in this pipeline, I'll quickly align them. We need to create a hop and creating a hop is there are a number of ways to create a hop. You can drag from one transform to another, holding down the Shift key and then letting go of your mouse button over the second transform. In this case there is an option for error handling that we'll look at later, but just main output will do for now. Another option is to drag with the scroll wheel instead of holding down the Shift button. There are a couple of other options that are available in the documentation, but the easiest way, I guess, and the most common way to create hops is holding down Shift or scroll button drag. In this select values, there are three tabs. First one is the select and alter where you can select or create a selection of the available fields that you have. So in this case, I'll keep all the fields. You can rename them. In the Remove tab, you can remove fields from your stream is that question or sorry? And in the Metadata tab you can change the data type formats of a given field. So for example, if I wanted to cast this birth date to a string, I would change the data type here and change the format or use a format of year month date with dashes. A very useful keyboard shortcut in this case is to select a row and then control K and that keeps only the selected row. That is a lot more efficient than deleting the rows that you don't need. So then in the if I click on this button, on this transform icon, now I can get a number of options. So show output fields shows me the fields that will be available after this transform. So in this case, this is all the fields that we read from the CSV file with the birth date that was transformed into a string. And the rest is basically, in terms of functionality, more of the same. So if I add another filter here, here we can specify a I'll use the state more in the examples today. So if we now use a filter and I'll add two dummies, keep it simple. The difference here is that instead of having one hop going from one transform to another, we have two different types of or two possible destinations. So in this case, the filter creates a hop that sends data to a transform that holds the data for the true condition or for the false condition. And the result of this filter is always either it matches the condition specified in the filter or it doesn't match. So you have the true or the false. And there is a third option to use the main output of the transform, which is the same as the true condition. There are a couple of similar transforms or transforms that have similar cases. In the switch case, you have the similar option to what we did in this filter rows. But there are multiple options there. Just like in your favorite programming language, a switch case statement allows you to evaluate a field and passes the results of your different conditions. So I could create a case for the different states and then process the data differently, add a constant or whatever. For each of the states, there are a couple of other transforms that might be interesting. For example, a stream lookup. It it yeah, the stream lookup needs input bart, of course, yeah, not output. I'll add it in here. Well, I won't go into this example too deeply, but I'll add another. So in this case I specified this it well, yeah, that's inconvenient. But this stream lookup uses the information from the text file that we read and uses the information that we get from the dummy. And I should have prepared this example better, but the data coming from the dummy or from any other step will be passed into the stream lookup and will create a second input stream for the transform. There's actually a beam example, the complex one, that shows all the switch cases merge join lookup. It's like a whole collection of them, I think. Well, I'll close it anyway, so the complex input is well, I don't think it makes a lot of sense to go into too much detail here. But we have, for example, a merge join here that combines data from customer, data from the state, data that was uppercased. And the two streams coming from customer data and state data will be combined on the state that is available in each of those and will be passed on to later transforms in the pipeline. And then the Lookup here will use the data that was provided by the merge join. We'll combine that with the count per state and use the count per state here for every match that we found in state code from the two input streams that we have. So the lookups and joins allow you to enrich and combine data from various a number of inputs. And the switch case, like I mentioned here, does exactly what I described in the previous example. So in this case, we have different processing for California, Florida and New York. And then finally this is written out to another file on another view file. So let's switch back. In this case, I added a Basic pipeline here that reads the same file that does a sort by state. So sort is another transform where you sort based on one or a number of fields and describe and define whether that sort is ascending, needs to be case sensitive and so on. And finally does a group by. So we'll look at state and do a row count without any additional fields. We'll do a row count. So we have the number of customers in the file. Other operations that you can do here are the default aggregations. So sum, average, mean, median and so on. But this is well, walking through all of the transforms will take us well, it's not possible in the scope of this session, but we have all the documentation for each transform. There's a help button that opens the documentation for each. Ah, brilliant. So all the all of the transforms are documented and the documentation is available behind the help page. Now, if we run this pipeline, and again, if you come from the Pentagon or Kettle world, this will be very familiar. If you click the Play button in the upper toolbar, we have a run configuration. And that's what I mentioned in the presentation or in the slides at the start of this session. Run configurations are configurations where you specify on which engine you want to run. So we'll use Local for the most of or almost all of the samples today. But there are configurations or engines that you can use to run on beam dataflow. So on Google dataflow, direct beam, runner, beam over Spark, beam over Flink, and remote on the Hop engine, but through the Hop server. Log level is where you specify the amount of logging that you want to see. Basic will be fine in most cases, I guess. Detailed debug and role level, as the name implies, will give you more details, more information about what is going on behind the scenes, but will be very furbish. This is not something you want to use if you're processing tens of gigabytes of data. There are parameters that you can specify in your pipeline, variables that you can specify here. But in general if we click Launch have to reselect my run config here's. So if we run this pipeline you get an overview of which transform was used or was processed. They're all finished now. Their duration was between two and nine milliseconds. Number of rows processed per second is specified here. And in the logging tab we get a little bit more details about or we get the full logging information then in the local run configuration well we have the indication of the status of the transform and in the lower right corner there is a small data grid that shows you the output of a specific transform. So we can have a look at a number of phases in the pipeline and that allows you to quickly debug what is going on. The other way of previewing a pipeline is by clicking on the icon next to the play icon and there we can specify a number of rows to preview. The default is 1000 and then Quick Launch and that will give you a similar pop up with similar information to what we just saw in the preview through the icons on the transform. In this case, I can click get more rows. We're at the end of the data set here but if you have more data you can fetch the next batch of rows here or stop the preview. And a similar option to debug is I'll switch to another transformation, another pipeline sorry, where we have again the same file. I exclude New York in a filter and a calculator where we have the year of birth. So the year from the birth date in this data set. And if I now click Debug we have the same pop up dialog that is configured a little differently here. We still have 1000 rows but instead of retrieve the first x rows thousand in this case the option Pause Pipeline on condition is now enabled and this option allows you to create a condition that is very similar to the filter that we just saw in the previous example. So for example, if I create a breakpoint on state with a value of California and I click Quick Launch then we won't have the first 1000 or whatever you configure number of rows but the preview will stop at the first occurrence of the breakpoint. So in this case we have twelve rows in reversed order and the pipeline stops executing the first time we meet the breakpoint. It's actually paused. Yeah. What did I say? The pipeline is not stopped but it's paused right now. So if you hit the run button again it will continue to get next California occurrence. Exactly. Yeah. If I hit Get more rows I will get the next batch of 13 rows until we find the next California. Then the record after that is again California. So we only have one row before we meet the next breakpoint and then finally we get the there's one more, but we keep running the pipeline, pausing it as soon as we find the next occurrence of the breakpoint and so on. And that allows you to quickly skim through your data as you go. And this is really useful to investigate issues in your data at times. And then finally, if there are no more California's in this data set, in this example, then we get the remaining data. But actually it's similar with the preview. In this case, if let's say I run this for a preview of ten rows, then the transformation is paused after ten, I can get ten more, but since the pipeline is paused, we need to click stop to stop the pipeline. Otherwise it will remain active and your pipeline will show. So if I run a preview of ten click close, actually. Well, anyway, the pipeline is paused after the number of rows that you preview it's. Actually, I didn't know this before. It used to be that if you click close that the pipeline remains paused and is still active here in the UI. I didn't know it's still the same. Well, just hit run. Oh, it gives the run button now. Yeah, exactly. Somebody did something wrong then. Well, that's what surprised me as well. Anyway, my impression is to just get rid of all that previewing and debugging. I don't know, we talked about reworking it. It may need some love, but the idea is useful in well, yeah, it is useful to be able to preview a number of rows, in my opinion, but well, that's something we can look at later. Um, but that's it for running previewing debugging pipelines, basically. Let's have a quick look at logging. Well, yeah, we looked at the log files or the logging output here. If you run your pipelines through Hop Run, you can pipe that information to a log file. But we also have the pipeline lock, that is a new metadata type that was added a couple of months ago. And that allows you to basically create a pipeline that will capture logging information. So if I'm, when I specify a name here, I can select a pipeline or create a new one. I'll just create a new one. Now you it generates a basic pipeline. I'll get back to this in a second. So the name of that pipeline is automatically added here. And what this will do is at the start at the end of a pipeline, and if you configure it during your pipeline, it will send the logging information to the pipeline that we just created. So let's switch back. So if I enable periodic execution during execution every 30 seconds, then that pipeline that we just created, the pipeline logging here, will be executed and will receive the logging information. And since we are in this pipeline or in a pipeline processing the logging information. We can basically do whatever you can do in a pipeline. So this enables you to write logging information to a database relational or graph push to a Kafka stream. I think on the chat we talked about sending logging data to elastic, you name it. Every target platform that is available in Hop can be accessed here. So for example, in this case, I'll write to a log file it just use DMP it it. And while we're here, the data that you get here is pretty verbal. It's actually a little bit more or quite a bit more than what we had in the Kettle logging. But you get the logging date, the pipeline name, pipeline file name, so the start, the transform name, number of lines, read, written, input, output, updated and rejected. So you get a lot of information about what is going on in your pipeline here. But there's an option in the pipeline logging transform to only log the workflow, the pipeline information. So there's a checkbox in there knowing the transform bart this one. Yeah. So if you disable that checkbox, it gives a lot less output if you just want to have a log file and the only field you're interested in is basically the log text of the pipeline. So just disable that checkbox. This will give one record per transform that is running. Well, it is actually initiated, not running, but at Start you don't have that many. There's nothing happening at start yet. But anyway, that's how that goes, right? So if I run this pipeline again now, it it what did I do? Let me check. Well, I have I have another example here that does exactly this and I'll switch to that's it. So what do you have? Well, I'll open that in Vs code file contains all of the information, the login information that we just saw in the text file output. So this basically captures the logging information that is specified here. But since, like I said, since this is a normal pipeline, there are a lot of options to process, filter, combine your data with other sources or with other information. So this is another powerful addition about logging matt or hounds. Anyone else? Well, you often don't want to log everything, so if you open up that metadata object for a second yeah. So logging parent pipelines only. Yeah, parent pipelines only means that if you have metadata injection scenarios and you want to log the lower levels, you should disable that because it came up in the chat. Or if you have a loop in a pipeline executing workflows, if you want to log the workflows below as well. So disable that. Right. Especially for the workflow log because it's the same there. If you just want to log to a text file, just execute at the end of a pipeline, I guess. Only keep it simple at the beginning. Just write the logging text and forget everything else. Most is in there, then you have the log files. If you can log to something like a database or something more persistent, if you log to a text file, it's easy with the VFS drivers to log to S Three or to any of these big data platforms, cloud platforms. So that's useful so that you can just pile up all the logging over there, but that's pretty much it. Okay. Well, and worth mentioning is that similar to the pipeline logs? We have workflow logs and pipeline probes and workflow logs are, well, exactly the same, but work on a workflow level. And pipeline probes work exactly the same, like the pipeline log, but process receive the data, the actual data, instead of the logging data in normalized format. Yeah. Jason so, yeah, that allows you to parse your data for data quality, auditing, et cetera. Right. Then I do have a question on pipeline logging. Yes. Is that something that's supported to set up using containers, how to set that up? So as long as the runtime has access to the metadata. Right. So typically what you do is you set up a small project. So the way that we run, for example, our integration tests is that we have all our integration tests grouped by project, basically. So all the Cassandra tests are in a Cassandra project. All the transform tests are in a transforms folder, and those are projects with their own metadata. So what we do when we run this in a container, in a docker container that we have for that purpose is the first thing we do is just say, okay, we set a variable to the config files. Say, this is my hop config folder, and in there is everything that we need to have a project for transforms integration test. So there's only one variable that you need to set. And all of a sudden, the whole configuration is clear to the runtime in the docker container. All it needs is basically the metadata. And then yeah, absolutely. Then it can write to wherever it wants in the outside world. But that's a very fast and convenient thing to do. It doesn't need to write anything in the container either. You can mount a folder from the host system, but yeah, that's a good question. It's very easy to do. Thank you. You're welcome. All right. One thing that is worth mentioning as well is error handling. Of course, most of the error handling or a lot of the error handling that you do will be done in pipelines, in workflow stories. So if a pipeline fails, you capture that logging and process it according to what you need. In some cases, it makes sense to handle an error in a pipeline gracefully and allow your pipeline to continue running if there is an error or if you have some number of errors. So in this case, again, similar pipeline to what we looked at earlier. Sorry, I'll add another dummy transform. And what we've seen in one of the first hops that I created is this option. In this case I'll use the error handling of the transform and that gives you a red hop. And this red hop will contain all of the rows that filled in the select values transform. So in this case, just to have a very obvious failure, I casted the state field, which is a string to a date and obviously that fails. But in the properties of the transform we can specify error handling. So in this case, the dummy step is the target transform. Error handling is enabled and you can configure another number of additional fields to keep the number of errors that occurred a description, error fields, error codes, and even a maximum number of errors that is allowed before this transform fails. A maximum number or maximum percentage of errors and then a minimum number of errors to read before to doing the percentage evaluation. So that gives you a lot of flexibility to make sure that a pipeline doesn't fail because there are some errors in your data. And well, if you run this, the second dummy here takes 98. So all of the rows because obviously you can't cast a string to a date, but in real life use cases you would have a limited number of failures here that could be acceptable. Similar transforms that support this are for example, what table output is a common scenario where you write data to a database table and if anything fails, you want to capture the rows that failed in an ops table, for example. So this is something that you want to use with care, but it is powerful and useful in some cases. And then finally we'll have a quick look at unit testing. The options that you have here there is a unit testing category and the way this works is that you create a data set and a data set is a definition with a number of rows of data that you will use to test against. So in the case of this pipeline here, if we create a data set, we'll set one data set as an input data set, another data set as a golden data set will populate those two data sets. And if the pipeline runs, it will process this file or another file that we fetched from some source. It will process the data in the other transforms in the pipeline. Finally write that out to another text file. But we'll also run a unit test and that unit test will compare the input set to a golden data set. And if the results of the processing with the input set against the golden data set succeeds, then that test is successful. So for unit testing, we also have the toolbar on the right hand side of the main toolbar. I have a unit test for this pipeline here and this has the customers data set as input. The. State count as the golden data set. So you specify those here with set input set, golden data set. And if you run the pipeline, then in the logging there is a message that this test ran successfully. So test passed successfully against the golden data set. If I now change the condition that I have here so my input reads the entire data set. The golden data set contains everything with this condition set to not New York. If I inverse this so everything only New York and run it again, I have the incorrect number of Roso, this test fails and there is a transform and an action to fetch the available unit tests in a project and run all of those at the end of your daily boat. For example, to make sure that your data is working as expected or that the data is processed in the way you expect it. The data sets are available in the metadata perspective so you can watch the structure here, have a preview of your data sets as well. It's well, this was pretty quick but hope you get the idea of the unit testing. I don't think this is used very much at the moment, but it is really powerful and increases the stability of what well, most of our integration tests are built on it. Bart, if you want I can show a few later on maybe. Well, yeah, definitely. I know it's used in the integration tests, but I don't think it's used that often in real life projects so far. Well, it's new. Well, it already existed as a plugin in kettle, but it's definitely something that will become more and more important. But well, yeah, feel free to show yeah, sounds good. Or if there are other people have questions, show my messy desktop. Let's take for example, so one of the integration tests is one for a merge join, right? So you have a couple of records on the left hand side of the merge, a couple of records on the right hand. We have the field value twice in both inputs. So what we expect is basically the output to be value and value underscore one. So that field needs to be renamed. So this is something that that test validates. It basically says, okay, this is the output and it does the validation against this data set. What is interesting is that you can select one of these tests automatically along with the pipeline that you put the unit test on because you can have multiple unit tests per pipeline, right? But having this open automatically is quite useful to run these tests in a workflow. So these main workflows are run automatically on the container every night. So you can specify these. So Get test name gives you the list and you can select them and then you can also jump to them from the context menu. So open this example. So we have the inner left outer. So here we have multiple golden data sets per variant of this transform. And we specify the field mapping from the output fields. In this case we have value one and value two and a sort order. The sort order is important because transforms and pipelines in general don't really guarantee a sort order unless you specifically sort the data. So this makes sure that we can always compare apples to apples in these integration tests. You can see the data sets over here. Make sure that you use a sane naming scheme. I always use the naming scheme golden data and then the name of the pipeline that I'm testing. And then maybe with an extra indicator for the output transform. So if you look at these, you can see the CSV file that it's stored in. You can actually look at these data sets like double fields that we had earlier. So you can see these text files in Hop itself, no need to jump somewhere else. But in the metadata you can also see the data in grid format, or you can even edit them. So it's very useful that you have this edit button. Then you can just copy paste from Excel, from any other source to create a data set. So for example, if I wanted to create another data set here, I can use this create data set. It will use the row definition, the output metadata, basically to create an empty data set. And then you can start typing in values here, one, two, three. Or you can paste in here. Or another option is to write output rows to a data set. You don't have to have an active unit test or create a new one or whatever, but you can create data sets quite quickly with this simply by specifying create data set. Okay, this is this one. Now I'm going to run and write to the data set and it will stream the data to it. So this way these tests take like setting something like setting this up maybe 20 minutes, and then it's done. And so we started building a whole collection of integration tests. This was the first one. See that parallel copies of an ad sequence still gives the same output. This is using a different way of testing. It's not an invalid way. Right. Because if the output is not expected, it aborts. But for more complex scenarios, it's easier to use an integration test. There's a variety of ways to test. As long as you test, it's fine. Right. So this is more along what Bart was showing earlier. Yeah. So what else is there to say about that? Yeah, make sure that you test. It's easy to do. Matt, just a quick question. Sure. Do you have any scenarios where we would test the child and parent data table populations? Like in the data warehousing? Right. It's very common to have the dimensions and the fact populated, right? Yeah. So for example, for Cassandra Mongo, and, for example, Neo for J, what we do is we clean up a database, for example, create some indexes, create some nodes in a database, and then we validate. Right. And the validation usually happens through, let's say, a set of validation transformations. And these basically read the data back in and then compare against, again, golden data. Right. So this allows you to basically say, I'm going to spin up an empty database, a MySQL in a container, or in this case, Neo for Jake, Cassandra, Mongo in a container. It's empty. We don't mind that we want it. We're going to stuff all sorts of data in there. And you can use your own pipelines, your standard pipelines with test data, and then see if the output is what you expect. In this way, you can test your pipelines that you run in production with test data, special null values, all the junk that you can find, all the bat records you could put in there. Right. And then you write to the dimensions, see if they still slowly changing, see if you get the primary keys that you expect. That sort of stuff. It's quite easy to do. Takes a bit to set up, but not that much. And we have an example of that in the project itself. Right. So the integration tests for Neo for Mongo, for example, it inserts into a collection and then it validates. See if I can have a validation. So this reads the it tries to loop over, tries to do some weird stuff, and I leave notes in there to see. Okay, what are we doing exactly? Querying Mongo and see that we still get the exact output, as many rows as we put in. Simple tests. But very valuable for us, the more that we add these end to end tests, the more stable our whole thing becomes. Because obviously during these tests, we also test other transforms and stuff in the workflows. In fact, from the 190 transforms, even though we have a limited amount of integration tests, we were already testing well over half of all the transforms in our portfolio. Yeah. Does that kind of explain yeah, sure. Good pointers. Thanks. So cassandra same thing. Create a key space for hop in an empty container. You need a key space. It outputs to a table and then reaches back out. It's just simple things. And yeah, if somebody changes something to the Cassandra output or input yeah. It will break. Right. Every night we run that test on a docker container, uses some simple docker compose setup that works really well. It caught lots of stuff that we broke. Yeah. Thank you. You're welcome. Any other questions? Yes. Are those in the samples, or do we have some samples on that unit test and those going against golden? So the integration tests, I can show you these. These are actually in the source code, but yeah, this is open source. I can just show you where it is. So if you go to Incubator Hop Apache, Incubator hop, hopefully just hop pretty soon. So here are the integration tests. And like I said earlier, every folder in here, like the one I just had, Cassandra, has a whole hop config. So you just point the hop underscore config underscore folder to this folder and open this up in open Hop GUI, then you will drop into this project. Basically, this hop config basically says there's a lot of crap in here, but at the end it says there's one project. And yeah, that's it. It's just one project and the default and it has a testing. But I think the only testing configuration is the Cassandra host name, which is defined by docker compose, but you can change that if you want. So that's the whole sample. Basically, we have not just one sample, but a whole bunch of them, all self contained project. And the docker integration, the way that that works is we have docker integration tests, and this is basically the docker file for the unit test. It's a little bit modified from our standard docker file. It basically tests the code, the whole code that you build, because we run this in a build test cycle. And the docker compose basically says, okay, we need basically postgres new for Jmong or Cassandra. We're working on splitting these up into different smaller docker compose instances. But yeah, this is the way it runs and it works fine. This is where that Cassandra host name comes. You'll you'll find your way around. Thanks, Matt. Yeah. And if you have any issues with this, we do plan on writing more documentation about it and it might even be worth it. Doing another hop session on this more technical deep dive and go into how you get from here to here. Basically, I'm sure that Hans is dying to talk about that because he did an excellent job setting this up. Basically, this is the output. So for the Cassandra input output, you even get something goes wrong, the logging, all these transforms, what did I show merge join earlier on? Where is merge join? So, yeah, we tried to group multiple of these tests into one workflow. So if you have multiple tests, like in this case, there are two unit tests, two transforms that are being tested. I think five or five transformers being tested, five output data sets. So a lot of testing. We don't want to overload that panel here with too much junk. Right. Anyway, I'll stop rambling. Thank you, Matt. Just like the integration tests will create those as more real life or projects oriented tests and samples as well. The integration tests are now to improve the hop code and the hop development, but we'll make that available as samples in real life projects as well at some point. Sounds awesome. Yeah, it's very powerful and quite simple to set up. I found it remarkably easy to add because I think this week I added a whole bunch of them because we were reworking some stuff last week I added a bunch for metadata injection, for example. There's also tests and in metadata injection you can actually add tests on the parent and on the child. I just did the parent because I was just interested in the output. But you could actually integrate test in the template, basically in the child pipeline. Yeah, that's pretty cool. Did we have any other topics on pipelines? We only have the best practices left. All right, go ahead. Do you want me to do it? Well, you had slides, I heard you say a couple of bullet points. Sure, but I'm here. Yeah, I think for me, I think if you look at these integration tests, if we look at these, it's all about naming schemes that's for me, the most important one. It becomes such a garbage can if you don't have at least some naming convention. Yeah, I think that is well, not related to Hop, basically. It's a general fact that naming conventions in database schemas, anything software related, benefits from having consistency in naming in project structure. So that applies here as well. Size Matters is what I took from the best practices in the Hub documentation. And that basically means that the number of transforms in your pipelines should stay within reason. It's hard to add any hard numbers to it, but 250 transforms in a pipeline is not a good idea. I think that if you start creating, if you notice that you need to scroll through your pipelines to see what is going on, then that may be an indication that you're cramping too much functionality in a single pipeline and you need to split things up, make things more smaller, more modular variables. Well, parameterize everything, make sure that there are no hard coded well, that there is no hard coded anything in your project. Basically. Should we talk about environments and project variables now for another time? It's very easy to use, I've been doing it more and more. Yeah, it definitely is. Well, feel free to expand on it, but even though the topic today is end to end pipelines, it's a lot more cover in 2 hours or in 1 hour, definitely. So feel free to expand, but I don't think we'll be able to cover it in great detail. Maybe just walk through them and then see what questions we have. Start, if anybody's interested. Yeah, definitely. Well, logging. We had a look at the pipeline log workflow logs. There are log files and again, I don't think the platform where you store your logs is the most important. I think the most important thing is to store your logs somewhere but definitely use them. Keeping your logs around for five years but never looking at them doesn't make any sense. Use your logs to build logging, to do monitoring, et cetera. And maybe not the most important thing maybe is not to do operational alerts, that is important, but keeping an eye on the trends. If the same volume, processing the same volume of data takes increasingly longer, if your processing speed in rows per second starts to decrease, things like that are very important to keep an eye on mappings and recurring logic in general. So I think this applies to pipelines workflows as well, keeping all of the functionality that you find yourself repeating over and over again in separate modules and separate pipelines and mappings whatever is always a good idea to make sure that you don't do any code duplication in your project. A little bit of code duplication is often okay and is not a problem, but if you have the same sequence of 1015 transforms in your pipelines that return several times, then those are ideal candidates to move to mappings, similar for metadata injection. In a lot of cases you'll have pipelines that do more or less the exact same operations. So in a lot of cases you'll have well, I think a common example is onboarding data in your database or a data lake where you pick up a CSV adjacent, a whatever file, make a selection of fields and load those to a staging area, to a folder in your data lake and whatever. And those pipelines are ideal candidates to use with metadata injection where you use a template pipeline inject the metadata in runtime and use that combination of runtime metadata and your template pipeline to keep things as clean as possible, designed for performance. I don't remember what is in the documentation there, but I guess the general idea is that even though you don't always have the real life amount of data, so it's not always possible to tune while you're in development. You probably will have an idea of the volume of data that you'll process. And that allows you to make a couple of design choices that will optimize for performance. Governance basically means keeping everything in version control, doing testing, doing a lot of the things that we talked about today, version control, testing, error handling, making sure that your projects are as robust as possible, that errors and failures are handled gracefully and so on and loops, that is. I think in the documentation we mentioned workflow and pipeline executors to run repeatable tasks. If you need to run the same pipeline for a number of years, for a number of customers, for a number of territories, whatever, well, make sure that you run these in pipelines through pipeline executors where you can pass field values from a database query as parameters for example, to your other pipelines. Well, and the best practices are available in the documentation. So at the very end of the user manual and I think it makes sense to worry through this every now and then. I think we will update this as we go, but a lot of this are valid in general, won't change that often. There won't be any landslide changes here anyway. So these are basically most of them are common sense to use in your developments and in your project in general. Any questions? Any remarks on this? This is good. I mean, it covers a lot of scenarios and helps us. What is the option, I mean, to run this pipeline, let's say on a Spark Apache Spark environment, let's say in EMR or on my local Spark setup. So a few weeks ago, we did a whole session on that and it's been recorded as well. Oh, I missed it. I'll check that out. So we have a working example there as well. Yeah, sure. Yeah. And we so, first and foremost, for Spark, if you go to the user manual yeah, I have no idea. Every time I share, I just search Spark. Right. So the Spark there's some information here about how to run with Spark or run directly. If you look at this one, for example, it comes straight out of the box with the direct runner. I think to test locally, I think you can do something like Spark, local Spark, and then say, I think there's always like a mouse over here, so local four threads. But yeah, we did like a whole session on it a few weeks ago. I checked it out. So you can just take your feed video, but to test, you can use a direct runner. And then once your unit test and your direct runner on the Beam runtime runs, then you can port it over to Spark or Dataflow or whatever project. Okay, that's nice. I mean, Matt, what's your experience comparing, let's say, if I write a Scala code on Spark and if I run the pipeline, mean is the performance have you got a chance to compare the performance and all performance is typically lower per CPU for things like Spark because it does a lot more work, right? Spark fling dataflow. They're not just running, they're running safely in a multi threaded environment. The thing that they have really going for them is that they scale. So typically what you should do is get a whole bunch of nodes, run it on a big cluster, and then obviously that would be a lot faster to run than on my little machine. But if it's a little bit of data, it's probably much faster. And if you don't care, you can use the Hop engine, which is a very simple, optimistic engine that will fail in case anything goes wrong. It will not run on multiple nodes. Sure, I get that part. Yeah, that's the distinction, right? Yeah, sure. The question is this. So, let's say if I have got some ETL processing to do on huge volume of data, and if I write, let's say, a pipeline to do that and for the same activity, if I write a code in Scala and run both of those on, let's say, spark cluster, do you see? The performance would be approximately the same. It's hard to say. We used to do these tests back in the days, and we were always surprised when kettle or hop was faster. And usually that doesn't come from the fact that the engine per se is faster because it's just Java code that you're executing, but it's because these transforms, they have been around for 20 years, and they're pretty well optimized, if you know what I mean. And if you hand code, it's easy to make a mistake, and usually that's where the biggest difference comes from. Right. Okay, sure. Sounds good. Yeah. Your mileage may vary. Definitely. And we're an open source project, and we're in the same Apache branch as Sparkling and the others. So we would love to work with you or anybody else that wants to do these sorts of benchmarks, I guess. But I don't have a large spark cluster around to test on a day to day basis, but it is easy to test. Okay, definitely. Fine. So if I've got any queries, I can go to the chat room, right, and ask those questions, right? Yeah. Or user mailing list, but yeah, the chat is most sure. Thanks, Matt. You're welcome. I appreciate it. Any other questions? I guess the main thing is, if you get started with this, is just to start playing around with it. There are other topics, the transactional models of streaming data for Kafka, but I think it's best to leave these other things to another session, I guess, right? Yeah. This is the problem. There's just too much to fit into a 1 hour and a half session to get into much detail. So we'll have to split specific topics up into separate sessions. Mmhmm, cool. All right, if you guys have topics for the next session yeah, exactly. Bring them forward. I think we already had, like, a few suggestions. So what was the next docker and hop web? Yeah, hopweb. Well, and I think the request was a session on docker and hop web, but those are probably two separate sessions. Yeah. Maybe build, like, a session on customizing or using the docker container and customizing it. Maybe we can fold the integration test that we have in there, because it's very similar, I guess, right? I don't know. What do you think, Hans? You're up to? Fun topic. These docker containers are built by default on the Apache infrastructure now, so they get updated every time we update the code. The tag zero point 70 is on there and then also 99 snapshot. Yeah. So the latest tag will point to our latest release, which would be zero point 70, and then you have the snapshots release for zero point 99. That's the mistake I made last time when I ran the Hulk web docker container. It was using the latest version, which is zero point 70. The latest version voted on by. And if you need to specify zero point 90 snapshot to get the very latest version, which is a lot better anyway. Yeah, great topics. Yeah, we can do a whole deep dive on unit testing, but maybe let's do another session on integration testing and stuff like that because those topics keep coming back as well. Yeah, we plan to keep doing these sessions and put them up on YouTube. We have plenty more topics where those came from. Maybe we switch them off a technical session and then more a user maybe in four weeks, then one on workflows, something like that. Now that all the dialogues are cleaned up, the next session will be technical again, and then three h eight is technical again, three h nine could be workflows and well, I think what we may do is create a list of topics and provide a way to vote for priorities or something like that. Sounds good. Okay, well, if there are any more questions, things that you want to look into, then we can call it a day, I guess. Thanks a lot. You're welcome, Bruno. See you later, guys.