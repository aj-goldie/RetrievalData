Speaker A: Hey guys. So I didn't know I was going to be one of the keynote speakers, so this is probably going to be the most reduced scope talk of today. I'm talking about type hints, and in particular I'm talking about how pedantic might be all you need to build with language models. In particular, I want to talk about structured prompting, which is the idea that we can use objects to define what we want back out, rather than kind of praying to the LLM gods that the comma is in the right place and the bracket was closed. So everyone here basically kind of knows, or at least agrees, that large language models are kind of eating software. But what this really means in production is 90% of the applications you build are just ones where you're asked in the language model to output JSON, or some structured output that you're parsing with a regular expression. And that experience is pretty terrible. And the reason this is the case is because we really want language models to be backwards compatible with the existing software that we have. Codegen works, but a lot of the systems we have today are systems that we can't change. The idea is that although language models were introduced to us through chat GPT, most of us are actually building systems and not chatbots. We want to process, input data, integrate with existing systems via APIs or schemas that we might not have control over. And so the goal for today is effectively introduce OpenAI function calling, introduce pydantic, then introduce instructor and Marvin as a library to make using pydantic to prompt language models much easier. And what this gets us is better validation makes your code a little bit cleaner. And then afterwards I'll talk over some design patterns that I've uncovered and some of the applications that we have. This is basically almost everyone's experience here, right? Like, you know, Raleigh Goodside had a tweet about asking to get JSON out of bard, and the only way you could do it was to threaten to take a human life. And that's not code. I really want to commit into my repos. And then when you do ask for JSON, you know, maybe it works today, but maybe tomorrow, instead of getting JSON, you're going to get like, okay, here you go, here's some JSON. And then again, you kind of pray that the JSON parsed correctly. And I don't know if you noticed, but here user is a key for one query and username is a key for another. And you would not really notice this unless you had good logging in place. But really, this should not happen to begin with. You shouldn't have to read the logs to figure out that the passwords didn't match when you're signing up for an account. And so what this means is our prompts and our schemas and our outputs are all strings. We're kind of writing code and text edit rather than an ide where you could, you know, get linting or type checking or syntax highlighting. And so OpenAI function calls somewhat fix this, right? We get to define a JSON schema of the output that we want, and OpenAI will do a better job in placing the JSON somewhere that you can reliably parse out. So instead of going from string to string to string, you get string to dict to string, and then you still have to call JSON loads. And again, you're kind of praying that everything is in there. And a lot of this is kind of praying to the LLM gods on top of that. If this code was committed to any repo I was managing, I would be pissed. Complex data structures are already difficult to define, and now you're working with the dictionary of JSON loads. And that also feels very unsafe because you get missing keys, missing values, and you get hallucinations, and maybe the keys are spelled wrong and you're missing an underscore and you get all these issues, and then you end up writing code like this. And this works for like name and age and email. Then you're checking if something is a bool by parsing a string, it gets really messy. And what Python has done to solve this is use pydantic. Pydantic is a library that do data model validation very similar to data classes. It is powered by type hints. It has really great model and field validation. It has 70 million downloads a month, which means it's a library that everyone can trust and use and know that it's going to be maintained for a long period of time. And more importantly, it outputs JSON schema, which is how you communicate with OpenAI function calling. And so the general idea is that we can define an object like delivery. Say that the timestamp is a date, time, and the dimensions is a tuple events. And even if you pass in a string as a timestamp and a list of strings as tuples, everything is parsed out correctly. This is all the code we don't want to write. This is why there's 70 million downloads. More interestingly, timestamps and dimensions are now things that your ide is aware of. They know the type of that you get autocomplete and spell checking, again, just more bug free code. And so this really brings me to the idea of structured prompting, because now your prompt isn't a triple quoted string, your prompt is actual code that you can look at, you can review, and everyone has written a function that returns a data structure. Everyone knows how to manage code like this. Instead of doing the migration of JSON schemas in the one shot examples, I've done database migrations. I know how some of these things work, and more importantly we can program this way. And so that's why I built a library called instructor a while ago. And the idea here is just to make OpenAI function calling super useful. So the idea is you import instructor, you patch the completion API. Debatable if this is the best idea, but ultimately you define your pydantic object, you set that as the response model of that create call, and now you're guaranteed that that response model is the type of the entity that you extract. So again, you get a nice autocomplete, you get type safety. Really great. I would also want to mention that this only works for OpenAI function calling. If you want to use a more comprehensive framework to do some of this pydantic work. I think Marvin is a really great library to try out. They give you access to more language models and more capabilities above this response. But the general idea here isn't that this is going to make your JSON come out better, right? That idea is that when you define objects, you can define nested references, you can define methods of the behavior of that object, you can return instances of that object instead of dictionaries, and you're going to write cleaner code and code that's going to be easier to maintain as they're passed through different systems. And so here you have for example, a base model, but you can add a method if you want to. You could define the same class, but with an address key. You can then define new classes like best friend and friends, which is a list of user details. If I was to write this in JSON schema to make a post request, it would be very unmanageable, but this makes it a lot easier. On top of that, when you have doc strings, the doc strings are now a part of that JSON schema that is sent to OpenAI. And this is because the model now represents both the prompt, the data, and the behavior all in one. You want good docstrings, you want good field descriptors, and it's all part of the JSON schema that you send. And now your code quality, your prompt quality, your data quality are all in sync. There's this one thing you want to manage and one thing you want to review. And what that really means is that you need to have good variable names, good descriptions, and good documentation. And this is something we should have anyways. You can also do some really cool things with pedantic without language models. For example, you can define a validator. Here I define a function that takes in a value, I check that there is a string in that value, and if it's not, I return a lowercase version of that, because that just might be how I want to parse my data. And when you construct this object, you get an error back out. We're not going to fix it, but we get a validation error, something where we can catch reliably and understand. But then if you introduce language models, you can just import the LLM validator, and now you can have something that says don't say mean things. And then when you construct an object that has something that says that the meaning of life is the evil and steal things, you're going to get an invalidation error and an error message. And this error message, the statement is objectible as actually coming out of a language model API call. It's using instructor under the hood to define that. But you know, it's not enough to actually just point out these errors. You also want to fix that. And so the easy way of doing that in instructor is to just add max retries. Now what we do is we'll append the message that you had before, but then we can also capture all the validations in one shot, send it back to the language model and try again. But the idea here that this isn't prompt chain, this isn't constitutional AI. Here we just have validation error handling and then re asking. And these are just separate systems in code that we can manage. If you want something to be less than ten characters, there's a character count validator. If you want to make sure that a name is in a database, you can just add a post request if you want to. But this is just classical code. Again, this is the backwards compatibility of language models. But we can also do a lot more, right? Structured prompts get you structured outputs, but ideally the structure actually helps you structure your thoughts. So here's another example. It's really important for us to give language models the ability to have an escape hatch and say that it doesn't know something or can't find something. And right now most people will say something like return, I don't know in all caps, check if I don't know all caps in string. Sometimes it doesn't say that it's very difficult to manage. But here you see that I've defined user details with an optional role that could be none. But the entity I want to extract is just maybe a user. It has a result that's maybe a user and then an error and an error message. And so I can write code that looks like this. I get this object back out. It's a little bit more complicated, but now I can kind of program with language models in a way that feels more like programming and less like chaining. For example, we can also define reusable components. Here I've defined a work time and a leisure time as both a time range, and the time range has a start time and an end time. If I find that this is not being parsed correctly, what I could do is actually add chain of thought directly in the time range component. And now I have modularity in some of these features. And you can imagine having a system where in production you disable that chain of thought field, and then in testing you add that to figure out what's the latency or performance trade offs. You could also extract arbitrary values. Here I define a property called key and value, and then I want to extract a list of properties. You might want to add a prompt that says make sure the keys are consistent over those properties. We can also add validators to make sure that's the case, and then re ask when that's not the case. If I want only five properties, I could add an index to the property key and just say, well now count them out. And when you count to five, stop and you're going to get much more reliable outputs. Some of the things that I find really interesting with this kind of method is prompting data structures. Here I have user details, age, name as before, but now I define an id and a friends array, which is a list of ids. And if you prompt that well enough, you can basically extract a network out of your data. So we've seen that structured prompting kind of gives you really useful components that you can reuse and make modular. And the idea again here is that we want to model both the prompt, the data, and the behavior here I haven't mentioned too many methods that you could act on this object, but the idea is almost like when we go from c to c, the thing we get is object oriented programming, and that makes a lot of things easier. We've learned our lessons with object oriented programming, and so if we do the right track, I think we're going to get a lot more productive development out of these language models. And the second thing is that these language models now can output data structures. You can pull up your old leetcode textbooks or whatever and actually figure out how to traverse these graphs, for example, process this data in a useful way. And so now they can represent knowledge, workflows, and even plans that you can just dispatch to a classical computer system. You can create the data that you want to send to airflow rather than doing this for loop, hoping it terminates. And so now I think about six minutes. So I'll go over some advanced applications. These are actually fairly simple. I have some more documentation if you want to see that later on, but let's go over some of these examples. So the first one is rags. When we first started out, a lot of these systems end up being systems where we embed the user query, make a vector database search, return the results, and then hope that those are good enough. But in practice, you might have multiple backends to search from. Maybe you want to rewrite the user query, maybe you want to decompose that user query. If you want to ask something like what was something that was recent, you need to have time filters. And so you could define that as a data structure. The search type is email or video. Search has a title, a query, a before date, and a type. And then you can just implement the execute method that says if type is video, do this. If email, do that. Really simple. And then what you want to extract back out is multiple searches. Give me a list of search queries, and then you can write some like async iota map across these things. And now, because all the prompting is embedded in the data structure, your prompt that you sent to OpenAI is very simple. You're a helpful assistant. Segment the search queries, and then what you get back out is this ability to just have an object that you can program with in a way that you've managed all your life, something very straightforward. But you can also do something more interesting. You can then plan. Before we talked about extracting a social network, but you can actually just produce the entire dag here I had the same graph structure. It's an id, a question, and a list of dependencies where I have a lot of information in the description here, and that's basically the prompt. And what I want back out is a query plan. So now if you send it to a query planner that says you're a helpful query planner. Build out this query. You can ask something like, what is the difference in populations of Canada and Jason's home country? And then what you can see is, you know what, if I'm good at leetcode, I could query the first two in parallel because there are no dependencies, and then wait for dependencies three to merge, and then wait for four to merge those two. But this requires one language model call. And now it's just traditional rag. And if you have an IR system, you get to skip this for loop of agent queries. You know, an example that was really popular on Twitter recently was extracting knowledge graphs. You know, same thing here. Here what I've done is I've made sure that the data structure I model is as close as possible to the graphviz visualization API. What that gets me is really, really simple code that does basically the creation and visualization of a graph. I just define things one to one to the API. And now what I can do is if I ask for something that's very simple, like, you know, give me the description of quantum mechanics, you can get a graph out that's basically in like 40 lines of code. Because what you've done is you've modeled the data structure, graph is needs to make the visualization. And we're kind of trying to couple that a lot more. This is a more advanced example, so don't feel bad if you can't follow this one. But here, what I've done is I've done a question answer is a question and an answer, and the answer is a list of facts. And what a fact is, is it's a fact as a statement and a substring quote from the original text. I want multiple quotes as a substring of the original text. And then what my validators do is it says, you know what? For every quote you give me, validate that it exists in the text chunk. If it's not there, throw out the fact. And then the validator for question answers says, only show me facts that have at least one substring quote from the original document. So now I'm trying to encapsulate some of the business logic of not hallucinating, not by asking it to not hallucinate, but actually trying to figure out what is the paraphrasing detection algorithms to identify what the quotes were. And what this means is instead of being able to say that the answer was in page seven, you can say the answer was this sentence, that sentence, and something else. And I know they exist in the text chunks. And so I think what we end up finding is that as language models get more interesting and more capable, we're only going to be limited in the creativity that we can have to actually prompt these things. Right? Like you can have instructions per object, you can have recursive structures. It goes into domain modeling more than it goes to prompt engineering. And again, now we can use the code that we've always used. If you want more examples, I have a bunch of examples here on different kinds of applications that I've had with some of my consulting clients. Yeah, I think these are some really useful ones. And I'll go to the next slide, which is this doesn't have the QR code. That's fine, the updated slide has a QR code, but instead you can just visit Jxnl GitHub IO instructor. I also want to call out that we're also experimenting with a lot of different uis to do this structured evaluation, where you might want to figure out whether or not one response was mean, but you also want to figure out what the distribution of floats was for a different attribute and be able to write evals against that. And I think there's a lot of really interesting open work to be done. Right now we're doing very simple things around extracting graphs out of documents. You can imagine a world where we have multimodal, in which case you could be extracting bounding boxes. One application I'm really excited about is being able to say give an image, draw the bounding box for every image and the search query I would need to go on Amazon to buy this product. And then you can really instantly build a UI that just says for every bounty box, render a modal. You can have like generative UI over images, over audio. I think in general it's going to be a very exciting space to play more with structured outputs. Thank you.
