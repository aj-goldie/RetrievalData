Speaker A: Hi everyone, and welcome to the Mlops podcast. I am your host, Dean, and today I have with me Jason Liu. Jason is a machine learning consultant where he pair programs with various ctos to build awesome machine learning applications, as well as doing prototyping, tech strategy and tech writing work. His focus is mainly on LLMs and rag applications. He's also the creator of instructor, which is an open source tool that helps you extract structured data from LLM outputs and also for doing a lot of really awesome things with LLMs. We'll probably discuss a bunch of these later. Before that, he was a machine learning engineer at Stitch Fix, where he worked on LLMs, computer vision and recommendation systems, and I know that he built a lot of really crazy systems as those teams grew. So we'll probably talk about all of those. Thanks, Jason, for joining me.
Speaker B: Thank you for having me.
Speaker A: Awesome. So let's dive into it. What are you most excited about in the world of machine learning and AI right now? There's a lot of things to be excited about, but what's your topic?
Speaker B: Yeah, I think the biggest thing right now is that we're almost only limited by creativity. I think there's been a lot of apps I built earlier this year where I wish I could build them for the past five, six years. Journaling apps, automatically generating transcripts and all that kind of processing work. In the past, I never built them because, one, I didn't really want to learn front end code, and the NLP systems at that time were not sophisticated enough to do that. But with the admin LLMs, they are both the programmer and the backend that I use to build these systems. Now, what this means is instead of trying to build a v zero product, to try to bootstrap my dataset out to train a language model, to train a machine learning model, I can use a machine learning model to both build my product and serve that product and then sample data afterwards to improve it. That's been super exciting to see it. I think that shows in all the different startups that have been showing up, both in the infrastructure side and on the consumer side.
Speaker A: Yeah, that's definitely interesting. And when you're, when you're saying like, use it as a backend in front of developer, do you mean like in the sense of pair programming, in the sense of GitHub Copilot, or in the sense that the models are much more accessible and easy to incorporate into different applications, or both?
Speaker B: I would say. I would say both, right? Like, I had built a journaling app that I wanted to build in like the past five years. And, you know, most of the front end was written by the language model, the backhand was written in partnership with the language model, using copilot and the actual infrastructure that serves the recommendations and does all this conversational stuff also put a language model, and that only took me, I think, four weeks to build, whereas in the past I had to manage the Twilio API to send texts back and forth. You have to figure out rule based systems. You're building a state machine, and now the language model just handles all of that really easily.
Speaker A: That's very nice. And do you use a self hosted LLM for that, or are you using chat GPT or something else?
Speaker B: Yeah, I'm just using g four for like almost everything now. Like, the latency is getting a lot better and just having something like, I think Telegram is really all you need to have an interaction with a language model.
Speaker A: Yeah, that's true. I see that they're like, I'm still blown away by the speed of experimentation that OpenAI is doing at their scale and usage. I opened chat GPT this morning and I don't even remember what the thing I typed in was, but it was so much faster than it was yesterday. So it's definitely getting to the point where you're at pretty much near real time for anything that's not like really real time application, but you can wait a few seconds, then you get everything that you need very easily. So I tend to agree with that. And then did you use copilot or AI in building instructor, or was that all you?
Speaker B: I would say instructor was basically all me because it's only like 200 lines of code. I think a lot of it there was mostly thinking about how to make the API as simple as possible. I will say the, the. Oh, sorry. I would say GPT four has helped a lot in running the documentation for instructors. So I have a couple of custom cpts that are prompted to help me write documentation for the language model, for the language for the library.
Speaker A: Interesting.
Speaker B: So I will give it a code snippet and it will produce a blog post or documentation that I can edit. That's been super helpful.
Speaker A: That's a nice use case. Are you automating that or is that like every time there's an update, you send it to, to the model and just paste in the response? No.
Speaker B: So what I usually do is I will code up an example that I think has some kind of educational value. I'll save that in the examples folder. I'll copy paste it, send it to my GPT and it will come up with the documentation version of that code snippet. So it will break it down, give an introduction, give it a conclusion, and try to motivate the example.
Speaker A: That's awesome. Yeah, I did that with dags, but then now I'm testing it. We have a chatbot deployed to our discord channel where if someone asks a question then it responds first and I'm trying a different version, not locally, but I guess in my personal chat JPT account. So every time the current bot gets a question I copied into mine and see who does a better job. I think mine is winning, so you probably need to deploy it. But the copy pasting it works. It works, and it's scalable enough for these applications. So maybe taking a step back, can you tell me and the audience what instructor is and maybe how you got to make it?
Speaker B: Yeah. The general premise for instructor is to be able to steer language models with code. In particular, in Python we use pytantic. Pytantic is a library that can use classes with type int to generate JSON schema and then validate that JSON schema. So if there's certain errors, like you want numbers to be greater than zero, or you have custom functions that run these validators, these error messages can be parsed by the language model and then used to correct outputs. Now, instead of writing prompts, that feels like you're writing code and text edit, you're actually defining classes and models and validators and then passing that to language model. So it feels more like programming and making things backwards compatible with code rather than trying to jump into the future and just write paragraphs and paragraphs of prompts.
Speaker A: Interesting. And so when you sort of built this, what was the main use case that you had in mind?
Speaker B: Yeah, so a lot of the work I had done at stitch fix and with a lot of my clients at that time, I would say in the summer, was around query understanding and breaking down search requests into better written search. So if I was to say, tell me what happened last week, or compare and be a language model, can't really take that embedding and make a search last week does not embed the things that happened last week, but min date equals today minus seven days, that works just fine. What instructor was built for was to do a lot of this question and query decomposition. And so you would get a list of objects, and each object has a rewritten query, a minimum date, and some additional tags. And then we can use that to query full backend system interesting.
Speaker A: Is this something that is broken down in some way that's very intuitive to the user? Or do you have any tips on how to think about when something should go to a prompt versus when something something should be logic?
Speaker B: I would make it as much logic as possible because we know how to reason about code and reason about systems in ways that we have not figured out how to do that for prompts. If I have a list of search queries, the search query object looks exactly like the input for my search function. And so this kind of like connection, it makes it really easy to reason about these things. And then when you update your function to add another attribute, you get a little nice squiggly line that tells you that there's something missing in the class, right? And now not only is your code for the language model that coded for your IDE, the IDE also uses a language model. And now you're working in a system that is very like deterministic with the type system, but also very fuzzy with the ability of using a language model. Prove how these things operate.
Speaker A: Fair enough. So if I'm trying to formulate some set of instructions for this, you'd say start by trying to formulate this in code, and then if you can't, then maybe send it as a prompt and work with that.
Speaker B: You should think of instructor almost like a magical pipe operator. We have data and instructions, and then what you get back out are the arguments to other systems, systems that already exist or systems that you want to build in a deterministic way.
Speaker A: And do you think about maybe to the second point that you just made, like the system that you want to build but haven't yet? Is that sort of where the LLM could fit in? Like you can tell the LLM to behave as a part of the system that's not yet built and get some response that's pretty close to what you want and things like that.
Speaker B: I mostly think of it as a boundary in the same way that you can write tests for functions that don't exist and develop against that. You can define the inputs to a new system that isn't built. But the first thing that you would do is you would build the instructor function that map your to the expected outputs. The first set of tests are just making sure that the probabilistic text data map to deterministic structured data. Once those pass, then you can go build the system that takes in that structure. And that system is really simple to build because you're just taking in python classes and returning python classes as you would expect normally.
Speaker A: Fair enough. One thing that you showed me in a previous conversation that I thought people might be interested in hearing about is using instructor as a way to validate qualities about your data using LLM. So maybe you can share a bit about that.
Speaker B: Yeah, so one of the things that differentiate pedantics to other libraries or tools like data classes, the ability to define validators. And validators are just kind of tight types of superpowers where you have different functions that can check the properties of a value and then mutate that value or throw an error message. So a simple example could be making sure that the age property is zero, right? You can define a simple function that says if you know if the value is greater than zero, throw an error message that says the age must be greater than zero, otherwise return the value. You can do other things that title case, a string or whatnot. But because these are just generic functions, you can also just use language models, as you would maybe call a database or make another post request. And so by doing that, you can define more complex validators that can do things like content moderation, or verify that the chain of thought is reasonable for the answer that you gave, or verify whether or not the citations you use for an answer match the actual original.
Speaker A: Yeah, so I think that this is probably going to be a thread that we'll discuss in other podcast episodes that are upcoming. But I think that one of the points that come out of this, at least for me, is that this is a mental model shift and relates to the point that you were making earlier about the change that LLMs bring to the world, which is that you can now, in the use case of validation, you can think about LLMs as more than just a chat interface or some smart model that you can ask questions of. You can actually have it role play, in this case, a person, maybe, that's supposed to validate your data. So a lot of companies I know, I've spoken to a lot of people have part of their pipeline, which is human, in the loop where a person needs to look at their data and say that it makes sense according to some business logic. So that person can now maybe be incorporated into the automation pipeline via something like GPT or any other LLM. And so thinking about the role you'd want a human to play within the pipeline of data validation and then trying to delegate that to the model, I think is sort of what this unlocks, or at least demonstrates really nicely. So, yeah, I don't know if that correlates to how you were thinking about this when you built it.
Speaker B: But yeah, yeah, I think the biggest thing really is that once we bring all this magic of the language model back into just validation, data modeling, it gives us language that we've used for many years already. There's ideas like constitutional AI, but constitutional AI is verifying that the response of a language model follows certain principles. To me, that's the same thing as validation. That's the same thing as verifying that the password one matches password two, or verifying that there are no swear words in a username when you set up a gamer tag, it's very reductionist. But what it actually gives me is the ability to work with these systems, as I always have. We know how to handle database migration and schema migrations. We should be able to handle data model migrations for the language.
Speaker A: I think that's a really nice mental model for these things. I think in general, one of the things that struck me when we first spoke was how you think about UX in general and for the areas that we're interested in specifically. So if you can share maybe some thoughts about building good user experience for. Let's start with ML tooling packages, like instructor or other packages that you're interested in sharing your thoughts about.
Speaker B: Yeah, I mean, this comes down to a lot of the times when people ask me about instructor, they think of like, what is the roadmap? What are you going to build next? I think right now my main goal is to build a good knife. What you want to teach with the library, what you want to teach with the blogs, really is just like how to write better code in general. Those are the skills that you can take to any problem, whether or not it's language model specific. What I'm not trying to do is build out this knife, then sell you the kitchen, then build out a restaurant. I think a lot of it really is just getting out of the way of the developer and giving them better ergonomics on using these tools and then leaving the frameworks and all that kind of stuff to sort of settle down as the ecosystem matures.
Speaker A: Interesting. But then, I mean, maybe you don't have this aspiration, but like instructor could or could not be part of that future ecosystem. So is that something that you even think about? And if so, like, how does that impact the decisions that you're making on what to build or not to build?
Speaker B: Yeah, I think of less of what to build, what not to build, and more about what are the design patterns that are worth teaching to new developers. I think the introduction of language models makes something like Python more accessible to a lot of people. Again, the goal really is just to teach good programming and using instructor as a wedge to do that education a little bit better. I'm less worried about making a product or any kind of SaaS or enterprise business. Let's just focus on teaching python. Really.
Speaker A: I like your educational leaning. And then if we, maybe if we extrapolate or not extrapolate, if we move over to applications that are based on LLMs, how do you think about that? I guess I shared a few thoughts of making the task something that you'd give a person to do and trying to do that with the LLM. But what is your sort of user experience thoughts around LLMs?
Speaker B: Yeah, I think the biggest, I would say the most important thing these days is that I think everyone already kind of knows how to work with LLMs now. They understand that the prompts are very important. They understand that it's fine tuning data. I think the biggest thing that people are still missing is really thinking about how to collect that feedback. Language models gets you a product day one. But the issue is, unless OpenAI is changing a model, or unless you're swapping out language models every time a new one comes out, the way to improve these things are still very unknown. And so a lot of what I think about for the application layer isn't necessarily language model, but more on how are we collecting feedback data to improve these models in the future. Today you might want to use GPT four next year. If GPT five doesn't come out, you still want to improve your product in these ways. Collecting that feedback data is what's most important.
Speaker A: Yeah. The one thing that I've seen when talking to people that are building out applications or looking at the LLM applications that were, I would say, playing around with or not really deployed at scale in production or something like that, is that a lot of times when you look at how people actually use this, then you realize that there are a lot of improvements that you can make that are not directly tied to the actual details of the LLM. And that's being overlooked, I think, more here than in other cases, because it's such a powerful tool. So it's like you have a hammer, everything seems like a nail. But actually if you invest a bit of time in figuring out what the details of your use case are and how to wrap this very powerful tool in a way that's dedicated to your use case or your users or something like that, then you get orders of magnitude better results.
Speaker B: Yeah. I think a lot of people really, at the beginning of this year thought that the LLMs and these embeddings could do everything, but they're finding that it's not the case. But really, I think what that means is even additional amount of any kind of thinking and good design can take you even further as a result, because everything else has been handled.
Speaker A: Yeah, that's true, I think. Well, I guess we'll talk about rag in a moment. What do you think most people, we talked about one thing, I guess. What do you think about most people get wrong about LLMs?
Speaker B: Again, it goes back to this idea with instructor, which is that the LLM is not this special class of models. I see the LLM the same way someone might think of a database or some of trash in over a data center. This is just an accessible way to access some kind of special computer. Again, to me it's just code. And so being able to just think of building the system as a good design, good designer, a good programmer is really all you need to build a good system. You have to be intentional and thoughtful how you model data. You have to have good design and develop good user experiences and have good feedback, and everything else, I think will flow as a result. I think too many people think of this as a very special class of problem and, you know, elevate it higher than they really should be.
Speaker A: That's interesting. I guess one follow up to that is, do you think that the reason they aren't a sacred class of models is because they are so multimodal? Because I can imagine if you're trying to put, if you're trying to work with instructor and use classical CNN computer vision model, then that's not going to work in most cases because most of the data is not formulated in that way. I'm curious if multimodality is what finally unlocked this being another application.
Speaker B: Not too much. If you think of the type system, it's still very simple. Databases can store floats, int's and lists, and there's relationship across these things. The language model really is just mapping the type of all strings and all pictures to a more structured output. Most language models just map string to string, which is a very primitive, very primitive type system. My post requests returned a piece of text. I'd be annoyed by that. But with something like instructor, you just get structured data back out. But to me I just think it's a type system where these functions are a little bit more magical.
Speaker A: Fair enough. And if you'd have to sort of decide what to work on next, or what you see as the biggest problems when you're looking at these, let's say, specifically in the LLM ecosystem. But what are the main things that are unsolved?
Speaker B: I think the biggest one, in terms of both infrastructure and training, is how to capture feedback data. How do we build a product or a system where the interactions themselves can improve the model? For example, if you consider Netflix, we know that every time you watch a movie, every time you click a movie, Netflix is learning that and being able to improve. Netflix is even able to experiment. On the thumbnail, they show you to figure out which movie actor is going to be the one that you click on the most. But with the language model scene, it's really unclear how we get any feedback. Having a conversation with a chat agent, how do you tell it that it's doing a good bad job? Right now, all we have is a thumbs up. Thumbs down. Yeah. Do we optimize for the number of time spent? For example, character guy right now has a two hour time. Is that the right objective, or should it be the shortest amount of time? Because we're actually trying to do some kind of question answering. It's really unclear how that bakes into the product and bakes into the training data of that model. The second thing really, is the network effects. You consider the Netflix example. If we watch a similar movie and we both like it, and then you watch a new movie, there's a chance that I might like that movie. And there's collaborative filtering aspects. And as there are more users that are going onto this platform, the platform improves. But in the language models, again, it's unclear how these things work. If I have a conversation with a language model with a certain style, it does not really improve your interaction with that language model. And so figuring out the design mechanism that can inhibit introduce these network effects, I think are also going to be a very powerful interaction pattern.
Speaker A: Interesting. So, on the first point, I gave a talk about evaluating LLMs, or I guess customizing and evaluating LLMs. And part of the points I was making is similar to what you were saying. I'm curious if you think that the problem is more on the product side of like, in Netflix. The idea is, like, if I'm watching something, I don't need to thumbs up it, right? Like, if I watch something to the end that I probably liked it. In the example that you gave for a chat bot, maybe it might be the amount of time spent. I thought one of the things that I thought was really nice is how many people click the copy button. That sounds like a good correlation to getting value out of this thing, how many people share, of course, and things like that. But I'm guessing there's that layer, which I'm guessing is going to be very specific to each company. And maybe there's also, I don't know, an instrumentation layer for that, something like segment for AI that maybe needs to automatically collect this information. Would you say that there's equal problems in each one or that it's more one than the other?
Speaker B: I definitely think it's both. I think this segment for AI is a very clear example where even in systems, like in eighties agent systems, we don't really have a good way of seeing the entire pipeline of how these things operate. Just because it writes SQL doesn't mean it's actually going to be correct. SQL, if you copy paste that and you get an error, can we make sure that the error message propagated back to the language model? Maybe the runtime is very slow. Could the language model have written a more efficient query by understanding that the joins were done incorrectly? I think there are tons of different ways of improving the systems that people are not really able to capture right now.
Speaker A: Interesting. And your bet is that there's going to be network effects for LLMs? Because I would better get to that. But I'm curious.
Speaker B: I mean, I think if there's no network effects, the product you build is probably just not as right. You could imagine a world, for example, that. Let's consider one of my clients, actually, I work for a company called narrow and they do a lot of sales automation and rag against sales. And one of the things we work on is being able to extract snippets. Sure. So when you attach a new file to an email, how do you talk about this piece of content or this piece of marketing? Well, for narrow, one of our goals is to be able to identify the top performing snippets across your sales team. So even if you are like the worst salesperson and you attach a piece of content, we're going to introduce for you the best piece of content for that response and the best way of framing that content, you could argue that's a network effect. As there are more sales people on your platform, the overall performance of every individual will now improve.
Speaker A: Fair enough. Yeah, I guess you can imagine. Yeah, sorry, go ahead.
Speaker B: You can also imagine an example where maybe you're doing like legal clause search where when we pulled in data, there's a thumbs up, thumbs down system. And so as there are more like lawyers are using the platform, maybe the retrieval is getting better. And so as you get more users on board now, there's more personalization, more accurate searches, and again, that's some form of network effect.
Speaker A: Yeah, I think that the way I was thinking about this is that's sort of a network effect that's very tied to the, just the data, right? Like if you have more data, then your product is going to be better. Maybe this is sort of the standard definition of a network effect. NML, if you have more data, your product is better than you have an ML network effect or something like that. But yeah, that makes sense. I think that there's a lot of things that are going to be harder in generic models. Like if you're using chat GPT, maybe I'm using it as a marketing person, some other person is using it as a sales, so others is chatting with it about building ML tools or whatever. So generalizing from one to the other is going to be much harder. Unless you do what I think OpenAI is doing, which is a mixture of experts, then maybe for specific tasks you get the network effect in a subset. And that works within companies, like the example that you're giving. That makes much more sense to me. Every company is going to have their own jargon, their own products, their own things. And so you have to, or you're probably going to want to adapt the model to what they need, and as they get more data, that the quality is going to improve significantly.
Speaker B: Right, but that's a conscious decision from the product and for the company to include that training data. Right? Like I can very easily imagine a world where if you just have a simple transcript summarization prompt, you can integrate that with Zoom all you want. But from today and a year from now, that model isn't guaranteed to improve just because you have a language model in the hook. You still have to make that decision to collect that data and figure out what is the right way of interacting with these things.
Speaker A: Yeah, I agree. And I asked one last question about this is in your experience, have you already seen a case where people have done more than prompt engineering plus rag to customize LLMs in a real production application thinking about like parameter efficient fine tuning or lauras or things like that?
Speaker B: Not in my experience. I think for the most part there's still like so much room to prove your system just by doing rag and just better pumped engineering. I think most of the fine tuning will likely come from fine tuning embedding models to improve retrieval. But for something like GPT four, the writing and the reasoning is good enough that a lot of the improvement there will end up being just better prompting.
Speaker A: Fair enough. That makes sense. Let's talk about rag. One thing that we discussed is that you have this idea where rag is very similar to recommendation systems, and I'd like to share this idea with the audience as well. So if you can elaborate on that.
Speaker B: Cool. So I can almost go over a little bit about what stitch fix did and why I see this comparison with what stitch fix did in particular and how rag works. So with Stitch fix, what you did was you would send a request note to the company. He's like, hey, I'm going to a wedding in Miami. I want to spend some time at the beach, then I'm going to Alaska. What we use language models for in that situation was a process that request into a set of search queries that we can use to find clothing. Then we had a stylist take those recommendations, build you a fix, and then write you a note that tells you why we picked these things. So we might say we got you some a warm base layer for your trip in Alaska, but because you're going to this wedding and go to Miami, here's a bikini, here's a pair of shorts and some flip flops. And so what we did was inventory, filter that inventory. We then made a diverse, for some definition subset of that inventory, and we sent that back to the user with some note. That's effectively exactly the same thing as what rag does here. The inventory of the text chunks that contain the answer. The sourcing of those text chunks come from embedding search, from re ranking, from all these different filtering tools. Generation is just taking in these text chunks, putting them into prop, and then asking for an answer. And then again, the shipment of clothing we send you is exactly the same thing as the answer we have with the citations of where those text chunks came from. And so when you buy something from us or return something from us, that's the same thing as giving us feedback or whether or not the answer was correct, or whether or not the citation was relevant for the question. If you think about the two systems, they're actually very much in parallel. And sort of a lot of the things that we've learned from building out systems as districts, I think, really apply to the rag application role.
Speaker A: I really, really like this. I think a lot of people are going to find this useful and thinking about how to solve, let's say, deeper problems with a rag application. It gives you a domain that's already a lot of work, has been put into it. There's probably a lot of learnings that we could apply. One question I wanted to ask you about, Rag, is I think a lot of times in the past year when people mention this, they only think about embeddings and vector similarity as the sort of ranking methodology for selecting which information to use. Is that the same way that you think about it, or do you already, or are you already in position where it's clear that vector similarities a any type of query among many that you need to do in order to decide what to use?
Speaker B: It's funny, because I had been building these embedding models and vector database search stuff in 2016 and 2017, and the reason I had done that at that time, because I was just an intern, the vector embedding work is the lowest hanging fruit. And to me, the easiest thing. The hard part of search is actually everything else. I think this year, vector databases have become more popular, but I think that upcoming years, people are going to realize that this is, to me at least the equivalent of an intern project that can be done in two to three months. And the hard work begins when you build out more opinionated search. Vector Search really is a fuzzy way of doing any kind of retrieval. And oftentimes there's just very specific categories of filter that we wanted to apply. Right. Like we could embed the word blouse and search that. But there's still a chance that, you know, of all the things that show up, there are going to be things that look good with a blouse. And so the recommendations that we get aren't all blouses in this example that we have a citrix. Right. And that's going to be the case across many, many different kinds of circuits. Yeah, I think in the example I gave with using instructor like that was the example of going beyond embeddings. Not only do I rewrite the query, I can pass in things like relevant dates, different keyword filters, and Boolean logic that cleans up the search to produce something that is actually truly useful.
Speaker A: Yeah, I think that's a really important point. I think that the way I tend to think about vector search is it's like associative memory. So you hear something and then what are the first few things that come to your mind? That's basically, I think it's a good analogy for how this is working. And of course, if I throw a word at you and then you think about things, not all of them are going to be the context that I might have intended when I gave you that word. And so those sort of later filterings make it on topic for the user. And I think that that's one of the things that I also really liked about the demo that you gave me for instructor is that to me that's the most immediate use case. You're able to both do regular filtering, but also use the LLM to ask further questions about the results to get the most specific thing that you can to the user. So this actually ties in also the UX thing. But yeah, I think it's a really useful analogy, and I think we should repeat the mistakes of the past of thinking that we're reinventing the wheel when there's a lot of things that have already been done in this space and we could just use them instead of reinventing them.
Speaker B: Yeah, I think it goes back to this idea of like, these language models aren't really like a sacred class of models. All these things have their backwards compatible frameworks and language. We just use that language. It actually makes models less magical, but it also makes everything that we're doing a lot more tractable. There are years and years of retrieval, like information retrieval and recommendation systems research, and how we can just easily apply them to the language model case. One thing I do want to call out in this embedding example is that a lot of people are not fine tuning embeddings. A lot of people are using the embeddings from hugging face. And what this means is that the actual objective on what to recommend is not clear. For example, eta two cannot figure out this comparison. I love coffee and I coffee. Should they be similar or different? If it was a dating app, they should be very different because they're specifying the negation of a preference. But you can also reasonably think, well, this is just talking about food preferences, and so they belong in a similar space.
Speaker A: Yes.
Speaker B: So again, just the product that you use and the data that you end up collecting should really go and inform how these embeddings are being produced.
Speaker A: So can you maybe let's dive a bit into that, because I think that would also be useful to a lot of people. How would you go about fine tuning an embedding model at a high level?
Speaker B: The general idea is embedding models learn by doing this thing called contrastive loss, which just means that we want to push things together when it makes sense and we want to pull things apart when it don't make any sense. And so you can imagine a world where if you just have a question with some retrieved documents, you could say, well, everything I cite that's relevant. I want to push those embeddings closer together. And the things I don't cite, I've got to push them apart. Or you could, you know, hire some bunch of people to label some data. Or you could, like, use the language model to tell you what is similar, what is dissimilar. But ultimately, the data that you want should be in the form of something like this. You have a piece of text and you want to have examples that are similar to it for some definition. So in your dating app example, maybe negation of preferences would be dissimilar, whereas the similar preferences might be similar. And that, again, isn't really determined by you, but determined again by the users of your product and the feedback that they give.
Speaker A: That's awesome.
Speaker B: For a dating app, it would just be the people you swiped left on versus the people you swipe, you know, write on, from movies to things you liked and the things you didn't like. There's analogies in every single dataset.
Speaker A: That's great. I think this would also be useful. And I guess, do you have any specific recommended reading on this topic? Because I think a lot of people are going to be doing this a bunch now.
Speaker B: Yeah, I would say take a look at all the great work that, that the sentence Transformers team has done. So you just Google sentence transformers and look at their documentation. It's very good. And goes over a bunch of different examples of how we can do this kind of fine tuning. And mostly just understanding that the architecture doesn't really matter too much. What really matters is the training data that you have.
Speaker A: It all goes back to the data. There's no escaping it. Everyone knows, and I feel like I'm shortening the timeframes with every episode that I record. But everyone knows that in the world of ML and AI, a week is like a year and a month is like a decade and things like that. But if I were to press you on it, what would you predict for the next year in AI and machine learning?
Speaker B: I would say the biggest one is that things like transfer, learning, and fine tuning will become even more accessible. I wouldn't be surprised if in the next year, 30% of all of the GPT calls are called to fine tune models rather than generic models.
Speaker A: Interesting.
Speaker B: I think that would be one of the big bets that I would take, and whether that's a laura from a local model or anything like that. I think these companies are going to get to a point where they are able to collect that data and actually train these task specific models. Because many of the times we don't need general intelligence, you need to do something for the problem we're solving.
Speaker A: Interesting. The subtext of what you're saying is also that maybe the landscape will become more distributed with respect to applications. Is that also correct to say, I'm.
Speaker B: Not sure really distribute against what we distribute against these different fine tunes, but my money is still on something like OpenAI just being 90% of all the LLMs.
Speaker A: Interesting I calls, but then OpenAI doesn't. I mean, they offer the GPT's functionality, which is great. I use it. It lets you provide context, it lets you provide guidance, but it's not something that you can interact with in code. Yet. Do you expect them to release something like that, or are you saying that that's going to be enough for most use cases?
Speaker B: I see. I would say like, I'm less bullish on the assistance API that they have that came out recently, but I'm actually more bullish on the fact that once GBD four fine tuning becomes available, that as you get user data, it just becomes more and more accessible to just fine tune GPT four rather than worry so much about the prompt.
Speaker A: I see. Interesting. So also, I guess the takeaway from this is that you expect the ratio between prompt engineering and fine tuning to change, because now it seems like 98% of things are prompt engineering, right?
Speaker B: I would hope so. I would hope so. Prompt engineering makes sense when you have no data, which is why people keep talking about evals and what their evals are. But once they actually have objectives, they know exactly what they're training against. And these general benchmarks will matter less because these general benchmarks don't capture the internal benchmarks you have around your business. And I think in the next we're going to be really capable of just saving all this data and then fine tuning models that works specifically for them. And these conversations around these large benchmarks will matter less and less.
Speaker A: Fair enough. Do you think that how is the evaluation landscape going to look then after this?
Speaker B: I think it will just become less around the evaluations against large benchmarks and specifically around the data sets build. I don't think when Netflix has an a guy that is writing summaries for movies based on the transcript, I don't think they really look at the hugging, face embedding benchmarks or human eval because they have evaluations against neither will Amazon or Facebook, any of these companies. And I think in the next coming year or so, even the smaller companies will have their own internal evals and they'll worry less and less about these larger ones.
Speaker A: Yeah, I think our bet, or my bet is similar in that sort of everything needs to be more specific, and that means that the bigger evals are not going to cut it for most people and that the evaluation, like, you shared sort of your thoughts about things that are still not unsolved. I think that capturing feedback and evaluation are correlated, but I think that evaluation feels unsolved. Like, it feels like we're showing off on a lot of benchmarks that are really hard to translate into the end result. I did this test on myself, this experiment, and I built a test set for prompts that I thought were useful to our users that are asking for support on our platform and things like that. Then I asked both GPT 3.5 and GPT four to give me a response, and without knowing which response came from where, I ranked them, and they were closely tied, which is very different from the experience when you're chatting with a model and there's a back and forth. Right. Like most people, I've not met a lot of people that have tried both models and like, yeah, they're more or less the same. Like, GPT four is. Is meaningfully better, but with these evals, it wasn't very easy to say. To say that. So I think that being able to evaluate, to evaluate these models in a way that's much more correlated to what we actually experience in the end is something that needs to. Still needs to be solved, and I'm hoping will be solved in the next year.
Speaker B: Yeah. To add to that line, you had to like what we actually experienced. In the end, what we experience isn't even the EVA. If we're thinking about building a business, right. I think a lot of people will come to me and they say, oh, we want to improve our rag system. When someone says that, usually what I hear is, we want to improve churn or more truthfully, want to improve revenue. And people don't really have an understanding of how these evaluations actually are predictive of the business outcomes they want to drive. We talk about latency when talking about price, but really, price doesn't matter. You're actually driving a business impact. But it's still very unclear. And I haven't seen any research come out from any of these big companies that say a 30% reduction in LLM response time improves revenue by 1% in conversion of a shopping chat. No one has really published any kind of research like that. And again, it goes back to this idea that there has been many times where I've worked at companies where we improved the ad click rate of a model by 1% and actually see a large improvement in the revenue of that model. And again, that goes back to this observability pipeline of, can we even figure that out? Can we figure out, and can attach every single ll response to a churn metric, a lifetime value metric to an average order value metric? Because those are the things that actually, right, like you don't want to have a chat bot that's trying to do sales and optimize for the number of back and forths. That's like time to conversion becomes lower. And so I think all these things that these businesses will wake up to in the next year and realize that, oh, wow, what we should really be measuring, measuring impact against, is the outcomes, the business outcomes, rather than human eval or like MMLU.
Speaker A: So with that sort of, I think, not controversial take, but probably some people will say that that's controversial. Tell me something that you think is true about the state of machine learning and AI, but that few people would agree with.
Speaker B: I think if you've been doing much for a long time, nothing much has changed. I think the only thing has changed is the order in which you build your data flywheel. It used to be you build a product, you get users, you collect that user data, and then you build a model that improved that system. All we've done in this pipeline is moved the last step to the first step. But this is a cycle, this is a flywheel. At any point in that flywheel, you can't really tell where you started. For people who've been building this machine learning systems that are trying to drive a business outcome, it looks the same as every other problem. People really want to make this LLM very, very special. They want to do LLM for X, LLM for Y, but really it's always just been machine learning for x, machine learning for Y, which is really just good x and good Y, and you still just have to build a good product. And again, if we just demystify all of this, we can go back to good fundamentals of how to build things.
Speaker A: Fair enough.
Speaker B: I guess the second thing that I really talk about as well is, I think just that, I think more economic value will be unlocked through structured data extraction and things like that. You're training a language model to communicate with system, like a software system, rather than building these chat bots that humans. Yeah, I think making that work really, really well will also just be hugely beneficial for all the things that we're doing in the day to day.
Speaker A: Yeah. I think that this might actually be, like, a controversial take, but I like those, so I'll risk it. I think that the, like, if you think about economic value for first principles, right? Like, if you invent a new activity and you create a product that's really good at doing that, that's cool. But it remains to be seen what that value of that new activity that you invented is. You can build the best new gadget that does x, but if no one cares about x, that it doesn't really matter in any sense. What we know has a lot of value is all of the things that we're already doing and being able to automate them, or not automate them in the sense of, like, reducing the people that are doing them, but just having more people doing that in real time. And I think that that also ties in. A lot of these things are tying in. Into this conversation. It ties into, like, letting LLMs role play in different things, because when they do that well enough, then you get the actual ability to do more with your time and. And more things that you actually want to do and not, like, cool things that you wouldn't do before, like asking a computer to write and illustrate whatever. Children's book. But I'm not sure if there was a huge deficit in children's books. That's like, the biggest application that we need, even though maybe. I'm not sure, but, yeah, sorry, I want to say something.
Speaker B: It was very much of the quote, I forget who said this, but really, this idea that, like, you know, Ford introduced the automobile, but the innovation really was the assembly line.
Speaker A: Yes.
Speaker B: I think what these language models are doing should be more around improving that assembly line and building a good assembly line, rather than trying to reinvent the car, for example, because that's also where the structured output really comes in, where a lot of that is just making these computer systems a little bit more robust or more scalable or more reliable.
Speaker A: I like that. So let's end with recommendations. What do you recommend for the audience? And it does not have to be related to data science, machine learning. It could be. Oh, man, Netflix shows that you like.
Speaker B: Oh, that's a hard one. I didn't really think about this. I would really just recommend people use products, like, use good products and really think about why they're good and the different ways that we can improve these systems. I think once you just start thinking about how different systems can be improved, whether it's using your laptop or driving a car, we can see these patterns occur everywhere. Once you get the taste for trying to figure out why something could be better. We can bring that back into the stuff that we do and make our systems a lot more interesting. Would you just say, do a bunch of things and complain, and then once you do that enough times, will really figure out why you like something, why you don't and apply it to your own life?
Speaker A: I'm a big fan of complaining. I think that there was a Steve Jobs quote about realizing that the box that you're in was built by people who are no better than you and starting to question whether or not you can change that structure and the world around you. I think that it's sort of a product thinking approach where the things that you take for granted, you probably shouldn't, and they might be able to improve, and you might be able to improve them personally. And then taking it to the things that you actually work on, where you actually have the influence to improve them right now and applying that, I think it's very easy to get into the mental model of like, well, I'm not making an impact on anything around me. Might as well just, you know, live the. Live the things, live the life that I. That I have and be fine. This is getting philosophical, but the moment you start questioning these things, you're able to do much more and make a much bigger impact. And I guess now is the best time to do that. When you have LLMs at your service, you can shorten the time to build significantly.
Speaker B: Yeah, and a lot of it is just doing the reps and working on developing the language. You need to describe some of these problems. Right? If you complain once, you just sort of have an internal feeling about it. But if you do it enough times, you develop language and you develop taste, and those are the things you can actually do and take and do productive work.
Speaker A: That's awesome. That's a great recommendation. If people want to follow you, where should they do that? Yeah.
Speaker B: So if you want to follow me, you can check me out on Twitter jxnlco. I would say 70% of it is like machine learning stuff and there's 30% is just random stuff up in my life. Try to keep it a little bit more personal. And yeah, check out the instructor blog. The URL is kind of complicated, but I'm sure you'll put a link somewhere in the bio. But yeah, check that out as well.
Speaker A: I definitely recommend following Jason on Twitter and also reading the instructor blog. I found it really nice. The code examples are very clear. You sort of very quickly understand the magic in it and then try it for yourself. And I think it's going to be very useful in teaching people how to think about these making this unstructured thing much more structured and much more useful in many other use cases. I have high hopes for it.
Speaker B: Great. Thank you. Thank you.
Speaker A: Sorry, go ahead.
Speaker B: Oh, yeah, no, go ahead.
Speaker A: So thank you very much for taking the time to be a guest on this podcast. I had a lot of fun talking to you, and I'm looking forward to the next time I get to have you on.
Speaker B: Thank you for having me as well.
Speaker A: Okay, so that's it for today. Bye, everyone. Thank you for listening to the Mlops podcast. If you enjoyed this episode, please like the video or subscribe. If you have any feedback, leave a comment below. Thanks again for listening and see you next time.
