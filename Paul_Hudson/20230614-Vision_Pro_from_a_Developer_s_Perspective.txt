 Last week I had a 30 minute hands-on demo with Apple's new vision pro headset along side a 90 minute run through at their developer center Now I spent a lot of time writing Swift and writing about Swift So all the way through sessions my brain was just fizzing us thinking how's that effect done or how can we adopt that now own apps and so on? So I had to sit down and record my thoughts Yes walking through my experience what I had but also tell you what I think it means for developers and Give you some advice when it takes to get ready your app ahead of the vision pro release I should add to be clear. I'm talking specifically about Vision OS and Vision Pro from a programmer's perspective if you're looking for a mainstream review of the device There were a lot of great folks out there from mainstream press like MKBHD and similar who's on wonderful videos go and check them out instead Before I start I want to say how grateful I am to Apple for letting me try the device give me a chance to really try out half an hour Because almost everyone else who tried it was from mainstream press and as a result I've got a different take right you know I approach everything through this lens of programming You know we've we've seen someone is API's before we know ARKit we know we have to get we know so if you why it's things we know it's things already using and What Apple has done is take his API's we know and extend them greatly in this release, you know SwiftUI now has three D support for example It then ties them all together with extremely close coordination and of course adds an a huge amount of groundbreaking hardware But most importantly polished and polished and polished the whole experience again and again and again and to be clear what we see as vision Pro V1 is to Apple actually vision Pro V 1000 this is so polished right now my demo took place in a building called the field house at Apple Park I was not allowed to take photos inside the building and When I asked Apple's PR folks who were with me for the demo could they get a picture or video of me using the device They said that wasn't allowed either in fact the only picture I was allowed to take was of the field house itself From outside and even then the Apple PR person was escorting me said I had to take the picture from exactly where she was standing They were very precise and I'll put that in the screen in case you're really really curious My understanding is this thing was built the week before dubbed and Torn down the week after dubbed which just blows my mind because it really felt permanent like it always been there Now in the walk to the field house the thing I was just Well, mainly concerned about was how it would work with my glasses right and let's face it a large portion of our industry Where's glass as well so many folks are going to be worried Please don't be worried the device has room inside the headset for magnetically attached prescription lenses it has slot in I left my glasses behind and it was a Really freeing experience for me quite frankly because it felt like temporarily I had regular eyesight. We're none of the sort of horrible extra pressures used to for having glasses on with Oculus or PSVR The demo that they walked everyone through was designed to show off the core features of the device Right plus a few things that couldn't show in the keynote and so that made us started typing through photos seeing how they looked seeing the old style panoramas come to life in that 3d space and Then seeing the new immersive like really immersive 3d videos that are shot with the camera itself built into the headset and it felt actually Weirdly emotional watching some kids. I don't even know singing happy birthday in front of me because you're there and You're in the room and it feels like they're singing to you and That almost Unnerving level of realism extends to the immersive environments too like the Matt Hood environment You see it versus a panorama, but you turn the digital crown right and what happens is It hard to explain that the mountain just kind of leaks into the room It didn't just get scale up bigger and bigger and bigger. It just starts to flow around you like Matt Hood was made of mist and Then just pouring out the picture and take it up all space around you and here's what you see How good the occlusion detection is in their AR kit stuff because it was quite remarkable There was a knee-high table in the room with me and Matt Hood just kind of expanded around it and then over the table and Through the table. I was kind of waving my hands testing out the occlusion and it worked great It was so impressive. I even had to ask one of the engineers afterwards What I seeing my hands waving around or a video of my hands waving around because the quality was so good And I can have questions to the lot about the quality of the screen. That's how good it is And honestly the weirdest thing was when Matt Hood became fully immersive I Genuinely felt a little shiver of cold Right it was so real that I had a placebo weather effect in my brain and it told me yeah Be cold now shiver. This is a cold place in fact the very first time I gave you was this thing We extend your hand and a butterfly flaps around lands on your finger Again, it's so realistic. You want to flick the little bug off That's what it feels like. You know, you know, it's not there. It's that realistic I mean walk through a selection of apps like a Messages and Safari and TV and more what I can say for sure is that the difficulty ramp up for us as developers is really shallow Apps built in both SwiftUI and UI kit work brilliantly pretty much out of the box They put in a lot of work here. They make extensive work here, which means most apps ride a good experience just going great What's more though if you do a little extra work just like your tailor your SwiftUI code for Mac OS or watch your ass or whatever then you get a really good experience It's just small things like a document new glass effect You know, we have like a background of like an ultra thin material into a UI Fine, that's great. But then you glass effect background just for vision OS Because this new glass effect actually has some depth attached to it So you can place a window and walk around the window and see the thick pane of glass holding up your app But a different style entirely also the idea of you know tailoring for light and dark mode Isn't a thing anymore you just don't do that because the UI automatically adapts to your surroundings That's a point of it in the simulator. You'll see when you get the SDK ships and a few weeks from now You can actually set your surroundings to simulate a various number of places and Set the time of day show me the room at daytime show me at nighttime to make sure you white adapts correctly This makes it more important ever to add apples vibrancy effect the things like text to make sure it really stands out Now the whole UI is controlled through eye tracking and Jesse tracking do look we want to activate then make a a small tap or pinched drag with two hands activate stuff and Like I expect everyone else I started out doing gigantic big gestures to control things But in the end I was doing micro gestures in my lap down here next to my hips. It was so much easier to work with I actually started having fun with the UI at this point because I just grabbed a safari window Moving around quickly all of the place in out up down to the door Just to see how responsive it was and also admiring the shadowing effect It beautifully flows around the room when I was like oh the shadows moving around of the table and the floor does it do? It's on such a great job with things like occlusion and wall detection It works really really well, and you know, it's no surprise these API has been around for years now They've been building up for years now and so unsurprisingly apple deliver a great experience with them I know when things were small Like for example text in a random safari website. I was told just pinch a zoom in with two hands That works. You can do that. Of course you can But I just grabbed the window right and made the whole window bigger and it was this huge huge Safari window and it all just reflows flawlessly in safari in real time You know the device has an M2 chip inside. It's just packed with power And apple developer around Tuesday by the way. I got to meet his engineers who worked on VisionOS And I was really curious how they might have to get this kind of performance like I know it's a fast chip right But it's pushing a lot of pixels it turns out that advice actually had a small fan inside to keep it cool Now taking steps so you don't really realize it's happening that means a fan gently blows warm air upwards away from your face You don't really feel it, but they also said this is like brilliant. They very gently play some noise canceling audio for the fans Through the speakers the counter at wherever it's small sound remains the fans And so even if you're in a room that's absolutely silent You'll feel completely quiet. You'll hear no fan noise at all. It's remarkable The developer is also where they said here in some of the various ways There are subtle differences when building for VisionOS Than what you used to on other platforms. For example, if you have a 3d model placed into a swift UI stack What happens is the back of the model hits your plane with your text isn't other images Normally in terms of UI we kind of center things We like centering virtually horizontally But depth though it goes to the back of the thing not the center Which basically is done because what folks expect You know it means we are wondering why half our models are invisible But wouldn't replace them right? We also learn things like how Did I make type? Is subtly adapted on VisionOS things that were A regular font are now a medium weight font are slightly heavier Things are semi bold and now bold just increasing level by a little bit to be easier to read in these 3d environments Apple also said they have a recommended tap target area of 60 points So things are easier to select with your eyes We also learned how at how existing SwiftUI views adapt this new 3d spatial computing environment With so many existing views changing subtly There's a new layout style called an ornament and these ornaments sit in 3d space alongside your views And they avoid clutter by getting out of the way when they aren't being used For example SwiftUI's tab view seen all the other platforms on iOS bottom bar and Mac OS group buttons On watch west swiping pages and tvOS it's a picker But on VisionOS it changes again it becomes this new ornament style here Showing a list of icons across the left of the screen on your iris and the icons it expands Out to show the labels for us icons as well You'll also find that popovers and sheets and context menus are also adapt or To actually become these ornaments which is really really nice In fact, I think basically almost every iPad OS SwiftUI app is on VisionOS And it's extensive support In fact, one piece of advice they were very clear on was to look closely at the desktop class iPad API stuff announced last year If your app already worked great there with things like a handling window resizing for example Then you're in a very very strong position for VisionOS And before you ask you cannot monitor eye tracking at all There's simply no API to do it heck even reading gestures with someone's hands requires The user taking on a privacy notification first and accepting it What you can do is attach a hover effect to controls like if you're done tvOS work It's when a user eye kind of rests on a button it lifts off the background in 3d for example You can't tell what's happened or not happened but at least apply the effect for you when the focus is on it with the eyes If the user does grab permission to read their hands you'll be able to read what they're pointing at You can use skeletal detection with a finger joints and so forth But no-wise, right? Apple is taking privacy extremely seriously here Or just yeah, let's rise If you're wondering, Apple has been very clear on this front SwiftUI is a preferred development environment for VisionOS now They have said right from the beginning SwiftUI was involved in VisionOS And today the majority of the core system apps are built with SwiftUI It's extremely baked into VisionOS which is amazing to see Yes, of course you can use UIKit to build apps in VisionOS But you lose some behaviors, right? You know, placing 3d content in your view means embedding a reality view from SwiftUI And Apple have flat out deprecated storyboards for VisionOS They're very clear, SwiftUI is the best way to build VisionOS apps Now if you have concerns about your app working well or not well on VisionOS Apple have said the intent to offer compatibility testing They'll look at your app for you at the Coop's End of Headquarters somewhere presumably And identify any potential issues on your behalf and let you know what to do to make it work better I can imagine it being quite a stampede for the service So watch out for it being announced Anyway, back to the live demo Sorry, one app we tried was FaceTime Which features the new digital personas Apple talked about in the keynote It works well of course, it's very, very clever, but two things stuck out to me in particular First, the person speaks to you and you can grab them and move them around in 3d space And the voice as they're talking follows their persona around Absolutely flawlessly, the audio is incredible And it's really an experience all by itself The headset uses directional audio position right next to your ears I don't have any AirPods on at all or similar I can hear folks talking to me next to me where I'm sitting And hear perfectly clearly what the headset was saying to me in just amazing 3d space And it meant it was audio was loud and clear for me But I wasn't cut off from folks around me And critically, it was almost imperceptible to folks around me Now, yes, of course, if you're listening to a loud music And someone's right next to you, they're going to hear a little bit Just like having loud headphones on, for example But otherwise, what you hear is just for you But you're also able to hear folks around you perfectly You aren't cut off, you aren't blocked off in the world As you choose to be, because you can connect AirPods to you, to stand to you Part of the device set up, by the way, was scanning my ears and face To figure out face size and shape And I guess part of that also helps with audio calibration Actually, what I mentioned in the device set up is very, very clever When you put the device on, you're asked to make your eyes follow a series of dots around the screen They lift at your hands straight to the screen, in front of you Presumably, to measure things like hand size and visual acuity That's the first thing I'd about the FaceTime call was the audio was just so impressive It sounds amazing I think all by itself is going to offer a whole range of delightful opportunities for developers The second thing was how my caller brought up a free form board I Apple's whiteboard thing with various things on And then you start talking me through the board And that alone, I get it, it's not special Because we used to idea of share play working in our apps But, uh, what I loved Like, really genuinely clever stuff was how my caller was able to turn towards the free form screen While talking to me about it, and just gesture towards it As you can see here, the dudududu What Apple's doing here is quite magical Because they've ensured that everyone in the FaceTime call Use the same projection transform when using share play content So everyone looking at this board faces the same way It is such a necessary pointless gratuitously amazing detail I love it, it just, it works so well, it's unreal Uh, towards the end of the day, there are two things Apple showed that weren't in the keynote Because they just wouldn't work Uh, first one, a new video format shot for immersive 3D And it's filming deep 3D I don't actually mean like a simple two camera setup you don't mean getting like A theater, for example like Aft R3D It's all new stuff that sucks you into the picture in a really remarkable way Suddenly I was climbing a mountain or a tight-ripped walking or whatever It's very, very impressive And it's interesting because if they had shown that during the keynote It would look like a regular movie It's just hard to project that kind of 3D immersion into 2D space We just can't do it The second thing was the dinosaur demo, hopefully you've heard about already Which just blew the top off everything I've seen so far Uh, you know, you don't have to take place in a small room in its purpose-built Fieldhouse structure Apple Park And the room was built up like someone's lounge Right? And it's so for there's a coffee table, there's books added around it at a... But there's a large wall directly opposite me, which was empty But in the final demo, a bright line kind of appears down the wall And it just pulls apart like a portal into a dark and barren world And here's some rumbling coming that a dinosaur walks towards you Out of the... out of the world into your room with you And it's all live, you know, I got up and walked around The guy thought was tracking me perfectly It's definitely me and so forth And that was neat Of course, it's green up if you feel you can just watch out and touch a dinosaur But my coding brain kicked in right away I walked right up to the wall, which I know is not there But I can't see anyone, I'm just seeing this 3D space I know it's not there, but I'm walking around with this wall And I'm peering around this flat surface Into the larger dinosaur world And then I see another dino come out and start to fight the first one And I'm just really enjoying this remarkable AR wall anchoring of done here with this scene, it works so, so well And the demo enter Now some folks asked me if I had problems with the way to the device I had no problem, it's only 30 minute demo, so who knows, two hours, three hours, four hours, we'll see Some other folks asked where the demo was on rails Like where they had fun, exact path through The answer is no, although there was a range of apps to try I very happily took my own path, you know They wanted to show me what it's like watching avatar 3D and a virtual Dolby Atmos theatre Very cool It was cool, I love those, very neat But I was like that And I see the moon environment, let's try that instead And it worked great, I'm like on the moon watching avatar, because you might not, you know Anyway, my main takeaways are as follows One You've got to try this thing, understand how good it is Seriously Apple plans to run developer labs around the world Kupotino, London, Tokyo and more And I can see why Because when you try on the headset, folks will say Yeah, I want to build for this Second, to be really clear, it works great With glasses, you get a custom made prescription insert with Zeiss lenses, it is slots in magnetically, you leave a glass as behind It looks and feels amazing Apple have put a huge amount of work into this thing, making it accessible for everyone No matter what your needs, they'll help find a way hopefully And third It is really easy to get started with the code Right, your SwiftUI code works well out of the box With a few tweaks, it's going to look great And from there, it's really all about how far you want to take it with reality kit and more Now, if you're looking for ideas, I feel pretty confident Shareplay will be a particularly hot prospect, because The feeling of immediacy amongst friends is just powerful So if you're looking at shareplay or similar, good luck, that's a mere great starting point And now, the question I've been asked the most since I demoed a thing Will I get one? And the answer is absolutely very simple, yes, I will I already have several ideas, I want to try out building, right? I want to get packing straight away and build up and ship stuff But I also want to help others learn to build apps for Vision OS Because I can see this thing being a genuine smash hit And so as soon as Apple announced a developer kit, it's available And the register for it And get building and get writing and get trying and experimenting Because it's a really, really exciting area right now And for me, I think that's one of the most interesting parts about Vision Pro is because Apple has this huge pool of developers Already building apps to platforms with Apple, right? And thanks to the marvelous way Apple have adapted SwiftUI to the device It means we're pretty much ready to go I mean, yeah, they want tweaking here and then, they're there If you want to be your app, it's a really sing on Vision Pro, fine, you can add tweaks But at the same time, Apple released what? 50 or so, sessions at WBC23 Specifically to help developers make the most of Vision Pro So please do take the time to watch them What is many as you can for you dive into coding I've watched them all, that's for sure And also watching Apple releasing new human interface guidelines, Higgs Just the platform Again, although our apps work well with no changes It's a little bit of tailoring, specifically for Vision OS will make them feel really at home Anyway, that's it for me I hope you can tell I am genuinely excited by all the possibilities of Vision Pro gives us I feel rare in to go As soon as the SDK ships as soon as development preview kits arrive, bang I'll be on it from day one, I cannot wait If you have further questions, please post them below in a comment And let me know if you plan to purchase the Vision Pro for yourself (soft music) (soft music) (soft music) [BLANK_AUDIO]
