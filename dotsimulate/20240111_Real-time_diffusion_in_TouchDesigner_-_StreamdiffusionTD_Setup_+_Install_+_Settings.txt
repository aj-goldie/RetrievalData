Hello and welcome to this quick first guide on stream Diffusion TV. This operator allows for real time image generation from within touch designer and is built upon the incredible stream diffusion repo, which is an innovative diffusion pipeline designed for real time interactive generation. Basically, it allows for stable diffusion to be run about five to ten times faster than what I've been able to get in other tools such as Comfy UI and automatic 1111. In this walkthrough I'll go over downloading, installing, launching, and controlling the model all in real time and all from inside touch designer. I'll also touch on what each of the op parameters do and a bit on the stream diffusion pipeline in general. By the end of the video you will have everything you need to get set up and should be ready to experiment on your own. A quick note on system requirements. I am using a 40 90 and the speed of the image generation will be dependent on the computer you are running from. For now, everything runs locally and only works on Windows with an Nvidia graphics card. In my experience, depending on the settings, it will take about six to eight gigs of VRAM, so just keep that in mind. You'll also want to have git added to path, and I will have a guide on that linked in the description. So first things first, you want to have the operator loaded in your Touchdesigner network to set up the stream diffusion repo. You will want to go to the setup page. If you already have stream diffusion installed and running on your computer, you can just set the root folder here as the base folder parameter. If you haven't used the stream diffusion TD operator yet, you should also hit install NDI and spout requirements only and you should be all set once it finishes. If you are starting completely fresh, you want to click the download and update SD pulse. This will open up a folder selection menu and allows you to choose a location on disk for the stream diffusion repo to be downloaded to. Let's just call this test install hit select folder. This will open up a little command prompt, but it should only take a few seconds as it clones the repo. After that is done, you should see the base folder parameter is now set as that folder path if you need proof. If you open the folder, you should see some files and folders that have been downloaded. In order for this to work, like I said, you need to have git added to path. Once you have the code downloaded, click install SD. This will create a virtual environment for you inside of that base folder and install all of the Python libraries that you need for this operator to work. This is the longest part of the installation and will be dependent on your Internet speed, but it shouldn't take forever. I am going to save a few minutes and speed this along with some retiming. Once you see installation finished down here, you are all set and you can exit this window by just making sure it's selected and pressing any key to continue. If I go to the base folder, you can see that venv folder containing all of the python libraries we just installed. Once installed, this base folder parameter should stay the same anytime you use stream diffusion TD on this computer in future touch designer projects, as that is what links the operator to the installation we just made. There are a few more things to set before we start running the model. In the setup page stream type is how to set if the operator will use NDI or spout. In my testing, both run at similar speeds. For this tutorial, I'll keep it on NDI. In the settings page, look for the model ID path parameter. This can either be a local file path to a model on your computer or a hugging face ID as I have here. If it's a hugging face ID, the model automatically downloads from hugging face and is stored in your local cache. Next, briefly looking at the step schedule parameter. With this plus and minus UI, you can set the number of steps you must do this before streaming, and I recommend starting with two for faster generation or four for better image quality. We will touch on these step sliders more later. This similar image filter might be useful for webcam inputs with camera noise, and it can reduce flicker in the final stream, but I'll leave this off for the demonstration. All of the other settings can remain at their defaults for now, and we are ready to start streaming. Hit the start stream pulse at the top of the settings page, which begins the process in an external command window. While this guide focuses on touch designer and working with this operator, the backend for this can be run separately and controlled by other apps compatible with OSC and NDI or spout. In the command window you might see warnings about Triton or xformers. These don't seem to cause any issues. Xformers doesn't seem to be any faster than the nonexceleration mode, so at this point I wouldn't recommend using it. Once everything is loaded, you should see stream active with FPS information as the last line in the window as well as in the operator's stream info parameter. Let's make things a little more interesting and add a basic noise operator. I will look into this and have this fixed soon. But sometimes the internal NDI operator doesn't show the available source names. So a temporary fix for now is to hit this x on the left side twice to disable and enable the op, and the correct stream in name should automatically be selected. And now once you see the image on the other side, you know the pipeline is working. Now we are ready to dive into experimenting with the model, and let's go through the parameters that you can control live. First up, seed. If you're familiar with generative media or image diffusion models, the concept of seed will be pretty straightforward. It is a randomizing value that in this case sets the initial noise that the diffusion model begins its denoising process from. The basic version of Seed is every time you change it, you get a completely new image that still follows the other settings. Initially, I suggest just keeping this at a constant value, which will give you a better sense of what other parameters do and will also maintain better stream stability. Moving up to the prompt parameter experimenting with prompts with the real time feedback from the model is pretty wild. Just type in whatever comes to mind and well, you'll notice that when typing a prompt, it doesn't update immediately. This is because certain touch designer parameters only update their value after you click away or hit enter. Before we fix the prompt thing, I want to just animate the noise. Maybe that's a bit too much. Let's go zero one. Perfect. We can create a more interactive prompt box using a text comp. Let's do this quickly here, hit tab and add a text comp to the network. In this first page set, type to multiline, turn on word wrap and edit mode should be editable. Continuous update and now activate the node by clicking the icon at the bottom right or pressing a on your keyboard while the node is selected. Then maybe name this operator something a little more specific like prompt. Then you want to click and drag this to the stream diffusion TD's prompt parameter and choose reference. Now as you type in the box, you can see the immediate effects on the prompts changing the output image. Maybe I'll change the noise to be animated a little faster and I can do a color shift on the fireworks here. It's simple, but super powerful stuff. You might notice I just wrote this little thing that says negative prompts do nothing and as you change them, yep, doesn't seem to have any effect. Now going back to the step schedule parameter section, one of the key ways stream diffusion is able to generate images so fast is the way it denoises steps in batches instead of one after another. Like other image diffusion models in this operator we controlled this with sliders that will drive the position and also the amount of step batches in this parallel denoising. Hopefully that isn't too far from the truth and the math and actually makes sense, but generally you can think of it like denoising an image to image. Lower slider values add more randomness early in the process, and higher values on sliders make the output closer to the input image. It is suggested to keep the steps in order, with the first step being the lowest value and so on. In my experience this definitely seems to be true for one five models, but SD Turbo doesn't seem to mind less conventional values. It's worth playing with. Instead of CFG we have guidance scale and delta. These parameters are much more sensitive in stream diffusion compared to stable diffusion, making the range much smaller. Also, neither parameter will have any effect unless guidance scale is above 1.0 and the effect it does have is often too much if over 1.5. Overall I found it a lot less useful than CFG, but sending it around one one provides a slight boost. In contrast, depending on the prompt, every parameter you see that is not disabled when the stream is active is definitely worth experimenting with live to get a better feel for what it does. You can also drive all of these parameters with anything in touchdesigner, such as building out some audio reactive chops or maybe a dynamic dat based prompt system like pretty much everything else in touchdesigner. There is no one way to do it and I am very excited to see what people discover and make with this. Before I wrap up I want to try and hit the other parts of the operator. We have the Lora page. This allows you to use custom Lora models during the live inference by turning this toggle on and referencing the model with a local file path or hugging face ID. Each model has a specific weight that can be set before launching the stream. The next page is callbacks. This lets you add custom python code to extend the functionality of the operator. You can access them by clicking this arrow and finding this script. Here I have three callbacks currently on receive frame onstream start onstream end in on receive frame there is a commented outline that will print the frame count when a frame is received. If that gives you any ideas then this will possibly be very helpful for you, but if not, then that is all good and you can just ignore this. You can stop the stream with the stop stream button in settings. In the setup page. You can also set the OSC ports and the video feed name before launching in case either is taken up by something else in your project. There is also this visible window toggle which will launch the command window silently in the background allowing touch designer to stay in focus. I would not suggest doing this until you know that everything is working perfectly. The last thing we have not covered is the about page. There you can find some links to my Patreon Discord and Instagram. The discord server is currently the best way to contact me and it is where I post all my latest tools, experiments and discoveries. I also give as much support there to Patreons as time allows me to feel free to say hi, share what you are working on or just lurk around. That's everything for this first introduction to the stream diffusion operator. Thank you to all of my patreons for their support and I hope you have a great time trying out real time diffusion. Happy experimenting.