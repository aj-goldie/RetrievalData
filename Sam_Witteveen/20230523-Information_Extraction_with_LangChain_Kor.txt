 So in the last video, we looked at how to make some market research type content through getting two agents to converse with each other. If you haven't watched that already, go and have a look at that. One of the big challenges then is we've now got a bunch of text files with information that we probably want to get into some kind of structured format. If we were doing some kind of market research where people are talking about a product and qualities of that product, that kind of thing, then we want to be able to extract those out and be able to put them in some kind of report or some kind of structured data to make use of that. So today, what I want to do is go through a package called core, which is kind of like an add-on to a lang chain, which is basically built for information extraction. So what I'm going to do is go through some of the examples in here and then I'm going to show you at the end of using it for a real world use case of where I take the conversation that I had about two people talking about restaurants and we're going to extract out the restaurants, their locations, the dishes that they had in there. So this is the package core you can see here. It's still in a reasonably nascent stage, I would say. There's a whole bunch of things that have been added and my guess is that it may evolve over time. Already though, it's very useful to use. One of the big challenges that we have in natural language processing is if we want to make a nerd detector or something like that, we're doing a named entity, a recognition, or even any sort of extraction information extraction thing, we usually need to train up a model and to do that, we're going to need a decent amount of data. So if we were doing this kind of task and we had the data already that was all nicely labeled, we would probably go and train up a bird model, a distilled bird model, something along those lines, maybe a small T5 model to extract that information and label it nicely as we go along. The challenge is what if you're doing something where you don't have any data to make a model in the first place. And this is where a tool like core comes in. This allows us to use a large language model. In this case, we're going to be using the chat open AI model to basically go through and extract out information that we can then use for either creating our reports or even eventually for making a data set for us to make a proper nerd model later on. So let's just jump in. So this is using Lang chain under the hood. It's basically got a simple sort of workflow of where you go through, you get your text, you clean it up, you split it up, you define a schema that you're going to use to go through it. And then you give it to the large language model to go through and extract out what it is that you want to have. So you can see here we're basically bringing in some sort of standard stuff. We're going to bring in the Lang chain callbacks so we can see how much money we're spending on this. From core, we're going to have this chain, which is the fundamental chain, the create extraction chain there. They then have core nodes. So these are type of things that you can extract from here. So you can define an object, which we'll look at. We can have texts, we can have numbers in there. We can also have some other structured things, which I'm not going through today. And then one of the things that we'll finish up with is showing you that we can actually define pedantic classes and use them to do or to assist with the extraction of data from this. So let's jump in and look at some simple examples. So first off, we just define a large language model. And then we need to define the schema of how we're going to basically deal with this. So we're going to have this is just some examples taken from their documentation. In this case, we're going to basically have an object, which is going to be extracting out personal information. We give it a description. So the description is going to help the large language model to work out what to what to basically extract. And also we're going to give examples of things, which will then also help the large language model to work out what to extract. So things like the description and the example are things that are going to go into your in context learning in the prompts to be able to make it easier to extract this stuff out. So here we've basically got we're going to extract out personal information. The attributes we're going to get, we define this one as a text. And this is going to be first name. The description is the first name of the person. An example, John Smith went to the store. Here's John is what we would extract out for that last name. We would extract out this age. We can extract out that as well. And then we can give it some sort of full examples of where if we had a sentence that had these in it, this is what we would expect it to extract out. So John Smith was 23, sure enough John Smith aged 23, Jane Doe age five, and that extracts that out. Now to make the chain for this, the basic chain, we just passing in the LLM and the schema that we've got here. And then once we've got that, if we want to look at the prompt that it's actually using, we can see the prompt here is basically your goal is to extract structured information from the user's input that matches the form described below goes on there. We can then see also that it's giving us an example of what these are with the description for each of these. And then we've got some examples of the actual extraction for these that are also being put in there. So this is giving us the in context learning. Then our text will go in here. The output will then generate. So sure enough, we can see if we pass in David Jones was 30 feet old, a long time ago, it's able to pick out David Jones age 34, right, from that. So this is like the most basic example that you can do. We can then start to nest things as well. So we can actually go into the schema and nest a bunch of things into the schema. Now to do this, we're going to end up using Jason. So above was using the CSV extraction by default. Here we can look at this doing the Jason. So here we're basically getting a from address. You can see here we're getting the text, we're getting street city, state, zip code, country. And then we give it an example, we give it a to address. And then we can see that for the schema, we can basically just pass in that, okay, we want to basically get the full name of a person. And then we want to get the from address and the to address. So those are being nested in to the full schema object here. Okay, so to use nested objects, we need to basically move away from the CSV encoder class to Jason encoder class here. Same deal, we're basically just setting this up, passing in the Lm, the schema, here we're just defining the encoder class and an input format. If we're going to have something for doing that. And sure enough, we've got our prompt again, and we can see that, okay, the prompt is showing what we would expect this to be out. We can see now it's saying please output the extracted information in Jason format. Do not output anything, except the extracted information, do not add any clarifying information, do not add any fields that are not in the same schema. If the text contains attributes that do not appear in the schema, please ignore them. All output must be in Jason format. So you see, it's a pretty long prompt that we've got going in there, we've then got our ICL going on here. And then finally, we've got our output. So sure enough, now if we use that, and we say, Elasto moved from New York to Boston, M.A. while Bob's 50 the opposite. So we can see that, okay, it's got the person's name, Elasto, to address Boston State, Massachusetts, person Bob Smith, to address city, New York. So it was able to extract those out in this case. And you can see that it didn't add the state for New York because that wasn't mentioned in here, whereas the state for Elasto moving to was mentioned in here. So these are the basic examples of it. The way that I, for myself, using it in the real world is to use it with the pedantic classes. So if you haven't come across pedantic before, this is basically just a class system, it's used a lot for fast API for sort of defining inputs and outputs to a certain class of what you expect it, what you expect to get out of this. So here, I'm just basically loading up the text file from the camel extraction that we did at yesterday. And you can see just printing out a little bit of that here. You can see this is basically just a conversation where they're talking about things. We can see that they've mentioned a restaurant there for you. And we can see that that restaurant is in Spain. They go on to mention mentioned dishes and other restaurants and other places. So what I want here is to be able to extract out that information about the restaurants, their location, perhaps their style of food and the dishes that people liked from there. So I've basically just brought this in. I've just done some splitting with it so that we've got it, split up, it turns out this one is quite small. So it's not a big problem, but showing you if you wanted to do a full-size dock, you could do that. We set up the LLM again and here's the new bit where we've got. So we're setting up this pedantic class here. And so we're basically saying this is a class of restaurant. The restaurant is going to have a name with a string field attached to this. Our description is the name of the restaurant, a location. So this is going to be optional. So the name must be there is what we're after. This is the thing. The whole reason for using the pedantic class is it's going to force the model to stick to this sort of group of information more. And it's also going to allow us to validate that information. Now, not validate in the sense of checking is this real information or fake information. Just validating is this in the right format so that we're getting out for each restaurant we're getting out. It's like we're getting out an instantiation of this class with these fields. And some of them are optional. You can see that location, style, top dish are optional. But some of them like the name, not optional, right? That has to be in there. So we've basically got that. We've got this validator that says, okay, this must not be mg in here. And then we can see here, we're basically saying from the pedantic, we're extracting this out with the extraction validator. We're passing in the class of restaurant, we're passing in some descriptions and examples, and just setting this up and we're saying that many equals true means that we can have, it's not just one restaurant we're looking for. We're looking for multiple restaurants in here. Okay, so now we basically set this up. I'm going to go back to the CSV encoder. In this case, they mentioned in the actual documentation, and I found this to be true as well, that the JSON decoder doesn't seem to do as well as the CSV encoder. So if you don't have something that's nested, if you don't have something that means the JSON decoder, you're probably better to go for the CSV decoder in here. So I pass in the large language model, I pass in the schema, I pass in our extraction validator that we've just defined up here, and then we're going to run this through. And so you can see here, we're going to basically pass in, let's just look at the prompt. The prompt is very similar to what we've seen before. We can see this time, you know, it's outputting it in CSV format. So it's using the pipe character as a delimiter here, and we can see that the rest is kind of similar in the, you know, do not add any clarifying information. You must follow the schema above, etc. And then we've got our example in there. Now, I could probably add quite a few more examples in there to get the ink context learning to be better for this. So that's something you would have experimented with. All right, I now basically run this. I run this as we go through it and just running the, the docs through, you can see that cost just half a cent to run this. And then now, sure enough, I'm getting this formatted stuff out. So, okay, I want to actually make that human readable. So here, I basically just written a little function to basically take this in and put it as human readable. And sure enough, you can see now, when we run that through that function, we now see that we're getting restaurant name, we're getting the restaurant, the location, the Rona Spain, I'm guessing, cuisine, not specified in this case, the top dish. And we can see that, okay, no more Copenhagen top dish fermented berries and ants dessert. We can see one of the restaurants that's mentioned from Mexico, from Bangkok. We can see quite a famous restaurant, Austria, Frances Ghana, from Moderna in Italy. And if you look at it, this is one of their famous desserts there. And if not the kind of language that we would expect to be associated with a dessert, yet this is actually what it is called. And so it's been able to extract that out quite nicely in there. And we can see that it's got Attica now, I'm pretty sure that it got all of the restaurants that were mentioned in there. So it's done a pretty good job of going through that. If we wanted to put that in a pandas data frame to give it a structured data, we could then basically just put it into a pandas data frame, we've got it like this. Run this from this point, you could just use a nice romp to take this as information in and write up a report that would mention these things and use them in some way like that. So you've basically automated everything from this stage of creating the conversation, extracting the information out of the conversation through to writing up a report based on this. And obviously you would have the report once you've done many of these pass through that kind of thing. So just to show you that how the the pedantic validation helps out, I've done one where we haven't used the the same validation here. And we can see that the output, so this is basically just going through, we're just using a sort of just we're just passing in the restaurant, pop thing, we're not passing in the validated. So it's just going to come out with a sort of blank validator where it's not using any of the examples that we had. If we basically run this through, you'll see that the output now is, yes, it does get the the first ones quite nicely. But then it starts making up ones or getting sort of partial ones, he can see it's got the the restaurant for Melbourne, Australia, but it hasn't got Attica as the name for that in there. And we can see the other ones, it's got a deconstructed lemon tart, but it doesn't have the name of the restaurant the same here. It's sort of repeating some of the ones up here. So that's where the validation is not being enforced in the same way. So this sort of shows you that using the pedantic classes and and putting good examples in there, it really helps you to get some good results out and just the results out that you want, not other things as well. Now this is far from perfect, it will make mistakes. I really appreciate that the author of the package mentions that one of the things that's really good at is making mistakes. I do find though, you know, it is useful. And if you play around with it with descriptions, giving it enough in context learning, you can get some very good results out of this for real world sorts of things. And then you could use this to build a proper nerd model with a but model that kind of thing going forward. Anyway, as always, if you've got questions, please put them in the comments below. If you found this useful, please click like and subscribe. I will talk to you in the next video. Bye for now. [BLANK_AUDIO]
