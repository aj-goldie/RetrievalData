 In this video, we're going to look at mini GPT-4 and I am going to go, we're going to explain a little bit about how it works, some of the things that you can do with it and then we'll go through some demos. So if you're just here for the high P demos, I suggest go to look at the chapters in below and go and look at the demos because I actually want to go through because it's pretty damn cool what this thing is doing and it's showing off some really nice things. So let's just jump in and look at their diagram. So what this basically does or rather let's look at an example of what this can do. So this you basically put in an image and then you can then ask it something about that image or you can ask it to do something with that image and it won't manipulate the image but it will use the image to generate text and that text could be something like where here you can see that they've got a pair of mugs, they're asking it to write an advertisement for that, you can go through and do that. So it's got a little diagram to asking it to turn that into a website and go and do that kind of thing or you could ask it specifically about things that are in the image. So the idea is that you're using an image plus some text to generate some kind of text put. So here's the diagram of what they've actually got. So you're going to put in this is like their image that they're passing in and this first gets passed into a vision transform. A vision transformer breaks it up and makes representations which is sort of spreads out across transform and then it's using this sort of query in transformer part here to make a representation of the whole image. So this part here is actually coming from a model called blip to. So if you've heard of clip for stable diffusion or anything like that where these models taking images and converting them into text representations or to just vector representations out blip to is a more advanced version of that and it was created by Salesforce and definitely it's doing a lot of the magic in here. And the authors of the paper, what they'll try to show was that, okay, you can kind of use this at the moment. But if you want to do something like GPT for or what's being commonly called as GPT V for GPT visual is doing where you basically put in an image and you ask it to do something with your image, you need the blip part but you also need a large language model and do these need to be trained to get us. So do we have to go and train a whole language model from scratch just for this and what they show is no, you don't. So they actually take the kuna, which if you remember back for kuna is basically a fine tuning of llama, which I think is about 13 billion parameters is the one that they're using for this. And this is a pretty decent model. I've got videos about it on my channel. You can check them out. What they do is they take that language model, they freeze the weights in that. So you're not training that at all. They take the blip to and they freeze that and then make insert a layer in between. And only thing that they're going to train is this layer. And what this layer is basically going to give an output representation, which will go into this linear layer. And this linear layer will then convert that representation to something that vikuna can use to extract the meaning of the image. So the blip to is actually doing the extraction of the meaning out, but it's like it's coming out in one form and it needs to be converted to work with vikuna here. And this is what they're going through as they're going through this. Now they've set up some sort of token things of how they do this. And the human would put something in, they've gotten assistant going back, but you're basically taking your image, converting it into vectors that you can eventually get into a large language model and then using that in a large language model to generate some kind of answer. So it's really important to understand that this model is really big. This model is pretty decent size as well. The VIT, especially the vision transformer, yet these are frozen. So there's no effort in actually training these. It's just training this layer in between. And they did it, I think, from memory with four A 100s for about 10 hours on a few million images. And then the last bit to sort of just get it perfectly going for vikuna, I think it's just done on one A 100 in about 10 minutes. So does show and people are asking them to release the first part of the kind of pre-training so that people could just fine tune their own ones for different tasks. If you had some special kind of questions that you wanted to ask about images of products or something like that, you would be able to fine tune it to do that. So you can see from this, it's able then to generate amazing things. So you've got these images going in. You can see here's a picture of Ramen going in. The company text going in with it is how should I cook this? It can generate a list out of the things that you're going to need and then a recipe for cooking this. If asking it about a movie, he gave me a short introduction to this movie. So that requires it to know what this movie is, which is kind of amazing. So it knows that that's from the Godfather. It's a classic American crime. I'm directed by Francis for a couple of goes on and gives us that. So a lot of this information probably would have been in Vakuna already, but linking it to an image is something Vakuna didn't have by itself. Even things like this is pretty interesting. The sort of cause and effect thing of like showing a washing machine with suds coming out of it. And it basically then explains there are several reasons that this might have happened. And they're giving good coherent text, makes sense for these kind of things. And it can explain memes, certain memes you can put up there can go through and explain these. Even for people if they're famous, like this one is a picture of Elon Musk. And so as who is this person in the image, you know, it must put it just that face there. Yet it's able to work out, person in the image is Elon Musk and then tell us a little bit about him. Let's jump in. You can come along and try this out like yourself. So they've got a bunch of examples here that you can look at, but they've also set up, at least for a short time, some links where you can try this out. So if you want to try this out yourself, encourage you come and try it out soon before they take the links. Let's go through and have a look at some ones that I ran. So it is quite slow to run these. So I've run these ahead of time and asked it a bunch of questions about these things. All right. Let's see how it does. Okay. So when we jump into the demo here, you can see that basically we can upload an image and then we can query the image. So what's going on here is each time it's basically, you know, it's taking the information from this and passing it into the language model with our query here. So you can see first off, I'm basically just for all of these examples, I just ask it, okay, what is this? And in this case, it gives it a really good answer, right? This is a ball filled with various fruits, including grapes, which we can see, limes, we can see, and pineapples. Well, I think there's probably one pineapple, but anyway, that's pretty good. All right. What color is the ball white has no problem distinguishing the white ball from the white background? That's curious about that. So straight away, I'm jumping in off the deep end and asking it, okay, well, how many limes are there? Now, I give it a query where it doesn't actually relate to the image. So I say if I ate all three limes, how many calories would that be? So this is basically relying on the Vakuna model here. So you can see the answers, assuming each lime is about the size of an average supermarket lime, eating all three limes would be approximately 300 calories. I'm not sure if that's true or not. Certainly convincing answer in this case. So at this stage, I'm starting to get a lot more confident. And so I think, okay, let's try it. Your network's a notoriously bad for counting things. So it's pretty amazing that I got the limes. I thought, okay, long shot, let's ask it for the grapes. How many grapes are here? And you can see the answer that it gives actually kind of makes, you know, a lot of sense, difficult to determine the exact number of grapes in the bowl without more information about the size and type of grapes. However, based on the image provided appears that there are several grapes in the bowl. It's probably a lot more than several. But anyway, that one will give it a pass. What type of grapes are they? It doesn't seem to know. It basically gives us, you know, that it can't determine it. And then actually what I was after was are they red grapes or actually green grapes is what I meant? But I wrote, are they red grapes or white grapes? And may have interpreted that as relating to wine? I'm not sure. But you know, I would have thought that it would have been pretty simple for it to say, okay, they're red grapes. Maybe in lots of places they're not cold, red grapes, I'm not sure about that. But it's not possible to determine the color of the grapes in the bowl without more information. So you see for our first example, it's done pretty good. Let's look at the next one. So the next one I do the screenshot of an Apple homepage looking at the iPhone. So I asked it, okay, what is this always start with that? Okay. It says this is a homepage for the iPhone 11 Pro. Very good answer except for it's the 14 Pro. My guess is that they must be kind of similar to what it's seen before. And that's why things that gives us some interesting stuff related to that. And I asked it what company does the page belong to? It said that I'm sorry, but I don't have enough information to determine which company the page belongs to. And it appears to be a generic product page for iPhone 11 Pro, which is sold by Apple. So it kind of gets it. But the thing I realized after this and I did some searches was actually this kind of format of having the number of iPhones like this with the sort of slogan or the text for it above is actually used by a lot of telcos as well. So it does make sense then that maybe it can't work it out, although you would think that like Mac and all the other stuff at the top would sort of give it away. Then I try. Okay. What does the text on the page say? It doesn't do a good job here. It's basically just making it up. It's talking about the iPhone 11. So next one I thought, let's just give it a logo. So I throw in this Chrome logo. What is this? It's a logo for the Chrome browser. No straight away that I then asked it who makes Chrome. So obviously that's not using the image. It's using the language model. Google makes Chrome. Okay. I said, what are the colors in the logo? Colors in the logo are green, blue and red, green, blue, red. What's this color over here? So I asked it, is there yellow in the picture? No, there's no yellow in the Chrome logo. So it's interesting to, I'm not 100% sure, but I'm not sure whether it's getting this information from the image or from the language model having just, in its training set, there's somewhere where it talks about the colors of the Chrome logo. That one I'm not sure. What I was really interested about was like, okay, what is the hex code for the green color? So the hex go for the green color in the Chrome logo is, and again, I'm not sure whether it's using this from the image or from having read it somewhere. I thought, oh, that's too good to be true, right? I can't be that. So I did a search and sure enough, this is the color that it gave us the hex code for. Maybe not exactly right, but pretty good. I thought the next one is okay. What is Chrome used for? Obviously, this is just testing the language model. So this is like just using the bacuna part of it and then what alternatives are there to Chrome there again, testing the pruna part of it. Last one didn't do great. It's a famous selfie, I think from Oscars or something. I'd seen other people talk about that it can recognize some famous people. So I thought I could check this in and see what is the picture of this is a picture of a group of people taking a selfie at an event. They're dressed in formal attire that certainly looks right with the man in the center wearing a tuxedo and others wearing dresses and looking at the camera. So that's kind of right. Who are the people in the picture? Okay, it didn't know who they are. I also tried to do things like, okay, what is the color of the dress, the female on the left is wearing? So I was hoping we would get red or something. We don't get that. So I'm kind of curious if you were doing visual question and answering where you're asking about colors, it'd be interesting to know how well this actually does of that. Because that's a skill that's been many people have had papers about it. I've been also involved with a paper about it in quite a long time ago. It's something that definitely there are data sets for testing that kind of thing if you wanted to try it. It's also curious about is anyone wearing jewelry in the picture? And they clearly is, but it doesn't seem to sort of give us this, I'm sorry, but I didn't have enough information to accurately determine. So this one kind of shows when it's not doing well what it's like. You could imagine though that you could sort of fine tune this to get better at certain uses that you want. So just as they talk about that last fine tuning being a very sort of simple one, could imagine that you could put in some things in that example to basically frame tune it for a very specific thing if you wanted to ask questions about products or about checking something in a picture that's something you could incorporate into this. Anyway, this was just a simple example of going through what Mini GPT for is and showing you some demos of what it can do and stuff. As always, if you have questions, please put them in the comments below. If you found this video useful, please click like and subscribe. I will talk to you in the next video. Bye for now. [BLANK_AUDIO]
