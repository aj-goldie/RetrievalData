 In this video I'm going to show you how you can run your Python Web Scrapers in the cloud.
 So we're going to be using cron job on a Linux server and we're going to set it to run our code
 at a specific time every day. So the code that I'm going to run is this one here. It's a basic
 web scraper that goes out this website and as you can see it's got new products every day.
 We just scrape that information and email it back to us. This could be any kind of scraper
 script that you've written. So there's a couple of things that we need to know first. That is we're
 going to be using a Linux server and it's going to be running in the cloud. So there are going to be a
 few Linux commands we're going to need to go through but I'll run through them all with you and you
 need a way to get your code onto that server. The best way to do that is get and GitHub if you have a
 GitHub account you can use that. If you don't you can copy and paste it across but it's nowhere near as
 easy or as convenient. So the first thing that we want to do is we need to create a digital
 ocean account. Now digital ocean lets us set up a droplet that we can, it was just basically
 like our Linux server in the cloud that we can use. Now these aren't free. However, if you click in
 the description you can use my link and you will get a hundred dollars credit to use and it runs
 out after two months. So that gives you two months worth of testing and messing around and you'd
 have to pay anything. So that's quite cool. There are other services like this too. Or you can use a
 Raspberry Pi if you've got one handy that works just as well. The newer ones are better obviously.
 So to get started once you log in you can see I've actually already got a droplet running here but
 we're going to create a new one. So we click create and create a droplet and once that loads we can
 select Ubuntu. It's going to be our basic plan to even when it's paid it's only five dollars a
 month. So if you have lots of scrapers that you run constantly this could be a good option for you.
 I'm going to select London because I'm in the UK and we're going to authenticate with a password.
 So choose a root password which you'll need to remember to log in with and make sure you're
 by by its rules. We can give it a name. I'm just going to call this one.
 You'd you're whitey demo but call wheels whatever you like and that will do so I'm just going to hit
 create. So whilst that's loading up I can just show you here that I've got my repo on my GitHub. This
 is the code that we just saw and this is what I'm going to use to get cloud onto that droplet.
 So that we are going to need to use a few Linux commands and we're also going to need to use
 SSH which is to create a secure shell to connect to our droplet. I like to use the Windows terminal.
 You can use either WSL which is the Linux one or you can use PowerShell if you don't want to
 install WSL on your system but this is much easier to use the Windows terminal from the App Store in
 Windows 10 as opposed to the default one. It just looks nice and it's easier to use. So hopefully this
 is done almost booting up. There we go. When it's finished it gives us an IP and we can click
 copy that copied and come back to our terminal and I'm going to be using the PowerShell because I
 suspect most of you are as well. So what we want to do is want to type SSH for secure shell
 routes because we're going to log into the root account of our droplet at and then paste the IP just like that.
 We just want to click hit yes to get through this and that's fine. It will add it to our
 unknown hosts which is okay with us. Type in your password and if you get it right there we are.
 So there's two things that we always need to do first with our new droplets and we need to update
 the system. So we want to do apt update. apt is like the package manager on Ubuntu if you don't know.
 That should run nice and quickly.
 What we want to do is we just want to make sure we start off with a up to date system before we do
 anything else. So it's come back and it said 56 packages can be upgraded. So then we just run app
 apt upgrade. This will take a little bit longer. So there's 174 megabytes of upgrades to do.
 I'm just going to let this run and I will come right back to you. So that's finished updating which is
 great. I'm just going to hit control L to clear the screen or you could just type clear and hit
 enter. So the next thing we want to do is we want to check the Python version on our
 on our droplet. So if you hit Python it will probably tell you can't be found and that's because
 we are on the next. So we need to do Python 3 and we can see that we are running Python 3.8.5
 which is fine which is great. The next thing we need to do is we need to get pip installed because
 we are going to need to install the Python packages that we need for our scrapers.
 So to do that we can do apt install and I think it is Python 3- pip. Okay yes it is and there we
 will hit yes there. I'll have all of these commands written out and I'll put them somewhere below
 either they'll be on my github link or in text just so you don't have to memorize them as I
 type them out super fast. Okay that's done. So now we can think about getting our code from our
 github onto our server here. Now the easiest way to do that is to get clone which we are going to do
 in just a minute. I'm going to talk just a second about requirements though. So if you were smart
 which like unlike me when you created your actual script you ran a virtual environment. So if you
 had a virtual environment where that you'd installed your code on everything you would have
 pip installed your packages into that virtual environment. You can then run a pip freeze and output
 that to requirements.text that you can then upload your github. So when you pull everything down
 onto your server with your git clone you can just run the requirements.text and then it will
 solve everything that you need. I didn't do that because yep reasons so we're going to go ahead
 and actually pip install manually what we need. So I'm going to do pip 3 remember I'm
 python 3 pip 3 on the next in this case so I'm going to do install and I can't remember what
 we need. It's requesting beautiful soup and pandas. So we can just go ahead and install those.
 So requests pandas and beautiful soup for. So I'm just going to let that install and it's done.
 Again clear the screen. The next thing you want to do is clone the repo. So we can see it's here
 essentially go back to the main page and I'm going to copy the URL and then what we want to do is
 we want to make sure that we're in the home directory on our server. So if you were to type ls here
 you would find not a lot. What we want to do is we want to type cd and then two dots to go back
 up. Now we're in our main root directory and then if we do ls we can see all of the main
 Linux folders here. Now doesn't it doesn't matter if you don't understand all of them. We need to
 know is that we need to be in our home folder. So we want to cd into home and then ls and we can see
 that there's nothing there great. So that's good. Now we want to get clone our repo. So we do
 get clone and paste the URL and that's going to clone all of that into our home directory.
 And now if we run ls we can see that it is there a lovely title.
 Projects. So if I go into that and we can see here we've got two files that read me and the
 py file. So if I was to run python3 and then run my py file we find that I've got an error. Now I did
 this because when you upload your code to github you do not want to put any passwords in there
 or anything like that. So when you have code that you need to have passwords for like this in this
 case is my email account. I find that you if you put that in a separate file you will be able to keep
 that separate and you won't need to upload it to github and you can just create it manually.
 So if I come back to my code here we can see that I have right here import creds. Now creds is another
 py file which I've got here and all it has is password and then your password here. Now I can
 choose not to upload this creds file to github and then I can have my repurrs.
 So I need to recreate this py file on my droplet so that actually works. So I'm going to use nano to do
 that. Nano is a text editor which is just always installed on the next service. You can use
 VIM as well if you want to. So I'm going to just do nano and then I'm going to say creds.py and in here
 I'm just going to replicate what I had before except with my real password this time which
 unfortunately I'm not going to show with you. So now I've saved that we can see LS and we have a
 creds.py file so I'm going to clear that and we are going to run it again and hopefully we should
 get no errors. I didn't actually have any output from this python file but my phone is just
 about to buzz and there it goes I just heard it. I've got an email and that's from this file.
 So now that I know that this script works on this system if it doesn't you need to do some
 debug into try and find out why. Maybe you don't have the right packages installed or maybe there's
 some other errors or issues that you'll need to resolve before you carry on. But now that that's
 done we can move on and we can actually have a look at using cron job. So to get to the cron job list
 it's called cron tab so we just need to type that in and then dash E and it's going to create one
 for us. Now these are all our user cron jobs that the system is going to run for us. This is the first
 time so it's going to say you need to select an editor. So as now knows the easiest one and that's
 very true. Get one and we get this up here. Now this does explain how to create a cron job and get it
 working but what I'm going to do is I'm actually going to come to another website which is cron
 tab.guru and this is really handy to see how what you put into this information comes out. So
 each of these stars as you can see represented by a minute hour or day a month and then the week.
 So depending on what information we put here will depend on how often our script is run. So if you put
 star star five stars like this and then the link the path to your code it will run it every minute
 forever. We don't want that but what I'm going to do at the moment is I'm going to just say
 every minute now because we're going to test it. But if we wanted to like I'm going to finish with
 run this code at 8 a.m. every day you would put an 8 in here and you can see that it says at every minute
 past 8 hour 8. It's not quite what we want so we need to change this to a zero and that's going to say
 at 8 o'clock in the morning. If we click on random a couple of times you can see that you can do all
 sorts of different things and you can kind of just click on this and figure out what each of these
 ones means and why you or how you can do it. So you can even use our name tags there but let's
 let's just go back to zero eight star star star and that's what the one we're going to use at the end.
 So now we want to test that we can get our conj jobs working properly. So what I'm going to do
 is I'm going to actually exit out of this and I'm going to create a new file in the same directory
 here and I'm just going to say we'll call it test.py and in here I'm just going to put a simple
 print statement that says if you can see this it's working. There we go and I'm going to exit and
 save. Now if I run Python 3 and test.py you can see that we got our output. So what we can do is
 we can use this to on our conj job to check that we are in the right place and everything's working
 by outputting this to another file and then we can check that file to waste what it's working.
 You'll see what I mean just a second. So we're going to run crontab-e and come down again
 and we're going to come back to our five little stars. There's two ways to do this. You could put a
 line at the top of your code. There's shebang line that basically tells the server that it runs
 with Python 3 and then you can make that executable but the way I prefer to do it which is simpler
 it's just simply put the path to the Python first and then the path to the script that you want to
 run. Now I know that on a Linux machine the Python path is /uSR/bin/pythin3 and now we need to put the
 complete path to our Python scripts which was home because we came into the home
 folder earlier remember. I think it was whiskey crontab/test.py. Now even though that file has output
 we won't actually see anything so we want to send that to a file that we can then look at. So if
 you do two sideways arrow things like that that will send it to a new file. So then we can just say
 we'll just send it to the home folder and we'll just say cront. So if I just make that above there you can
 see all we've done is this is the path to our Python this is the path to our script and two little
 arrows which mean we're going to send the output to a new file which is home/cron.log. So I'm going to
 save this, write it and I'm going to then go to the home folder and I'm going to wait for a minute
 and hopefully we get a log file that comes up with the output in it. So I waited for a minute
 and now if we type ls we can see we have cront.log and if I use the cat command which basically
 spits everything in the file out to the screen we can see that we have our Python code.
 Now that I know that this is successfully working I can then change the
 crontab to my Python script. If this doesn't work for you there's a couple of things that I
 would recommend checking. First one is that your Python script is in the home folder somewhere because
 the crontab is user-specific so you want to be in your user's home folder and also check that
 you have input the stars the five stars incorrectly and the Python path and your file path.
 But I did so I know that works. Come back to crontab-e and we can change this from test.py to
 I think it was called new whisky.py and we don't need to output the file, output it to anywhere
 because we know that it works and we hit exit save and there we go. Now at eight o'clock every
 morning that file is going to run is going to scrape their data and it's going to email it to my
 phone. So that's it guys that's how you do it you need to use some basic Linux commands you need to
 learn SSH a little bit as I said I'll have everything written up in a description for you so you
 can see all the commands that I use to do this and you can work through it yourselves.
 Don't forget you can run this on any Linux machine so if you have a spare computer that you want to
 use as a server in your house you can just install Ubuntu on it and this will work exactly the same
 way or if you have a Raspberry Pi you could do that too but just make sure you have one of the
 newer ones because the first ones are really really slow or if you want to do it all on in the cloud
 as I said digitalization has a hundred dollars credit going on at the moment if you use my
 link now it'll be really nice of you and get that started and if you carry on and you like it
 so if you have dollars a month there are other ones as well there's a little node or if you
 wanted to do a bit more manage you could do Python anywhere but that's entirely up to you how you
 make it work and so this is just the way that I do it so hopefully you've enjoyed this video I
 found it useful give me a thumbs up and comment let me know what you thought and consider subscribing
 there's lots of content on my channel already for web scraping and more web scraping, Python etc
 all that good stuff still to come thank you very much guys and I will see you in the next one bye
