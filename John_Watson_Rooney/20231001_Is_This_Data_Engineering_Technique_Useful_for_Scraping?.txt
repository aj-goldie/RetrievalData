There's a concept in the data world called ELT. That's right, ELT not ETL, very similar, but this process of this one is extract, load and transform. This basically means you take all of the raw data from your data sources and you load it into a central data storage, which you can then query against. I've heard of this being used in the web scraping world before, but it's not something I've ever done. And I wanted to give it a go and see how we got there. So what we're going to do is we're going to pull some information and we're going to save the whole HTML document into a database for us to then query against later. So I've got my terminal open and we're going to create a new file. Let's just call this main PY and we're going to open it with Neovim like. So now we're going to be using playwright to actually get the information for us. So we'll do. From? I think it's playwright. Sync API. We'll import in sync Playwright. We also need date time because we want to date stamp this particular set of data. So every time we get the HTML, we want to date stamp it. So we'll do from date time, import in date time, which is always one of those my favorite curious python things. And finally we're going to use SQLite three. So we'll do import SQLite three. Now you could of course use an Orm if you wanted to, but I'm not going to in this instance. I'm just going to use raw SQL query. The first thing we want to do is we want to create a connection to a database or create one. So we'll do our con is equal to SQLite three connect. And I'm going to call my database ELT DB. And when you do this string, it will basically connect to the database or create it. If it doesn't exist, then we need a cursor so we can execute commands against this database. So we'll do con cursor like this. And now we can actually create the table within our database. So we'll do Kerr execute. This is how we run commands against our database. And I'm going to put this into triple quotes because this is going to be over multiple lines. So we want to do create table if not exists. And I'm just going to call this Amazon because that's going to be where we're pulling the data from. Then we want to say, well, what fields do we want to have? We want to have the name actually, let's call it Item, which is going to be a text field, date, which is also a text field. Because with SQLite three, if you look at the documentation, it doesn't handle dates. It just suggests you use a text field with an ISO format or an integer field with the epoch time. And then we're going to have our data. Now I'm going to store this as a Blob type. I don't know whether this is better or worse than using as text because we're storing HTML, it's all text anyway. We'll try Blob and we'll see how we get on. So that should work for us. So I'm going to save this and then we're going to run this and we don't get any errors. And then we'll look in my file manager and we do have our database. It has been created. So we'll have this table in there. Now we're going to pull data for a few different products. I'm just going to copy these over because they have the Amazon ASINs in them. So we can basically just say, hey, go to these ASINs and give me all the HTML. And that's what we're going to save in our database. So we'll do with sync Playwright and I'm going to call this as PW. We will do browser is equal to PW chromium. Now I'm going to do connect over CDP, which is the Chrome DevTools protocol? You can just do launch here and that will work for you. I have another video where I talk about using connecting over CDP. So I'm going to run this command on my screen, which is basically going to spool up a chromium headless instance on port nine two two that I can connect to, which I'm going to just do now. Localhost nine two two like this. That's the only change. If you're not doing it this way, everything else will be the same. So now we can have our page is equal to browser new page and we can then loop through the items, go to the page, load it up and then give us the HTML back. So we'll do for item in items we'll do page go to and we want to say our URL, which is the Amazon URL https WW amazon. Co UK slash DP then we can use an F string to put in the item asin here like so if you aren't aware this is just the quicker way to get to an Amazon product page. When you know the Asin, you can just do it like got. So now we have actually gone to this page. We want to construct a Tuple with the information that we have that matches our table. So we can use an insert using the placeholders. So I'm just going to call this Tuple current and it's going to be the item which is the Asin which we're going to store in our item field which we created here. Then the date which is datetime. Now that's going to give us our date stamp, which is going to have the time that we actually pulled this information. And then page content, like this content, the page content is the HTML. So we save that to our database so we can pull it out and look at it later from here. What we want to do is we want to do our Kerr execute again because we want to execute against our database our insert command. So we'll do insert into the table name is Amazon and we want to put in our name, date and data. And we're going to say the values is equal to our three placeholders here and then current because this is our tuple that we're basically saying, hey, put this information into our database. We can then just do con commit to commit this database. Commit this information into this database. I'm going to add in a couple of print statements. So we'll do print, extracting, and we'll say item just so we can see this is what we're doing here. And then we'll have a print one down here just to say saved again. Item is probably fine. We'll put the date in as well, like this. Okay. And we didn't save the date into a variable. So we'll just do item. That will work just fine. Cool. So what I'm going to do now is I'm going to run this again. I'm going to come out of my terminal, I'm going to come out of Neo BIM, and I'm going to run it in my terminal. So we'll see that we're extracting this information and then we're basically saving it all into our database. We got an error and I've used the wrong column name. Okay, so no problem, we'll fix that. So it's item, date and data and I must have put in name. Okay, so I put in name, which is not right, item. There we go, fix that error. Let's run it again. Cool, that worked. There's a couple of options for you to check and see what information is in your database. You can use DB browser for SQLite, which is pretty handy, or you can access SQLite three from your terminal, which is what I'm going to do. I'm going to run it here and we're going to do open like it suggests, ELT DB. Now I'm in the same folder, so I can just access it like this. So if we did tables, we'll see that we have our Amazon table down there. Now we can run a query, we can do select Star from Amazon and our colon. And when we hit enter, we're going to select everything. We're going to see a ton of HTML come by. So just to double check that we've got the right information, I'm going to do item so we only get the items we can see that we have two there. So I'm going to exit from here and I'm going to run my code again just so we get another set of data in there. So you can see when we actually work through it. We have extra in there now. Okay, so that's done. So the idea behind this is that we now have the raw data saved in a database. So we can go ahead and write some code to query our database and then get the information out from it that we want. So I'm going to write a new file here. So we're going to call this transform PY and again in Neo Vim. I'm going to open it up. Now, we need to do some similar stuff here, so I'm going to do import SQLite three. And again, our connection is going to be equal to the same thing. Now, if you were going to make this more repeatable, you would probably have your own database file so you can reuse all of this and you probably handle any exceptions and errors a bit better. However, we are just winging it today. So we're just going to do it all again. There's only a few lines of code. Again, con cursor, super. So we want to query by item. So what the idea is now is that we're going to say, let's say we've run this code every day for a month and let's say, okay, give me all of the items and give me all of the HTML that belongs to this item. And then we'll query against that HTML and find out specific bits of information. So I'm going to say that we need to have an item. So I'm just going to copy from here. This is one of the ones we've got data for. And now we can actually write our query, our SQL query to get the information back that belongs to this item. So we'll do Kerr execute and we'll do select and we'll do select Star because we want all of the information from it from Amazon where item is equal to and we'll have our question mark in there because we're going to use placeholder. Now the Placeholder is expecting a tuple. So I'm just going to put item create a tuple like this now. Now we can say that the rows is going to be equal to Ker fetchall. So this is basically say run this query and get us all the information. And now we can just have this information here. So we'll do for row in rows, let's print out the type of row and this should be a tuple. So we'll run our Python transform and this needs to be DB because that's obviously not right. We can see that we have two tuples back, so we know that this is working. So if I was to go ahead and print out row and index, for example, index number two and run it now, hey, there's a load of HTML back, so let's import an HTML parser. So I'm going to do from selectolax parsers, we'll import in the HTML parser, you can use whichever HTML parser you like. I like selectolax. Now we can say our HTML is going to be equal to, we can pass it into the parser and we can say row and we'll index the second one here. Now, let's go ahead and see if this works. Let's print out HTML CSS first and let's ask for the title. We'll do text, and we also want Strip is Equal to True to get rid of any unnecessary white space. And my LSP is going to complain at this, which we're going to ignore. There we go. So we now have two lots of HTML from two different timestamps that we've saved into our database, which we're then pulling back out and querying against for the title. So let's say it wasn't the title that we wanted. Maybe we wanted the price. Well, I know that the selector for this is something like this one off screen, so let's see if we have that information there. We do. So we have the prices. So you could then construct what information you want from this raw data and query back against it. You could search against the date that you pulled it from. So you then have basically all that raw information available to you, rather than whatever it is you decided to pull out of the HTML at the time. So hopefully you can kind of see the benefits for that here. Although, again, we're now going to create a big database full of a lot of redundant HTML. It's up to you to decide whether you think something like this might be useful for your use. Case, if you've enjoyed this video, you're going to like this one here, where I talk a bit more about how to actually get the data out of the HTML once you've grabbed it.