 It's very easy when scraping a site like this one to get lost trying to do the wrong thing and overlook some really obvious things. So if I go ahead and do inspect element on this and we go and load the page up again, we might expect to see some Ajax or something like that happening. Now there are a little thing, a few little things going on, but none of these are really what we want. In fact, this is giving us HTML back. So the next thing that I would say is to have a look at this page source and actually check the HTML. The first thing I always do is search for the name of the product, for example, like this. You can see we're going to get 162 instances of this product name. So I'll go a bit further and I'll say, well, let's search for the color because this might be a bit more specific and a bit more interesting. And then we start to find things on this page that look more like a big chunk of data that we can access. So if we look here and you see this line here, it's got some information on it. Now this goes all the way across the page. So I'm going to go ahead and hit line wrap and we're going to go back down to this and there's this whole chunk of data. Now I believe this website is a view JS website and this is the view model that's being rendered out, being sent to the page so it can get all the information. But this is everything here. And it kind of looks a little bit like JSON data. But as you'll see, if I go ahead and try and copy all of this out, I'm going to do this the long way around, grab this and stick it into a JSON parcel like this one. So that's to look about right, but we get this error here. So if we remove the first parts even, or we're still getting an error, that's because this is not valid JSON. If I start by putting the quotes around this variance thing, you can see it just tells me there's an error at position 14, which is this one. We'll move that 41 and on and on and on and on. You might think, well, there's no point in going through it. I don't want to go through it to put quotes around every like key or what would be a key in a Python dictionary or in a JSON key. But fortunately, there is a Python package that can do that for us. It's called chomp.js and I've used this quite a few times and I always forget that it exists when I scrape sites like this one. What it will do is we can give it this information, all of this that's in this script tag here, I find it again. We can give it this and we can let it then pass that out and it's going to give us a Python object back, a dictionary, which we can then export to JSON. What I'm going to do now is I'm going to run through a bit of an example to of how I would get this information out, stick it through here and then end up with something tangible and usable at the end. I'm just going to quickly grab and copy the URL. I'm going to go to our code editor. This is NeoVim. Everyone keeps asking me. This is the basic IDE by Chris App Machine. Google it, you'll find it. It's all set up for you. It looks pretty cool like this. It's really easy to actually work with and change as well. If you like it, you should definitely try it. NeoVim, I go. I'm going to be using requests and we're also going to use select.alax, because we need to actually pass that HTML. We'll do from select.alax.parcer. We'll import our HTML parser, which we're going to need. Then we're going to import in our chomp.js as well. Let's go ahead and set our URL up as what we just copied. This nice long thing here. We can start to make the request. I will do response is equal to requests.get and I'm going to give it the URL. You might need to use headers and sometimes put a cookie in. Other times you might want to make use of a proxy. If you're looking to scale up your web scrapers and take it to the next level, the first thing that I would recommend that you look at is getting yourself a high quality proxy, like the one from the sponsor of today's video, IP Royal. I've been working with them for a while now and these are the proxies that I use. In fact, the ones that I recommend that you guys try are the Royal residential proxies, because these are 100% genuine residential IPs. This makes all the difference. They'll it will auto rotate for you and are super easy to put into new and existing projects. It's just one texturing you need to put into your requests. You can choose which countries you include, which ones you exclude, and they can all handle multiple concurrent requests, too, which means async is absolutely fine. If it's just throughput that you need, though, you might find that the data center IPs will work best for you. So if you think this can help out your web scraping programs, go ahead and check out the link in the description below and also use my code, JWR30, for 30% off your first Royal residential proxies order with IP Royal. So once again, thanks to IP Royal for sponsoring this video and let's get back to our script. But for this example, we're just going to leave it like this, because I think it's going to work for us. Now we can say our HTML is equal to the response dot text that we get back. And we need to do this, actually. We need to say it's our HTML parser. And then we give this the response dot text. So this is now our HTML all set up ready for us to work with. I'm going to save this and I'm going to come back to the page. And we need to go and find this script tag. Now this is an HTML tag, just like any other. So we can reference it using the script part and this type Java text slash JavaScript. So we can go ahead and do something like our data is going to be equal to our HTML dot CSS, because we're going to be using CSS selectors. It's a script tag because we want to search for the attributes we're going to use. The brackets here, actually going to make this double quotes. So we can use single quotes inside. And we'll do type is going to be equal to, and this is where our single quotes help text slash Java scripts. Wow, I can't type today. That's really bad. Like this. So you can see I'm using utilizing this and because we have the double quotes on the outside, we can use the single quotes on the inside. Let's go ahead and print out the data and see what we get back. Python three main dot pi and we should get. And this isn't a method. That's my fault. It should be this. There we go. So we've got nothing. And once you figure out the you spell JavaScript wrong, which means you weren't actually matching the elements that you wanted to and you run it again, you'll see that we get all of these elements back the scripts. Now that's because there are that many script tags on that page that's being run. So we need to get make sure we get the right one. There's a few different ways you can go about this. You can either find it by indexing it. This will work probably the easiest option, but becomes a bit more brittle because it's a you're going to be indexing a specific item from a list, or you could loop through them all and just apply the the data to each one and just pull the bit out that you need. So what we'll do is we'll just do the loop first. So we'll just say for script in data, which is all of our tags that we just looked at, we'll say that we want to do new is equal to chomp. Our JS job.js dot and it is past.js object. And we're going to say script and we need dot text because we want to give it the text from that element. And then I'm just going to do print new. And when we run this, we're going to see that we're going to get invalid input. And that's because if we go back to the site and search for the first one of these, the first one, if I can get back to the first one, please, is going to be this one here, which has nothing in it. There's no text, which means it's going to cause us some issues. So one of the things you can do is you can just do a try and accept block. This is not ideal. I think I think if we were going to make this a bit more stable, we'd want to find a better way. And I'm also going to do accept past, which you should never do, but I'm going to in this case for the moment for this demonstration, we'll find a better way to deal with it. So I'm going to enter and there's all of our data. So we are actually getting all of this information back and chomp.js has turned it into a Python dictionary for us, which then becomes, we can become Jason. So what we're going to do is we're going to reference our variance, variance, which is a key in the dictionary, which I looked up earlier and we'll run it again. We get a bit more manageable chunk of data. You can see how we can actually now really get this information from this page and it has everything in it. So now we've actually got the right information by checking for that variance key. We're going to go ahead and we're going to save this to a Jason file. So we'll do with open and we'll just say export dot json. And we need this as right and as f for our file. Then we can do Jason dot dump and we'll give it our new, which is a terrible variable name. Definitely don't go with that and go, we'll go up to the top of our file and we'll do import Jason and save. So in near them, capital G goes to the bottom, double G goes to the top of the file, really useful. Let's run this again. No output. That's great. So open up our file explorer and there is all of the Jason that we just pulled out from that one page and saved. And now Black will actually format that for us and here everything here is everything. This is all the information. Okay, so this is not liking this that well because it's not formatted properly, but you still have this all here available to work with. You could take this data and you could pull parts out. You could put it into maybe a data class or maybe into pedantic or something like that. And then you could save parts of it if you wanted to. But there's all sorts of information here. Here's the all the images. Even more what's right at the bottom reviews. Okay, so this is all the review information and this is all just there on the page. This is all of this information in here on the view source of the page for this product. A good thing about this is that this website is going to be structured in such a way because it's using, I think it's view. This will work for any of the products on this page. So you could get a list of URLs for products and easily loop through and pull them all out using this method here. This will work for you just fine. Now again, we haven't done anything with the data here that would be up to you to decide what you wanted to do. This is just a demonstration of the actual method itself, not a beal and endle. So things like except past, you wouldn't actually do or leave into your code. So please just be aware of that. If you've enjoyed this video, you want to watch more web scraping content. You want to click on my channel up here and go through some of the awesome videos I've already made.
