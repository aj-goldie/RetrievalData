 Scrapy has everything you need to tackle a web scraping project. It's full of built-in things that help, item handling and loading, various pipelines for databases, comprehensive settings for crawling and scraping, and a whole host of other great features. So then why don't I use it as much? To be able to answer that question, I think it's important to talk about the main pain point we experience when trying to extract data. It's not finding a way to pass the data we have or even trying to work out how we're going to output it or save it. Now it's actually extracting the data from the source. If you've watched any of my videos, you'll see that I've demonstrated many different methods on this specific subject and for good reason. This is by far the hardest part and can involve several different approaches and methods or even a combination of them. If you've ever used playwright to load up a page, get the headers and the cookies and then pass them on to requests, you'll know what I'm talking about. But once we have a reliable way of getting the data into our hands, the rest of it's usually plain sailing and can be achieved in many different ways depending on your needs. This is the reason I think I find myself using Scrapy Less is because I spend most of my time working out how to extract said data rather than worrying about what to do with it after. Now I'm not saying that that isn't important because obviously a complete web scraping system will include outputting to various formats, handling errors, missing data, etc. And as I mentioned, Scrapy does all this very well. But I think it's more about what you're actually trying to achieve and what the data sources are. So let's talk about the most common way to scrape data from the modern web. A lot of sites these days are front end systems that connect to a backend API. This API serves structured JSON data to render on the page. This generally works in a few different ways but it's important to understand that the data is actually being called from that other source rather than the actual front end site itself. Scraping is now more about finding that source and using it rather than like sort of downloading and passing the HTML. And in fact, this method is often easier than you might think. Many websites don't actually lock this down properly or you'll find it can be bypassed by adding in the correct headers and cookies. This kind of just shows me that the actual amount of work and the effort goes into the extracting of the data rather than anything else. As you're aware, the hardest part of scraping is actually getting the data that you want from the website and what we've been talking about, People always ask me what they should do and I say the first thing you need to do is get yourself a set of high quality proxies. The ones from today's sponsor IP Royal. They are super easy to put into existing or new projects. It's just one tech string that you can put into your proxy code, works with scrapie requests, even playwright and selenium will accept proxies too, so you'll cover it across all bases. So the ones that I suggest you try are the Royal residential proxies. These are all 100% genuine residential IPs. Now this is important because these are the best ones for scraping data. The other ones, the data center proxies are useful if maybe you want more throughput, but the residential IPs as I think is where it's at. They will all auto rotate for you. You can choose which countries you want to include, which ones you want to exclude, and they'll have unlimited concurrent sessions too, which means async is absolutely no problem. If this sounds like something that you might need, go ahead and check out the link in the description below. Also, use my code JWR30 for 30% off your first Royal residential proxy order with IP Royal. I want to be clear though that there are still plenty of websites that are just plain HTML, and there are plenty of good reasons to crawl them, but generally speaking, I feel like this is at the other end of the scale. Scrappy does excel at this. It allows you to quickly and easily create web crawlers that follow links based on rules, extract data, or save it as you go. In fact, Scrappy does a good job with JSON too, and we can absolutely do what I just mentioned with Scrappy, but in my opinion, it feels like a little bit, well, overkill, maybe a bit too complex for that job. The counter argument to this, of course, is that if the complexity lies in the data extraction, then why not use a framework to do and help you with the rest of the process, allowing you to focus on the main part of your task. I do think there's a lot of merit to this point too. The tools that Scrappy provides does indeed aid in the process of storing data, running crawlers, etc. and handling errors, all of this stuff that we do need. So I always say this, what are your goals for this project? Are you just grabbing the data and running, or will you need to run this daily for the foreseeable future, or are you managing a spider and a network of data pipelines? If it's the latter, you'll certainly see benefits from using Scrappy, but if you answer no to any of those questions, then perhaps writing your own solution in playing Python is the better option. This leads me to my next point too, which is that Scrappy is definitely not Python beginner friendly, whereas web scraping in general can be, so it gives you a bit of a juxtaposition here. It creates a structural project with settings, uses object-oriented programming, and expects you to have a good basic understanding of the language. It's a bit like Django for creating websites, I think. There's everything there ready for you, and it does certain things very, very well. There's all the features you could need, plenty of plugins, but there's inherently more code, more going on than just a single file script that you can run. I often refer back to the fact that as a Python beginner, Flask always looks more appealing due to its perceived simplicity. For me these days, I prefer to write total custom scripts. I like tools like HTTPX and SelectoLax and to handle and pass my own JSON and H2. I use data classes, I can pick which ORM I want to use, or even write the raw SQL queries myself. However, this is because of the nature of the scraping I do, which is tends to be more one-off data grabs and building data sets rather than prolong scraping of the same site. I am aware that this is my own personal experience and use case. If you are learning Python and you are interested in web scraping, then I would definitely recommend you try Scrappy. Give it a go and see what you think. You might find it becomes your perfect web scraping partner. If that's you and that's your path, then I've got a video right here that's going to show you the basics of how to use Scrappy.
