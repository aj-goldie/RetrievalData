 One of the harder parts about web scraping, getting the data from the web is knowing which method to use, which is the correct one for the site that you're looking at and also which is the correct one for you, what you're trying to get out of it. The first method is your basic HTML passing. Now this is really important to know how to do and you'd be surprised that this still works on a lot of websites out there and it's really powerful. So it's nice and easy to do. We make a request to the server, we get back an HTML page and we just filter through and pass out what we want from the what bits of data we want from that HTML. It's benefits, it's very easy to learn, it's very easy to use, it works very well and we can also make async work with it very, very easily too, which means we can actually be really quick and efficient when we can use this method. This video is sponsored by IP Royal. So I have been working with IP Royal for a while now and these are the proxies that I use in my projects for web scraping. One that includes the scraping methods that I'm talking about in this video. So they offer quite a few different types of proxies, there's data center etc but the ones that I recommend that you try are the Royal residential proxies. These are 100% genuine residential IPs and these are the ones that you want to use when web scraping. They will all auto rotate for you and you can choose which countries you want to include or not include and it's super easy to integrate into your code. There's just one line that you need to put into your request. There's unlimited concurrent sessions too which means async is absolutely not a problem. So this is definitely a good option for you if you're looking to scale up your web scraping. IP Royal have also given me a code JWR50 to give you guys 50% off your first Royal residential proxies order. So check out the link in the description below and use my code JWR50. So to check if you can actually use this method, the thing that I suggest to most people is you just look at view source on that web page. That's going to be a much more accurate representation of the information that you're going to get back into your code to see if the information is there that you can pass it out of the HTML. If you can, the tools I recommend you use for this are either requests or HTTPX. HTTPX is one of my own preferred now because it's easy to stick async in there if you need to and pass it out with Bs4 or my preferred option which is SelectoLax. I also prefer to use CSS Selecto as more than anything else now so that would be my recommendation there. But if, for example, you have a JavaScript heavy website or you need to do something like click on some buttons or scroll down or anything like that, then this method here is probably going to work better for you and that's using some kind of JavaScript rendering service. Now in here I'm going to include things like using playwright or selenium. To me, I would consider that a service that we're going to use to render the page because we're going to use it to actually load up the page itself and work through the JavaScript. Get it all rendered out and then return us back the best representation of that HTML that we can then use and go back to the first point and pass with our SelectoLax or whatever programming you're using. My preference is playwright. I find it the most easy and simple to use at the moment along with it being quite modern and very well integrated into Python. Then we just use that instead of our requests or HTTPX to actually load the page up. Now you can often get away with doing it headless but sometimes you can't and you also need to be a bit more careful about running into captures here too. There's a thing called playwright stealth or selenium stealth which helps remove some of the telltale signs that this is a controlled browser rather than the real thing. From there I just go ahead and grab the source of the page. I wouldn't recommend passing the HTML that you get or trying to get data from the elements within that unless you just need a tiny little bit of data. I find it much easier to return the whole page, the whole HTML and then pass it from there. I actually recommend this method to a lot of people because it's one of the more simple ones to use. We know exactly what's happening. We can see the page loading up and we can see the information coming back. If you're just trying to get a few bits of data for a few different sites for personal use then this is an absolute no brainer. It's a really simple way to do it. If their website that you're trying to get to is maybe you have your login or it's a bit more JavaScript you can't find the information. The downside of course is we are running a browser so it's much more resource heavy and although playwright does run asynchronously it's not that simple to actually manage and put in and you have to run multiple instances of a browser which is not that great. The last one is going to involve reverse engineering the websites API. This is best for full single page application style websites or anything where you can see that the front end is really heavy JavaScript. We can look in the developer tools and the network tab and you can start to load up more of the page. Maybe there's a load more but an off-flick through different pages. I have a look and see if you can find the request that's being made by the JavaScript front end usually by Ajax to its back end to request that data. From here we can then actually mimic the request that the front end is made by matching the cookies and maybe some other kind of session IDs or something like that and then we can actually make that request our self and get the JSON data back directly into our code. Now this is a bit more complicated this way because we have to manage those cookies and other things as well so it's a little bit more difficult in that respect but if the information is available like that we can actually get more information with much less calls much less requests than you would do if you had to load up each individual page because quite often you can actually tailor the request that's being made to give you more up more data per call from that API. This is a bit more harder to manage and isn't always as clear sometimes you need to hunt around a bit before you can find it but this would be my preferred method for large scale scraping if of course it's available on that site. For this you just need to have a look and find it that's the hardest part then you just need to make a request copying all the headers and the cookies and then just grab the JSON data that comes back. So I'm going to give you another one and then I'm going to talk about an actual framework and how that can fit in but quite often it's worth checking the view source for the page because you can find that inside of a script tag somewhere on that page there is a load of JSON data already put in that's ready loaded on that page ready to go into the elements. Now this isn't as common but if it is there what we can do is we can just pass that out you can ask for those script tags cut the bits out that you don't need and grab that JSON data so it's always worth looking for. So the final thing I'm going to talk about is a web scraping framework called scrappy for Python and it fits into all of these methods. So I don't tend to use it maybe as much as I should I'm much preferred to write my own very tailored scripts for my purposes but it is actually really powerful and it has everything included in it that you're going to need. If you're doing the first method the basic method it's got you covered you can have a spider up and running very very quickly. If you need to render JavaScript there is a scrappy playwright integration that can do that for you and if the data is behind that API well it's the same thing nice and simple. So I would highly recommend if you haven't already go ahead and check out scrappy. I've got a video on it for beginners right here which I think you will enjoy.
