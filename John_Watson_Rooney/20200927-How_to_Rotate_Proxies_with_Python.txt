 One of the main problems we face when web scraping is the fact that we get blocked from the website,
 we are trying to scrape for too many requests. That's because we have one IP address from our main
 computer and it sees too many requests from that IP address and it will temporarily stop you
 accessing the website. So one way around that is to spread out the requests over multiple IP
 addresses and this is basically what rotating through proxies is. In this video I want to show you
 how that works in principle and I'm going to explain a few upsides and downsides and stick
 around because I want to talk a lot about free proxies and why they are not actually any use to us
 at all. So hi everyone, welcome, my name is John and let's get right into it. So the crux of it is
 that when we send a request to the server through requests in Python we want to be able to send
 that through a proxy. Now the easiest way to do that is to import requests and let's set up our
 i SQL to requests.get and I'm going to use a website called httbibin, httpbin and it has an ip part that will
 let you it will send you the JSON response back and it will tell you what IP you connected with
 and whether it was good or not. So we can do htbin.org and it's /ip. So if I was to run this and print out
 our dot status code we can see that we're going to get a 200 response down here. So that means we
 can a to find, I'm not going to show you the JSON response because that's my ip. So if we wanted to
 use a proxy we need to go and get one. So if you googled free proxy list you might come up with this
 and if I refresh this page and we find one that says https says yes. So let's copy that and the
 port is 3128. Let's go ahead and do proxy is equal to
 3128. After our URL we can do proxies. It's equal to it's create a dictionary
 and we'll say http and then proxy and we'll also say https
 there we go like that and proxy as well. So I'm going to add another thing here I'm going to say
 timeout because a lot of these fails I'm going to do a timeout of three seconds and let's
 run that and see if this one worked. Okay we got connection refuse which means it to me that
 that does not work. So let's try a different one https let's try this one and we did that one
 oh that's this one 8080.
 Okay yes this one worked right so let's do print r.json
 the response the JSON response from the website and to clear that up and let's run it again
 and we can see that our response is this the origin that we've come from is 1641013128 which is
 the proxy that we specified here. So basically that's it the downside with three proxies is the fact
 that they are free and they are widely known and open and Google blocks them all so you cannot access
 any sites with this proxy. So this we just saw this one worked. So if I change this to https
 Google.co.uk and try to run it again we're going to get the same connection refuse error
 that we got before. In fact we got a JSON error because we tried to print out something that doesn't
 exist. So what I'm going to do is I'm just going to put that into a try and accept and we'll show you
 a few more examples and we'll just print failed and we'll pass and we'll put our JSON response
 in here so if it does work we get that instead.
 Okay so let's try it again and we'll get after our timeout time which was about three seconds failed
 Okay so I'll change this back to the http then http then sorry
 /ip and we'll run it again and maybe this one still works. There it does great.
 So this is all very well but what if we had a long list of proxies that we know work with Google
 and we know work with what we're trying we know that work with what we're trying to achieve
 so the sites that we're trying to scrape. The quickest and easiest way to go through that if you
 had a minis CSV file would be to import them in CSV so I'll show you how I would do that now. I've
 got a long list of proxies about four or five hundred saved as a CSV file on my computer which I
 downloaded from various free proxy sites there are 458. So what I'll do is I'll run
 right a script that loops through all of these and checks them all to see if they work.
 I'm going to get rid of all of this for now and we'll keep that and we'll do import CSV
 and then we can do let's create a blank blank list to add all of our proxies to
 and we want to do open our CSV file so we use a context manager with open which keeps
 open for us and closes it for us when we don't need it and mind is proxy list.csv and I'm going to
 open that read only so that's all we need and I'm going to do as f so we're opening the file as f
 and reader then is equal to csv underscore reader of the file which is f we called and then for row
 row in reader a pen list to our proxy list a pen and because it's multiple
 because it's multiple rows we not to make sure we do just the first one so we're going to index it
 and say only this row. So if after this check this is working if I print the length of proxy list
 hopefully if that's all good it should be CSV reader is not defined that's because that should be a dot
 sorry about that there we go 458 which is what I said it was so now let's go ahead and write a new
 function to loop through every proxy I'll close this collapse that sorry to go straight through
 every proxy on our now in our big long proxy list so I'm going to call this one
 extracts and we're going to pass in the proxy that we want and then I'm going to do pretty
 much what we had before and we're going to do a try because we want to make sure that we don't
 fall over one one of them fails and we do r is equal to the crests.get and it was I'll just type
 the URL manually again like this and proxies equal to proxy so this matches here so this is
 what we'll get past in except when we need to do is we need to do the HTTP
 and we'll do HTTPS just so it works quite that
 close r dictionary and timeout I'm going to lower this to two because there's quite a lot to get
 through and most of them are going to fail I think okay so if that works we want to print
 our r.json so the response from the website which we get and then after that I'm just going to say
 working just like that.
 We'll add a little dash in there so we can see that's all good and now what we want to do is we want
 to deal with it if it fails and we'll just do print actually what we'll do is we'll just pass it
 if it fails because otherwise we're going to add our text on our in our console screen and it's
 going to be quite confusing we won't see which ones work and which don't so after this we want to
 return and we'll just return proxy for now.
 Okay let's test our function we can use the same proxy that worked before this one which was
 80808080 so now we'll just run extract and we'll give it this so this then becomes our proxy variable
 and we'll see what we get out.
 Working fantastic so we know our function works so basically what we're doing is we're giving it
 a proxy and we're trying the request which I just showed you with that proxy and if it works
 it prints it to the console if it doesn't it just keeps over and goes to the next one.
 So if you watch one of my last videos I talked a little bit about concurrent futures and how we
 can use that to speed through a long list of requests or items or in that case I used
 I used URLs that we could potentially scrape and got one piece of data from each one
 and now this is another great example we could use it so I'm actually going to go ahead and import
 concurrent futures and we're going to run through this super quick and check if any of these proxies
 work. If you're not quite sure what this is just follow along now and I will have a link somewhere
 to the actual video I did recently which explains it a bit more detail. So with concurrent
 let's spell this right got futures dot thread or execute her as executor this is just lining it
 all up and we will do executor dot map and we want to then give it our function extract and then
 our proxy list. So when I run this it's going to do it all sort of simultaneously and we should
 hopefully get some responses back that some of the proxies in my CSV file work and doesn't appear
 that any of them do. We can see them slowly coming through so so far there are four that actually
 work and of course this is as in just work and not work with Google which these ones won't.
 Okay so that's finished so we went through 458 free proxies of which I downloaded and saved
 into a CSV and there are one two three four five six seven eight nine of them actually work
 as in you can use them however remember that none of these will work with Google so they're
 kind of pointless but this is not really the outcome not really the point of the exercise that I
 wanted to show you it was more that if you had a list of proxies that you know worked you could
 use this method and you could use your request and pass in your proxy just like this
 to spread out your requests and hopefully not get blocked from the website wherever you're trying
 to scrape. So all of the code that I've written out today will be in my github I'll put the
 links down below to that I also have another version where we web scrape this free proxy list and
 pull out the information from there so that might be a slightly more up to date
 free proxy list so maybe a few of them more will work. So give it a go follow along and see what
 your outcome is and again if you have a list of good proxies you could use this to scrape
 websites properly and quickly without game blocked. So hopefully you guys have found this one interesting
 it's quite useful it's good to know how to do it I'm going to do a follow up video to this where I'm
 going to
 try and find some proxies that actually do work with Google and do that in a proper exercise so
 thank you for watching guys don't forget to like comment and subscribe
 lots of web scraping content on my channel already more to come more Python stuff to come
 main videos on Sundays and some more live streams are going to come up next week so I'll
 guys so to make sure you keep your eye open for those and come and join in wherever chat thank you
 very much and see you next time bye
 [BLANK_AUDIO]
