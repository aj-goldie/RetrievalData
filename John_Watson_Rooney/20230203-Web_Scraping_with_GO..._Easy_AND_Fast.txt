 We all know how good Python is for web scraping. I've built my channel around it But I like to challenge things and I want to share with you how good I think go is for web scraping But only in this specific case now there is a go web scraping Chrome framework called Kali and it's excels in HTML passing and scraping. I want to share with you in this video how it works How easy it is to write how logical it lays itself out how powerful it is how quick it is So let's get started and I'll explain that along as we go Okay, so we have the skeleton built out now. We have our package which our main package I'm importing in Kali that error will go away when we start using it And I have my main function ready to start so the first thing that we need to do is we need to create an instance of our Kali which is Kali dot new collector. This is what's going to allow us to do everything So the main thing that I like about this is this c dot on HTML and the way that this works So what I can do is I can say that whenever we're on a page We can do something whenever we find HTML with an element that matches the selector that we're gonna give it we can then do something with that So think we can actually like move through pages we can select pieces of information to save and we can just crawl through in that way I think this is a really good approach of doing it And I think that you'll like it as well if you haven't seen this before so what I'm gonna do here is I'm going to go ahead and say well this is going to be the selector for the next page So we just grab that and show you where that is if you're not familiar with this already This is next page button down here we have li tag with a class next and then an a tag so what we need to do is li dot next these are just CSS selectors and then a then what we can do is we can Sell it to do something now what I want to do when we find this next tags I want to visit that URL So you see dot visit and then I'm going to say h dot Request and I don't need that one age dot request and then dot absolute URL And then we're going to say that because our age is actually representing that element that we found with that a tag We just need to do a dot a ttr to get the attribute from it of h ref So This is basically saying that our age variable is representing this element So if we wanted to find things underneath it we can do age dot child text or child attributes But in this case, we're just going to do this one another thing that we can do is we can do c dot on request Now this is particularly useful because what we can do here is we can just see where the pages are See what we're going and what we're doing or you could run any of this code under here in this side This when you actually make that request, but I tend to use it just to format dot print print ln and I'm just going to print out here something like visiting and then we're going to give it R dot URL So this is just going to print out the URL that we are visiting on that page very handy You see it in other Wobscrapping frameworks too So before we think about grabbing any information This is grab this URL here and do c dot visit again And this is going to be the one that's going to start it so we're going to do here So I'm going to save this now Now we're actually not collecting any information at all, but I'm going to do go run scraper.go And we should see all of the pages come through and this is basically us making a request and getting back every page that it finds Notice we go through all the way to the end until we don't find this element this Li next a tag so it stops after that What we want to do now is we want to think about grabbing information So we're going to do c dot on html again We're going to put our selector in there. We'll have our function again All nice and neatly filled out. Thank you very much to go please and neovin which is the editor that I'm using Which is fantastic. You should definitely learn how to use it And now we can actually do something with this information So I'm going to go back to the website again. I'm just going to find what it is called Product pod so you can see on the left hand side how the whole thing's highlighted and we want article class product pod So this is a bit of this part is going to be very similar to any Python web scraping that you've done so we'll do article Products Pod like this and now we have the access to this element that we can do anything with so I'm just going to show you will do format Dot print line again and what we're going to do is we're going to do h dot child attribute Okay, so we're going to say inside this element here. We're going to find the h3 tag That then then the a tag underneath and what I'm going to tell you that I want the attribute that matches title So get rid of this blank line So now if I save and run every time we hit this element We're going to go to the next page But every time we find one of the elements an element that matches this we're going to run this code So you can see how we don't need to do any loops or anything like that. It's all done for us. Let's just clear this up So we should get at the end of this Alode a names and these are all of you can see them all here loads and loads of book title names So we can expand on this and we can basically then go ahead and fill this out to what we want Before we do that what I'm going to do is I'm going to create a struct Which is going to be what we're going to put our data into so I'm going to call this one type item struct Stacked Like this This is where I'm going to define What information is going to go into this struct and the types that it is So this is a strongly typed language So if you're not used to this you come from Python This might seem like an extra step to you But trust me It's worth it and you will get used to it really quickly So I'm just going to say this is the information we want I'm just going to copy this over and explain it to you. There we go. So I've got one extra thing there So all I've done is I said there's going to be a A link name price and in stock This is a little bit like defining a dictionary beforehand But we're saying that each of these has to be a string So if I tried to put something that wasn't a string into the struct We would get an error And the JSON part is for when at the end We're going to move all of our data From the struct and we're going to marshal it into JSON format Just so we can see it a bit clearer out at the end And the names the link name price under the JSON is just me saying what the JSON field is going to be called So now I have this item struct What I'm going to do is I'm going to create a Items variable here and I'm going to say this is going to be equal to A slice of item So this basically says that this item is like a blank list Is going to be a list of any of this item struct So we can only put this item struct in here But that's fine because that's what we're dealing with So I'm going to remove this And I'm going to copy this over with all the selectors again Let's put this in here There we go So these are just the elements here and the selector So you notice we've got child attribute for these two That's me saying I want the h ref from that a tag I want the title from that h3 then the a And the child text is just the text from these two elements Now we need to actually append this to our items list So we do items dot append And you'll notice that when we append and go we do items append Then items again and then what we want to append it to Right so that's done now So what I'm going to do is I'm just going to come down at the end And then we'll do format dot print line and items So now when we run this we should go through every single page Then pull all that information out and at the end I'm going to get my items or spat out back at me There we go So it's a bit hard to read but you can see Here's the square bracket which is the end of our slice And here is one item with the information in there's the link The name the price and the stock value from that thing there But I want to have this in JSON we did that with the struct at the top So I'm going to do let's say our data and error because this JSON Marshall is going to give us an error back if one exists So we need to handle that so we'll do JSON dot Marshall And I'm going to do Marshall in dent Which is going to let me pass in my items slice And then say that I want it to look like this This is just basically telling it how to indent the information So it's just easier to read Then we'll handle my error so if error is not equal to nil Then we'll just do not that, thank you We'll just log dot fatal this error Handle your errors properly I suppose is what I should say And then we'll do fmt dot print Line and I need the string representation of the data Because otherwise we will just get a load of bytes load of numbers So I'll just run it again get this information back And there we go nicely printed all of the information that we've gathered I'm going to copy some code over that I pulled out But I didn't make up obviously I pulled it out of stack overflow So thank you to whoever put did that and let me grab this so what we can do now is we can actually time this It should have been ported in time at the top Usually does that automatically I just need to put in another line we need to defer our timer Which we've just created on our main function Like so great So let's save Run one more time and we should get A nice printout at the end of how long this took to run 5.27 seconds great for 50 pages Now moving on to I guess what people are going to find the most interesting So if you skipped across here no problem But we're going to make this async I'm going to show you just how quick this can be But there is a couple of things that we need to think about first Now to create and to make this async is super easy Which is one of the things that I love so much We don't have to mess around with anything we can literally do This true true like this I get my brackets right and then add in at the bottom here C.wait we want to after our visit because we want to do this However, this is not actually going to change anything because we are basically only getting We're only telling it one URL you one URL at a time So it can't do it asynchronously We need to give it a load of URLs to start with So we can actually make use of the time saved whilst we're waiting for requests to come back We can make more at the moment We're just saying here's a page go there and it'll find another page Go there find another page It doesn't know that there are 50 pages So we could either do visit and copy this down like 50 times We definitely don't want to do that But what we're going to do instead is we're actually going to change our approach ever so slightly So as well as going for grabbing the information which we need and going through the pages What we're going to do is we're actually going to pull the information from this on the left hand side here These categories and that's where I think this becomes really good If you're doing HTML scraping and you have loads of categories or you can get loads of links to start with Making an async scraper with go and collie is super easy So we need to grab all of this information and change our start URL So to do that we need to do again c.uc.on html because we're going to be going to the first page and doing the html And it is div.sidecategories Li URL i this is why I love CSS select it so much Now from here we want to say our link is going to be equal to i'm doing this at a slightly differently h.child attribute from the a again h ref And then c.visit the h.action absolute URL I'll request first request or absolute URL on this link So I've just I've just put this across two different lines in the moment But now what we're saying is that when we go to this first page which i'm about to change We're going to grab all of these links which means we can make use of async when it's here So I'm going to make travel my start page because it's the top one And we'll go down We'll comment out this and paste that in and save Okay, so let's get rid of wait first and let's make this not async So we have a base starting number Save run We can see that we're going through all of the categories here and some of them have extra pages Which we should see come up there you go you can see page actually xxxx whatever 8.09 seconds to get all of that information So now we can just redo our async like so And our c.wait because we need to wait at the end and let's run it again You see all the links flash out like that 1.67 seconds So we've gone from eight and a bit seconds to 1.6 to get the same information Just by changing our approach ever so slightly and doing the c.wait and c.thecoli that async is true So if you like the look of async and you want to do it in Python you absolutely can you're going to want to watch this video right here [BLANK_AUDIO]
