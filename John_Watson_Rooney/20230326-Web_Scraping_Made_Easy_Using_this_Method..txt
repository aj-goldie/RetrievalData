 I think this is modern web scraping. We just ask the API for the data and it gives it to us There's actually more common than you might think really although most cases you're gonna need some kind of headers or something But in this you don't snuffing there You just have to ask it make a curl requesting it all send you the chase and back all structured Needing tidy everything that you want in this video I'm gonna show you what to look for give you a couple of options of how to export it to so let's get started Right, so it's clear if you look at this page on the on the left that We don't even need to check the HTML or nothing. This is this is JavaScript. We're not load more I wouldn't even bother looking at it to be honest. I'd go straight to the inspect And go straight to the network tab It's running here So I'm just gonna delete all of this stuff and I'm gonna refresh the page and let's see what comes through The first thing I'm noticing these ones here that say endpoint. There's a number and this one says page is equal to zero That's a dead giveaway. So I'm gonna move this to my other screen or zoom in a bit We're gonna be focusing on this endpoint here So here's all the headers and stuff and this can be quite interesting sometimes However, don't always get caught up in this just check first But let's make sure this is the data that we want go to response preview. Oh, like it says total records current page Items per page. This is just an API response as we were expecting Score nodes and here we are. Here's all the information that that page was being given from the API to load up and to store This is what we want What I suggest you start doing is just grabbing the request URL. So I'm gonna copy the value I'm gonna open up a window here, and I'm gonna go curl and I'm gonna paste it in and there it is That's it. That's everything. No, you need to do to get the next load is to change the page number Now I didn't even look at changing items per page. Maybe that works - But how can we turn this into a Python script so we can run it and get all the data and have it all together Nice and neat. That's the main thing. So what we're gonna do in this video I'm just gonna make sure I got this copied we have our script open here So what we're gonna do is just create a short script to grab this information and we'll either And we'll output it to a CSV file and we'll also output it to a long JSON file So I'm gonna import in a couple of things. I'm gonna use HTTP X today. I really like HTTP X request will work fine Or whatever you like. I'm also gonna use rich Because it just helps when I print things out as my terminal so you guys can see them. I'm gonna import print You're absolutely don't need this. It's just handy. So to export stuff You can absolutely write it using the CSV module in in Python if you want to I use pandas. It's using a massive library for a tiny thing It works What to say and we're also gonna need JSON because we're gonna export things as JSON So the first thing we want to do is have a function that's going to download that JSON file. So we'll call it download JSON and we're gonna give it let's say we'll give it a URL so we want to know what we're getting out Then we'll do our response is going to be equal to hccpx.get this URL Remember I said this one worked without any headers or anything like that So we're not gonna put them in now when we looked at the response on the Over here in the preview You'll see that we have this all the data is under this score nodes thing So I'm actually just gonna spit that out. So we'll say for node in response dot JSON And it was score nodes like this. What we're gonna do is we're actually gonna yield it out So this is gonna turn this into a generator function. It's just a nice easy way of Looping through and getting all of the nested bits of information out without having to then end up with lists of lists That's why I use yield in this case. I was gonna yield it So this is gonna be JSON data. So let's try this. So we'll have our main function So we'll just do main And we'll say our URL is equal to is this still in my clipboard What is there we go and we'll do our? We'll print so as this is a generator function. It's an iterable. So we'll do four item in Our download JSON URL and we'll just print out the item for the moment Okay, and so then we just need our main function our main if name is equal to main Oh dear, I've lost it Main there we go main super Right run bang. So you can see them will come by. This is all the information. This is each individual university So this is all chunks of our data that we want So we want to actually be able to loop through the pages. So there's a couple of ways you can do this You could check that there's 35 pages and just hard code it Which is what we're gonna do or you can make a request And if you're gonna do this with an API that you're gonna work with more and more You can make a request and you can then ask for the total pages then then use that or the total total records or something like that We are just going to loop through it. I'm gonna store everything into a results list and we'll do four i in Range and remember this started at zero 35. Please So we'll do this and we just need to then let's make this in the middle of the screen indent this code There we go and There it that's instead of printing the item we will do our results Dot pen the item so we're gonna save each each time our download JSON yields something the problem It's generator We're gonna save it to our results list as I said this way We don't get a list of lists every time we do something with this function So let's go ahead and then print out the length of our main List and I'm also going to print out This so we'll get rid of that only that we'll do print page I and here we're gonna turn our URL into an f string and where it says page equals here We're going to make this I like that. Let me format this with black There we go sorted So let's go and run this and we should get find that we get 35 35 pages and 521 individual results Hopefully we do get that there we go 521 so we know we're getting everything So we want to talk about exporting this out now Now I'm going to use pandas But we're also going to save it as JSON So let's write the JSON Function first so we'll do death save to JSON And we'll just give it our Data like this So we're going to do with open We'll use a context manager with a file And we're going to call this one results dot JSON file and we'll make this Rightable and as f so we're opening the file as f Then we can do JSON dot dump. I'm going to dump all of the JSON From our data into our file Okay So let's get rid of the printing now and we'll just add in on this other line here We'll do save to JSON Our results there In fact, we'll write the CSV one and we'll do them both for the same time So to do CSV we're going to do Save to CSV and again, we want to give it the results And then we can make our data frame So we'll just call this df for data frame Is equal to pd dot data frame of our Results i keyboard skills results then we can just call df dot to CSV and we'll just say Results dot csv and i'm going to get rid of the index as well I really want my index it's false result Okay, we'll make that results actually For let's make it data so it's consistent with our other function to Great But let's add our save to CSV in here save to csv Results so we are going to make both of these save Okay, let's run it now And we should get a csv file at the end of this and also a large JSON file results JSON that we can actually Let what looks through there so that's done so there's our csv file and our JSON file I'll open the JSON file here in neovim We're going to Format with black and no black is a python thing but this does seem to sort it out well enough I don't think I've got a JSON one installed It is going to take a second though because this is like 30 000 lines. I think there we go So we can then search for Harvard there's Harvard I don't know any other ones. I saw Toronto was in there. Yeah Yale. Yep, there we go You can see those all the data all neatly structured all exactly everything that you could want From that website all nice and simple It's not always this easy But sometimes it is and if you're interested in this sort of way of scraping You're going to like the this video too, which is more the same thing (upbeat music)
