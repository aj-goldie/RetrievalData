There was a lot to like about this Web scraping package. It was requests and HTML passing all in one CSS selectors, XPath, all this cool stuff, and JavaScript support, which meant you could load up a chromium browser using this with just one extra line in your code and have it replace the HTML that it sent back with the HTML that was rendered from the page. So if you were dealing with JavaScript pages, it was as simple as putting this one line of code in and then you could easily go ahead and get access to those JavaScript sites. I spent a lot of time using this package when I was learning Web scraping. It was really good and I was kind of sad to see the fact that it got neglected a little bit, not updated for a long time. I understand that the maintainer probably didn't want to spend any more of their time dealing or managing this, which I totally understand. However, recently, I think in about April this year, we do have a new maintainer on this package. I'm kind of hoping for lots of good things to happen with it and I'd love to contribute to this myself. And in fact, you can still use it now. It does work. The main issue why it stopped being useful was the render stopped working and that is because it uses Pipeteer, which as you can see here, has been unmaintained for a long time ago. A long time. And it does say that you should consider using Playwright Python instead, which is what we would probably all use now for page rendering. But there is a fix for this. In fact, I looked through the issues and this guy here suggested it. I came through this and you can see that by setting the pipeteer chromium version before doing the install manually, it does still work. And I'm going to show you that this does work. And we can still use request HTML if you wanted to, for that really easy way of passing and working with JavaScript sites. Not all of them are going to work, obviously, because we're spinning up just a simple headless Chrome browser, which does get blocked quite a lot, but is a really useful thing to have and even for maybe just figuring out what works and what doesn't on a website. So let's go ahead and get this all installed. I'm going to create a virtual environment like so this is obviously really important when you're working with anything Python. I'm going to activate it. Act is just a shortcut I've created in my terminal emulator for like a source VNB bin activate I think is the long way doing it. Highly recommend doing that if you're on Linux. Now what we want to do is want to pip install so it's requests HTML like this. This is going to run through and install everything that we need. And now what you want to do is you want to do this export command here, pipeteer chromium version. Now this is just the Chrome version. I used this one because it was in the GitHub suggestion here from May 26. This worked for me, so this could still work for you. Now this is obviously exporting an environmental variable on a Linux terminal. If you're using Windows, you just need to look up how to set an environment variable in your PowerShell or whatever like this. So you want to hit enter on here. And now we want to do pipeteer install like so. And this is going to install the chromium version that you've suggested it uses here, rather than the old way of doing it, which was basically just calling R HTML render and having it choose whatever version it was defaulting to. Mine is already installed because I've done this before. So otherwise this is going to install chromium for you in the right place and now we can go ahead and just write some code. So let's do main PY do from requests HTML will import in our HTML session. Let's set the URL to just this test URL for the moment and let's put that in here. Now what we want to do is we want to create a session. So I'm going to say our session is going to be equal to the HTML session that we've imported. And this is basically going to give us everything that we need to make requests. So then R is going to be equal to session, get on the URL, it's going to give us all the basic headers, like user agents, et cetera, et cetera, so we don't need to worry about that. And now what I'm going to do is I'm going to say let's print out r HTML find. And if we go to the website here and have a look at the inspect find, the element names, each one of these you'll see is called a div class of quote. So we'll do div quote like so I'm going to save this and I'm going to run it. We get a blank list down here, nothing came back and that's because this page is all JavaScript. And if we were to view the page source and searched in here for div of quote, we're not going to find any because it doesn't exist. All the information is put in dynamically. What we're going to do now is just add in that one line r HTML render like so hit save, I'm going to have to ignore my LSP and I'm going to run it again and we're going to get those elements back in that short space of time. The chromium instance has been loaded, it's been given the information for our request and the HTML variable here that we're using has been replaced with the HTML that was rendered by that browser page pretty quick and pretty in handy. And this is the main reason why I really loved this library. Obviously, this is a super basic example, but if you've got any websites that need basic JavaScript rendering, this is a really good option because it saves you so much time from having to write any playwright stuff. Although we do have slightly less control over the browser stuff in here. If I go to here, you can see that we have some extra parameters that we can come in any JavaScript that you want to load on the page. So maybe something like scrolling down, maybe the scroll down, which is the integer, which does work. I always found it a little bit cumbersome, but it did work. Weight was always useful. Sleep I found to be very useful and I never really used so many of the other ones, but it does work well. Hagen talks about scrolling down. One of the other things, whilst I've just remembered that I thought was really cool was the fact that we could go ahead and do things like this, where we could say print r. HTML next. And this is basically just going to find that next page link and give it to us, which meant you can see it here, which meant that getting the next page and creating a URL really simple. We can just go ahead and get that out and then go ahead and move on to the next page. Again, you have to ignore my LSP, so hopefully you kind of get the idea and you can understand that this is still a good library to use. I'm really pleased that with these fixes they're suggested by these guys here and it does still work. So, as I said, if you're looking for something where you can do some simple JavaScript rendering, make a real quick script to grab some data, I think this is a good place to look. If you're more interested in more web scraping methods, go ahead and look right here on this video. I'll talk more there.