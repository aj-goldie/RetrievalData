Welcome, everyone, to the ontology summit 2024 on neurosymbolic techniques for and with ontologies and knowledge graphs. Today is the 27 March 2024, and we're really excited and looking forward to very interesting talk by Marcus Bueller. And now I'd like to turn the meeting over to our convener, Ravi Sharma. Oh, thank you, I will. I welcome everyone else on the call and the speaker, but I request Ram to introduce him briefly, and then I like to take a minute to introduce. Yeah, thank you, Ravi. And again, it's my pleasure to introduce Marcus Bueller. And he actually doesn't need much introduction, but I'll introduce anyway. Marcus is a professor of civil engineering at MIT, and he previously served as the head of the department. And one of the things about Marcus is he does a lot of interdisciplinary work. And in fact, I got a message from someone on LinkedIn saying that here we're going to hear the Leonardo da Vinci. But actually, Marcus also got the Leonardo da Vinci award from ASME several years ago, and he's been working on a number of areas which are of actual interest to this community in ontologies. And he kind of integrates ontological reasoning with generative AI like systems in several fields. And the topic today is more in terms of engineering, actually, it's another manufacturing session, but it's an engineering designer, materials design. Now, the techniques that he has developed can be applied to a broad range of topics in both biology and engineering. Marcus has won several awards, and he has also been made a member of the National Academy of Engineering recently. So, at a very young age, Marcus has achieved a phenomenal amount of recognitions. So, without much ado, I hand it over to Ravi to make a final remark. And then, Marcus, again, welcome to the ontology summit. We are very pleased to have you here. Yes, thank you, Ram. And thank you, Marcus, for coming. The point I want to make is that each one of the prizes that Marcus has got, besides his own distinguished career, is a hero and a motto for all of us. In physics, for example, Feynman. Of course, you do remember Feynman, along with Schwinger and Gelman. But Feynman was the apex of the team. Similarly, Drucker prize, similarly rice prize. I mean, where do you stop, Marcus? Don't stop. Please continue and wish you all the best. We are a little bit older generation, at least I am. So I very much welcome you. I'm excited to hear, especially about your multidisciplinary work in this area. Thousand papers in one discipline. Let's hear from Marcus. Great. Yeah. Well, thank you so much. And I mean, the thing really that I want to share goes to my students and collaborators that I had and my mentors in over many years possible. So let me, let me share my screen. And, yeah, I'm truly excited to be here. I've been interested in this topic for a long time, and so I'll go through kind of some fundamentals in the techniques we've developed. And I don't know how many of you are familiar with AI and generative AI in this field. I think many of you are somewhat familiar, but I'll review a little bit of the work we've done in that field as well. And then the second half of the talk will be really heavily on graph reasoning and related topics. So I've been working on this, actually, for a long time, and some of you might know the work I've done back in 2010, 1112, and that was sort of shortly after I came to MIT on category theory and graph representations of knowledge with ologs, with David Spivak at the time. And so for many years, I've been trying to build graph based systems. I can understand and, you know, represent and maybe predict new connections and new knowledge across different fields for many years. This was very hard to do. And actually, we couldn't really discover new things other than using traditional scientific methods, where we go to the lab, take measurements, and humans basically look at the data. Now, we have abilities to automate this process and actually make discovery through AI systems. And that's been an interest that I've been extremely interested in exploring, to explore to what extent we can do this and what the challenges are and what this might mean for the future of science and engineering and applications. So the right hand side is kind of an old paper from my lab, which you can take a look at this as well, of course, but the left one is a paper that I actually put a first version on archive last week. I've updated it a few days ago. So if you go to the archive link, you can find it. That's sort of the backbone to a lot of the stuff I'll talk about today. And there's also code with this, which I haven't released yet, because I haven't had the time to really fix it up and clean it up. But it will be open source and everything, so you can run all these techniques, like you said earlier, Ram and Ravi, on your own data, on your own system. So the goal is to share that, and so it will be open source, but to show you a little bit of what I'm doing. So since not all of you know what I'm interesting, so saying I am interested in a lot of different systems, yes. But I am a material scientist at heart, and fundamentally, and I do look at materials primarily that look like that on the left hand side. So they have usually multiple structures from the nanoscale to the macro scale. And the goal in materials understanding science, engineering, is a lot of times to figure out how does a material work and how do we design new materials for a new purpose. And that's, I'm sure, something that many of you have thought about as well. That's an incredibly complex problem. And one of the reasons why this is so complex is that these are really what we call heterogeneous systems. So they're not really hierarchical in a sense, they're scale separated. In fact, I'll show you examples later that a lot of these systems have to be understood really in its entirety in the entire set of scales and features. And we don't know at priority what is important and what is not. That's something that I've been struggling with for a couple of decades, since I got into this field, trying to figure out how do we do this? Well, and I think we have now actually figured out how to do it, and I'm very excited about this. So the structural complexity is very, very large in materials. This is a snapshot of something like a protein on the right hand side at the nanoscale and then the same protein at the macro scale. You can see that there are obviously lots of different features in there. And the difficulty is that you don't know what are these features are used for? What if you're interested in understanding what is the significance of any of these geometric features or other features? And if you're a designer, you want to be able to predict what is my optimal design for certain purposes. And even if you just take a subset of the space, if you look at a single molecule, it's already a gigantic a design space which is far greater than anything conventional techniques can solve. And I like to look a little bit at the history. So you mentioned Feynman and others, and that's great. You know, I think we got to think about or take a step back. You know, how do we model physics and physical systems, especially these multilevel models of real materials? And for many years, we've done, I put it in the 1950s here. Since 1950s, we had reasonably fast computers that could essentially solve, you know, equations that usually are preconceived and I and I say this, you know, because that's usually what happens. You know, humans look at data, and then you figure out an equation and use computers to solve the equations most of the time. And that is sort of done to various degrees, but usually that's the process. And it's very slow, of course. You know, it takes a lot of time. And I would argue that, especially if you're thinking about a multiscale modeling paradigm, which is sort of what this left hand side shows. And I'm sure folks at NIST have looked into this as well. It is somewhat limiting because you assume at some point you can separate the scales, and that's something that we have learned the hard way. I would say that's actually not possible in most cases. And you cannot separate the nanoscale from the macro scale. I mean, once you know how to do it, you can, but you don't know usually how to do that. So that's a challenge that we've been struggling with. Now, fast forward a little bit into 2020s. That's when we began to see that we could build generative AI methodologies that could actually begin to learn concepts in a more meaningful way. And what I mean by this is they can actually, first of all, they can integrate a lot of different types of information, physics, math, we know, but also huge amounts of data far beyond human capabilities. They can ingest, like you said, thousands of papers. They can potentially read over these results, and they can also communicate results to us in a way that we can interpret them while having an internal representation of what they're working on or modeling that can be extracted, if you wish, directly from the numerical representation of the model. And, of course, we are on a path perhaps towards what we call AGI, which is something that would give all of these abilities a huge boost. And so once you have much more generally intelligent systems, a lot of the questions that we're struggling with today might be overcome. This is an interesting time, and I'll explain in the presentation, really why I think this is such a special moment and how these systems can be applied to physics and materials and engineering and other multidisciplinary systems. The key is really that we want to be able to improve the ability by which we, we use artificially intelligent systems to learn. And that's a theme that I'll try to hit a little bit throughout the presentation. And I didn't have too much time to really polish this, so please apologies for this, but I hope to get the point across. So you want to think about how do we build AI systems that can deal with all these different kinds of data sources and concepts, things we have discovered already as humans, things we haven't figured out yet? And how can we use this to not simply do what we call curve fitting? And I say this in a very provocative way. You know, a lot of sort of early days machine learning was to train a model to solve one task. And a lot of times this is done by fitting statistically representations of statistical obstacles of data and to make, you know, likelihood predictions of some sort. But we want to think a little bit more, a little more deeply about how we can improve on this and not just do, you know, fitting data, but actually, actually train models to get a level of understanding of what they're working on. Okay? And that is obviously work in progress, but that's what I'm really interested in exploring. What are the tools to do this, and what do we need to do in innovation in terms of the architectures as well? What you see on the slide actually is an example of a genai framework that was used to design a really complex material which is visualized, actually on the right hand side. It's a mycelium composite materials as a living material that encompasses concepts of living systems and reorganization, but also a variety of engineering materials like proteins and clays, and work with texturing mechanisms. And it's sort of a beautiful example of how, in a system like this, we can come up with truly unique combinations of, in this case, material mechanisms that have not been utilized together, maybe not even on their own. They can create a whole concert of interactions, which provides a very powerful outcome. And so I'll walk through in the slides how we got to this and talk about some of the methodologies that underlie this particular type of machine. Basically, if you wish, that can solve problems like this. So before we do this, I want to talk about, and by the way, please interrupt me. I really mean this. I mean, I have lots of slides, 60 or so, and, you know, I can go through all of them, of course. But if you have questions, please interrupt me, especially as we go into more of the substantive parts of the presentation. Please interrupt me anytime. So how do we model physics? Right? So I said it earlier, we use the multi scale paradigm a lot of times. And this means, for example, if you're a molecular modeler, like, that's how I grew up, essentially. When I did my postdoc with Bill Goddard and, you know, and others, we kind of learned how to model materials by creating intertwined potential. And we use this to solve a multiparticle problem, a statistical mechanics problem. And this is powerful. However, you pay a price, because we only understand local interactions between the atoms. And in order for us to understand the collective behavior of the system, what do we need to do? Well, we need to actually integrate, um, over time and space, right? That's the price we need to pay. And so then we can describe, you know, the movie. On the right hand side is a, you know, the emergence of dislocations in a. In a very large system with hundreds of millions of atoms. And you can see there's some really interesting structures forming, which couldn't be predicted. Just by looking at the potential. You actually have to run the dynamics and do the statistical ensemble analysis to predict that at the end. And most other problems like this, too, like in polymers and proteins and biology, you're going to have to do things like that, and that's not practical most of the time. So we've been thinking about how do we change this and what we really want to do. I want to say, okay, so instead of starting my process by assuming interfaith potential, so I've provocatively crossed it out. And maybe, I don't know if Feynman would agree with me on this, but let's say we cross it out and we say we don't want to use this potential, because it kind of puts us, it's a very human centric, geometrically euclidean approach, but actually, maybe that's not the best way of describing how this very big multi product systems actually behaves. Maybe we can have another way of discovering governing laws that are equally foundational, but maybe more elegant to solve the hypothesis? Basically, there isn't just a single way of predicting this physical system modeling. It actually might be multiple. And who says that we have discovered the most efficient way of doing it? And obviously we haven't, because so expensive to do computationally. So the idea is to say, okay, if we want to describe a fundamental physical foundation, we can do this through graphs. You know, graphs are a very flexible way of representing a lot of different kinds of things. You know, it could be numbers, it could be atoms, you know, like. So this inter potential is sort of a special case of a graph model, because it creates a graph interactions at the atomic scale, and we integrate on that. We integrate on that scale on that graph to get the dynamics of the atoms and molecules here. But we can also train models to discover graph representations that are non trivial and actually are very complex and are beyond human abilities to model a system. And in this kind of graph representation, we are not limited to euclidean representation. So a point in a graph, a node, isn't necessarily just an atom. It could be an atomic, but it could be other concepts. It could be a knowledge concept like in a knowledge graph, but it could also be a feature, a pattern in a system. It could be a dynamical phase change. It could be an event happening. It could be a combination of different features that together form a cascade of events and so on. So this is the sort of thing that we are very, not very good at, I would say, as humans, to write down and actually come up with something. And this is where AI comes in very handy. You know, it's a tool for us to build models that actually learn these graph representations. And the other thing we get from this is that if you're dealing with graphs, you know, I can actually describe. So I mentioned earlier, I work on category theory, and that's why I like category theory, is it's a, you know, very abstract, very general sense of how we can represent, you know, anything, almost any. Anything we can come up with, you know, mathematics, music, art, literature, engineering, of course, and other things. So if I have a graph representation, I can naturally describe, you know, everything from the early civilization, ideas of symbols, mathematics, you know, sophisticated, you know, differential geometry and so on, all the way to computational methods, which really have, you know, given us a lot of leg way in the last century, you know, 1950s, sixties, seventies, when we had computers. Today, we can actually combine them. And that's something we haven't been able to do. Right? So an early computational model, you program that model and you create a solution to basically a differential equation most of the time, or some other governing law. However, you couldn't easily integrate that with other knowledge, like knowledge about, you know, symbolism or philosophy or, you know, knowledge that's written down in a text or maybe artisan knowledge about how materials are manufactured. So there's a lot of things that we can't model in an equation, can't model as a number, and those are kind of separate worlds from these kind of early stage computational models. So what we're trying to do now is to build models that can ingest a wealth of different modalities of information. And. Yeah, like shown on the slide here, kind of schematically, right? So you have an AI model that can build graphs, that, you know, model knowledge as a graph representation of information, but do this in a way that can ingest many different kinds of things, like simulation data, experimental data, equations, books, papers, patents, you know, even interviews with people that make materials, like I said, artisans, you know, and so on, legal documents. I mean, there's many different dimensions to a process, an industry or in a society that deals with the making of something or the framework in which societies work on different systems so that all can be. Should be ingested, essentially in a system like this, and then model as a graph. And so this is what we've spent a lot of time on, is trying to figure out, how do we actually do this? How do we build models like this? And these models, if they work, they should be able to describe, for example, the spiderweb, which is a complex biological material based on a set of graph representations. And the graphs should be sparse in a sense that they are really universal governing laws that describe not only how this spider web behaves around a certain set of training observations that I have, but ideally, I want to describe truly how the system mechanistically functions. And that's what graphs can do, especially if I train the model to learn, you know, really elementary relationships in a. In a. In a way that is as simple as possible. In other words, graphs are as sparse as they can be to solve, you know, a certain. Certain level of task. And, um, this really, and I have a quote of Feynman here, of course. And so Feynman talked about, you know, the. The importance of knowledge versus information, right? So the idea that you, you really want to confirm, convert your information, which are your measurements or things, you know, about something, into how it's connected. And this is exactly what we're training AI models to do, is to figure out how to take these pieces of information, how to connect them, and also to learn something about how important certain features are, certain elements are, and how they contribute to answering questions about the behavior or future behavior or inverse problems and so on and so on, and how they relate, of course, across different disciplines and different modalities of inquiry. So this is something that we're trying to do. Now, how do we do this? Well, there's sort of two or three things that I want to highlight. One is we want to think, if we build AI systems, we want to build them with something in mind. I think it's really important. And that's to go beyond saying, I have a model, that I'm asking a question, I'm getting an answer. Because what we really want to get into, I think, is to build AI models, frontier models that are able to think. And the thinking process doesn't have to be a model of how humans think, but it has to represent somehow a mechanism for thinking and a mechanism for learning structure and graphs are a great platform. But I really want to emphasize on this point, we want models to not just statistically infer on patterns. We want models to have some level of understanding. Now, how do we prove this and how do we do this? It's an ongoing debate, but that is the direction I'm pushing really hard in the work we do is to build models that can actually do this or do this better. And the question here is, how do we provide a structural space for these models to actually do that? And one of the ways we can do this, actually is to look at biological inspiration. Again. So we look at biology and you have something, and this is a busy slide, but bear with me on this. On the left hand side, you see the construction principle by which biological systems are made. And a lot of the biological systems are made from or through a mechanism we call universality diversity paradigm. UDP. And what is UDP? UDP is a framework that essentially says in biology, most biological mechanisms, functions are created by using existing platforms, like proteins, saccharides, chitin, you name it, the universal molecules, DNA. If a biological organism needs a new function, it wouldn't create a new type of DNA. It uses the DNA we have, the proteins we have, and it rearranges them structurally to create new functions. And in a way, what I've been thinking a lot about is, in the AI developments, a lot of times we don't do this, we actually build a model, and we build a model for a certain purpose. And that's especially true for more traditional machine learning techniques. You have one model for one purpose, and you want to get the model with a new capabilities, you train it again. So we've been thinking about building models that can actually be trained. Yes, and that's important. But they can also have an ability to think about themselves in a sense that they can, instead of retraining parameters to solve a new task, to simply use existing structures in themselves to solve new tasks. And this is something we can train models for. We can, we can do this, for example, based on graph forming attention models, which is the basis for a lot of the stuff I'll talk about by forcing these models to do what we call two forward passes. And so, in other words, you know, instead of asking the model a question and getting an answer, I'm asking a question. The model understands it needs to think about its own structure. So it looks at the question, it thinks about the question and asks itself, you know, how can I reconfigure my own architecture, not the parameters, but the way I'm built, such that I can better answer the question. And this is a learning process, so we can teach the model to think about itself, that there's a training process involved, and then you make a prediction. And this allows actually these models to essentially work on solving new tasks without having been trained for these tasks. And this is inspired by, of course, a lot of work that's been done, you know, in the last years on new methodologies of being able to do better inference, especially in the world of multimodal language models, which is a powerful framework, because these models are able to deal with multiple modalities. They're also able, of course, to deal with graph representations because they are ultimately in the interior of themselves, graph models. And I won't go into more details, you know, the mechanics of this, but those of you who work with these models, of course, understand or know that, you know, a tension mechanism is a graph forming mechanism. And if we can build on this and leverage this, we can do really great things with these models. And that's what the rest of the talk will be, will be focused on this, and how do we leverage this to higher, higher levels. So this kind of way to think about models, to self reflect, and instead of giving an answer right away to learn how to think about. And here's a visualization of how this looks like. So the model sort of what we're showing here is how the model thinks about itself, its own structure, and how it rearranges its own structure as it answers the question. So, again, it's not just answering the question, it's actually reflecting upon its own structure at every single iteration. When we predict a new pixel or a new word or whatever we're predicting now, that's not enough, though. And even though this is interesting, because it gives us the ability to combine a lot of different expertise and skills. So if you have models trained on the ability to reason or the ability to recall certain types of knowledge by doing these self awareness training loops, you can combine a lot of these in innovative ways because they can be combined. It's a graph forming mechanism. So you can get emergent new skills and abilities from this, but you can go even further. And the next level of what we do in pretty much all of our work now is multi agent modeling. So you also want to think about that a model can be powerful on its own, but it really has to have somebody to talk to, if you wish. And the talking doesn't mean necessarily in human understandable language, but it needs to exchange information. And that information can be cryptic, it can be machine readable only, or it can be readable for humans. And that's actually quite flexible. So a lot of times we build models that have multi agent setups. And so, in a way of thinking about it as a multi particle system, which is where, again, where I come from, this creates a system of systems where you have, now a particle is an agent that has very nonlinear relationships with all the other agents. And it's sort of, for me, yeah, this looks like an inter potential in quotation mark. And you get amazing things. We know that we have non linear relationships between particles, and we put a bunch of particles in a box and we simulate how they evolve dynamically. Some really interesting things can happen. And exactly this is happening here as well. So there are going to be capabilities evolving and emerging that were not obvious by looking just at a single particle, but they come about because we have many interactions. So these kind of systems can solve incredibly complex tasks fully, fully autonomously. So now, let me talk a little bit about really briefly before I go to the applications on kind of like, the space we're in. So we're dealing with materials. Design is a huge space. You know, we're dealing with, for just a small protein that has maybe 100 amino acids, we have something like ten to the 130 possible designs. And of course, there aren't even enough atoms in the universe to make all of these. And so even if you had a extremely rapid engine to make, make these materials in the lab, there wouldn't be enough atoms to really make all of these. So this is clearly an intractable challenge. I mean, probably all of you are aware of that, but sometimes when I give a general audience talk, I show that, and I want people to understand how complex these are, even for small systems. The other thing is the heterochro feature of the system, and that's shown here. So there are many examples. One is disease and biology. So you can have a mutation in a protein, and it could be a single point mutation. In other words, one out of hundreds of thousands of amino acids are different. And sometimes this means absolutely nothing. So people don't get sick? You don't get sick. But in some cases, if the mutation happens to be at the wrong spot or the right spot, depending on how you look at it, you're going to get a disease like rapid aging disease, brittle bone disease. There are many others. It's an outsized effect of something that's really tiny. If you were to be a tumor, so traditional engineering approach where you would say, yeah, I have a protein, I'm going to average out this protein. I'm going to make it into a continuum model or particle model that does not work here at all because you're going to miss these features. And those features are only visible once you have a large system. In other words, if I look at the protein that causes this rapid aging disease, I could by no way figure out this is going to be a disease, a pathological mutation. It only works once I understand how this protein behaves in the ensemble of an organism, a human. And the same is true for fracture problems, factor problems. We have usually small scale defects, things around cracks, and then you've got a big system that you're interested in, an engineering structure, and you have this outsized effect of something really tiny at the crack tip. And in a similar vein as in disease applications. So there are a couple of things that are really interesting now. I mean, we have sort of a framework for what we can do, but we also have compute needs, and that's the famous Kurzweil curve. I mean, basically talking about how much cheap compute is available. And, you know, and that means we can basically add computational complexity and still solve problems. And the idea is that if you have enough, if you have a good idea on how to build your model, and if you have cheaper compute, something really, truly interesting can happen, because now you're making the system more complex, but you're getting an outsized effect of your complexity, hopefully on the behavior. And so there's lots of different debate about this. Is the emergent effect in large language models real? Is it a mirage? Is it something? But let's put it that way. I think there's definitely something interesting going on, and it's not something that's totally out of the world, because if I take a multi particle system in physics, I know their size effects. So if I take, you know, ten atoms, or if I take a million atoms or billion, and if I confine them in a particular way, like in 2d or 1d, we know they behave very differently. So size effects and emergent phenomena is something that I'm quite familiar with from statistical mechanics. And, you know, I believe something like this is happening here as well. So we have complexity, we have a framework that allows models to learn in a more intelligent way, and we can do pretty exciting things. Like, for example, what I show you now are sort of graph based methods that can solve really complex forward and inverse problems. But in the beginning, what I'll show you are these are just single modality methods. Like here we have a model that can take an amino acid sequence as an input, and it can predict the motion of a molecule, like in this case, a protein. So without actually ever seeing the protein, it can predict, it understands how to predict how this protein will move. So it has a model representation of the structural dynamics of this particular system. That's pretty mind blowing. And in fact, I think for me personally, the only way I could have done a problem like this a couple of years ago would be by running a very large scale molecular dynamic simulation. Now, I can do this basically on my laptop, once I have trained this model. And so those are the kinds of things like alpha fold, predicting structures of proteins and so on, that are very, quite powerful with limitations. But I think we've got a long way to do this, and we can make very accurate predictions, especially if I can check predictions against physics. So that's something that, of course, is very important here, and happy to talk more about how we do that. But the model like this can be trained really based on molecular mechanics, quantum mechanics, quite accurately. We can do more complex things. We can solve inverse problems. So this is a paper we published last year on design proteins using this case, a diffusion model. So it's a combination of a diffusion attention model. There have been a lot of reports of things one can do with this that are very powerful. For example, if you know the structure of a molecule you want, you can design for that. I can design the amino acid sequence or the polymer composition or the chemistry that makes this particular design, and you can basically solve an inverse problem that would have been totally intractable until very recently. And this can be extended even to dynamics problems. So I can. Now, this is a review of literature a little bit back to the 1990s, when people started looking at dynamics of molecules, proteins exposed to mechanical forces, understanding how they unfold and how they fold and how they refold. And there's a lot of work being done there experimentally as well as computationally. These are very expensive, obviously, because we're dealing with dynamics of very small things. And even though they're small, they're very complex. What we can do now is we can very efficiently solve the forward problem. So if you give me a design like a sequence, a chemistry, I can tell you how this molecule is going to behave dynamically. But I can also do the inverse problem. And that's the amazing thing. I can actually, if you tell me this is the kind of behavior I want, dynamical behavior of my molecule, I can give you design suggestions to build this molecule. And we use this in the work, just sort of as a sideline in lots of notifications, obviously, and finding new placements for plastics, building super strong fibers with what we're doing with the army and DoD and others, to food applications, work with the USDA agriculture, creating new proteins as food substitutes, or as applications that can be used to make better food and do tissue engineering for medical applications. There's lots of different ways by which you can apply these techniques. You can always also apply this to more conventional engineering applications. So this is an example where we're applying this to designing composite materials. And again, you tell me what kind of composite considerative relationship you want, and I can design it for you using this algorithm. Solve the inverse problem. So this is sort of a very interesting world where I can do things that really weren't possible, and I can do fracture problems as well. So I work a lot on fracture passion of mine. And we also develop models that can describe dynamics of fracture as well. Now we can go even further than this, and this is sort of where now we're getting into the more graph based representations and so on. So, first of all, if I have a multimodal model, and here we're actually using this x Lora model that I introduced earlier. So such a model can do many different things, can actually predict, like here shown, force extension behaviors of proteins or chemistries to predict numbers. It can solve the inverse problem, but it can also reason over the results. And this is a little bit small, but probably on the screen you can see it. So if I go in and I say, okay, I have a bunch of different protein sequences. Yeah. Calculate the properties. And so it does that. And then you can say, I want you to figure out what's the best sequence for the highest stability. Okay, fine. You know, here's the one, the model response. And then I can say, you know, tell me why this is right. So what is actually going on? Why do you think this sequence is the most stable one? And because the model is able to access a lot of different ways of discovering its own internal representation of knowledge, it can make these connections. So it will tell us, yes, the strongest protein, the most stable protein, is the most stable because of certain type of chemical bonding and so on. So the whole conversation sort of talks about these in a human AI interaction in this case, which ultimately, I can validate this by building a model of these proteins using more conventional techniques, physics based modeling. And I can check these, and I can actually see that, in fact, what the model has predicted, actually is happening. So this is a very interesting evidence for the fact that these models are actually very good in building internal representations of the physical world, if trained in the particular way that we're training them with. I can do a similar thing for polymer design. So this is another example, and I think it sort of shows a workflow similar to the previous one, but a little bit more from a designer's discovery perspective. So I'm starting off and say, you know, I have a molecule, which is this guy on the top here. And I say, you know, I want to use this molecule. And I randomly picked this from a, a data set that I was working with. We randomly picked it. I didn't pay any attention to what it was. It just said, here's my molecule. I want to make a polymer out of this. What can I do? And so it gives me suggestions, and what are the different strategies? The most important one is thinking about reactivity. And so then I go ahead and I say, okay, I want to focus on reactivity. I actually have the ability, and this is how my model has the ability to compute and solve the inverse problem for twelve different features, quantum mechanical features of molecules. And I tell the model, these are the things I can actually compute and design for. What are the three ones you want me to address, and how do I change them? Do I make them larger or smaller? So the model gives the answer shown on the bottom. And then I actually implement this. And so now I'm actually creating an autonomous agentic solution system, where I basically say, here's my design objective. I want to increase, you know, so I want to increase the homo lumo gap, dipole moment and perishability. These are three things I want to do, and I want to decrease the band gap, and I want to increase the dipole moment ability for increased reactivity. And then from then on, the system actually works completely autonomously. So it generates solutions. It uses the ability to check the solution by using an agentic interaction. So these are adversarial relationships. So the, the agents aren't just plotting each other, they're actually fighting it out, right? So the one agent makes a suggestion based on its generative abilities. The other agent checks the results and analyzes the structure, and analyzes the properties, and there is an interaction going on. And then ultimately the system will evolve through different iterations to come up with an optimal design, which is shown here on the right hand side. Of course, I'm getting ranking, I'm getting a whole set of different solutions. And, you know, at the end, I get this molecule, which I can then analyze more deeply. And so I can now go back as a human. I can analyze this. I mean, I can actually use an AI system as well. But for the sake of this particular paper, of course I wanted to do this as a human. I did it also with the algorithm. But I also wanted to understand, of course, myself to validate predictions. And you can see what this model came up with is what we call a lactam molecule, which actually, interestingly, if you go into the history, this is the basis for nylon. Of course, we've known nylon for a long time, but so this shows the model has actually figured out that to build a polymer out of this platform, you got to create these hetero aromatic amides, which is what this lactam molecule is. So it has a ketone group and an amide group. And in different versions of nylon, you're going to have different configurations. The one that the model has come up here is a new one, because I'm actually forcing the model to discover a chemical structure that does not exist at all in the literature. So I'm forcing the model to discover something new, and it came up with something totally new. And of course, it can understand that and reason over it. And so, in fact, there's a pretty good likelihood that you can actually make a polymer out of this, which is fascinating. And so I won't go into more detail on this particular example here, but we can extend this also to include physics. So, in other words, multi agent models can include physical engines as well, either experiments or maybe a quantum mechanical simulation. So in this case here, it's a paper we published recently, or it's actually an archive, but it's in review. In that paper, we focus really heavily in protein discovery. And we have a range of different agents here, some AI based, some physics based. So they have the ability to generate new data on the fly when needed. And I think that's really the exciting part, is you can truly integrate, like I said at the very outset, data driven modeling with physics based modeling and many other modalities of interaction. So now let's talk about graphs. Okay, so I know this is part of the main thing we want to talk about is a graph presentation. So I can also. So the model itself internally has a graph representation in itself. The LLM, if we call it LLM, it's not just language, it's numbers and figures and images and all these things. But. But let's say it's. It's called an LLM. We can sort of put that in a, in a larger framework of ontological knowledge representation as well. And so what we've done here is we use, as you mentioned in the introduction, Ravi. So in this particular case, we used a framework of AI systems to construct a knowledge graph. And the beautiful thing there is that we can automate this process. So unlike back in the old days, you know, 20 years ago, not ten years ago, we build olog representations using category theory manually on piece of paper. I had to know everything about the system already, took us an enormous amount of time. Now I can construct these graph representations automatically over a few days. So this still takes a lot of compute, but I can actually construct knowledge graphs, ontological knowledge graphs systematically through generative AI tools. And a way it works is that you have a body of knowledge, like in my case, I looked at 1000 people papers, and I train models to create representations of knowledge in the typical way we're doing it, in these graph structures like notes, concepts and connections between the concept. And there are a couple of steps involved. And you can read up on this in the paper, and how we actually produce graphs that are consistent, as consistent as possible in the way the notes and relationships are named. And that can be done by iterating through the generative process process, but it can also kind of be done using what we call deep node embeddings. And this is something really important, because if you, if you constructing these graphs from many different sources, you're going to have slightly different ways by which certain concepts are referred to. And so we have a step involved here that essentially unifies the nomenclature across different sources. And this can be done actually very well using deep language embedding technology methods. And so this allows us to kind of build graph models that are consistent and well connected, you know, across many different domains, many different sources, many different papers, and that can be repeated, obviously, and made even better. So there's sort of an iterative process involved here. I can then use these graphs to do quite interesting things. And so the first thing I can do, I can help a language model answer questions. So language models are not very good. Usually if you ask a question to a model, it's kind of a single shot answer, might be correct, but it might be wrong. And the degree to which these are correct or wrong depends on the model and the size of the model and the way we've trained it. But many times a model will struggle, actually. And so what you can do, you can use graph based reasoning to help answer better questions. So instead of just asking a question, letting the model answer it. You can say, here's my question, and here's, by the way, here's some concepts that I can derive from the graph that relate the question I'm asking to a sub graph or multiple subgraphs. And the model then includes its own understanding of the question you're asking, the graph representation you're giving, like which notes are related to the question you're asking. And this is something like a rack based, graph based reasoning strategy. And the answer then becomes much better, of course, because there's a relationship of concepts that the model might or might not understand in its own internal representation. So you get much better, much more nuanced questions, answer relationships. And you can do this, of course, as well in an agentic setting. So in an agent setting, you can expand on this reasoning ability even further. So instead of saying, I have a graph and I want an answer, you can say, I have a graph, I want an answer, but I want multiple AI adversarial agents to figure out the answer, and then they tell us. So a human user might not see what's going on behind the scenes. You're only going to get the final answer. But to get the answer, there is a complex reasoning step involved by which multiple agents reason over the results, compete, and finally figure out kind of what is the best answer, what's the actual answer to the question with all the maybe rationale behind it, and so on. And we've used this in a couple of different ways, including in predicting new behaviors of materials. So what you see on the right hand side is in studies, we have shown that we can actually use these type of reasoning engines to come up with hypothetical material studies like experiments, and a model can predict correctly what would happen, what this experiment would actually lead to. So it has a modern representation of behaviors. And in this particular case, we did this test in a sense that we had a, we've trained our model and build our knowledge graph, and then we use the paper that was published after this was all done. So the model had no idea this paper existed and no access to this. And so we had actually real experimental validation of the predicted behavior. And it does a really good job with this. So this is really exciting. So these are kind of glimpses, I think, of what we can do in the future, um, with these types of complex AI models, other things we can do with graphs. And so now we're getting into much more interesting things, actually. So let's say I'm interested in mycelium materials, which is a field of study that's pretty niche. It's very small, but kind of using mycelium root structures, living materials to make materials. And, and so that's a small field, but it's complex in a sense. It involves biology, it involves engineering, it involves material processing, it involves sustainability, conservations, and so on and so on. And, you know, we have worked on this in my lab in the last couple of years, and it's very challenging to make clever design suggestions because we have no model for these behaviors. So we then thought, you know, can we do this more elegantly and actually use genai techniques like the ones I've shown, but incorporate really sophisticated graph reasoning abilities? And so this is, what's the main part of that paper that I mentioned in the very beginning is really about building these graph reasoning strategies. So what we can do, I can have a corpus of graph representation of a large set of literature, and that might be literature in, you know, material science. And I have another smaller graph of a situation of knowledge or in a specialized field like mycelium biology. And what I can then do, I can merge these graphs together. And I'm seeing here that, yeah, these graphs both talk about overlapping concepts. And so when I merge them, I get a lot more connections. So now if I do pathfinding algorithms, so if I'm sampling paths in this graph, so I can say I'm interested in mycelium and sustainability, what are the connections between them? You know, I can find the shortest path, obviously, and it sort of tells a story of how sustainability in mycelium is connected. Right. And I can then use this path to answer a question like, let's say I wanted to develop a new research hypothesis, like a better question that I could ask, or maybe I want to predict the behavior of a system. This is extremely powerful because individually, we have knowledge about mycelium, we have knowledge about sustainability, but we haven't really figured out the connection with these graph forming algorithms. I can build these connections, and I can sample them, and I can then have llms or multimodal llms reason over those graphs. And so what's shown here, actually are these graphs. So I can kind of extract a sub graph about a particular path that I've sampled. And I can, I can just take them literally the way they come, or I can do additional graph processing. I can understand, okay, you know, if I have a graph like this, I can measure certain features of these graphs, like, you know, focusing on bridging centrality nodes. And I can tell, you know, from these which nodes are important for the purposes, like some nodes are really critical to understand the connection right between the two concepts. Other notes might be really important, but very poorly connected. And, you know, this gives me a lot of leg way to kind of dig deeper into research questions. Ravi, you have a question. I think you muted. Sorry, Ravi is muted. So I think we can probably wind up and then we can take, get to the. I just wanted to remind that you have five minutes to conclude. Yes. Yeah, so I'll be done. I'll quickly go through these examples and we have time, enough time for questions. Yeah, great. So we can reason all the graphs in pretty complex ways. I can also use isomorphic analysis. So if I have two graphs that are not connected. So you might say, well, it's great, but what if the two concepts are not connected? Like, if I have a graph on biological materials and another knowledge graph on Beethoven symphonies, there have maybe absolutely no overlapping nodes, because they talk about very different concepts. So what I can use here, of course, is isomorphic analysis, which is the kind of thing we've done with category theory back in the day. And I can then find analogies between them and also use graph extension mechanisms to sort of extend one of the graphs and ask the question, how would the other graph extend? And I can transfer knowledge and insights mechanisms from one domain to another. So that is something we're working on right now. I can also do joint vision and graph reasoning. So if I have a graph representation, I can reason over these graphs. If I have a multimodal AI model, I can actually give an image as well, like a painting, and I can reason over the information in the painting as well as the information in the graph, and I can make an answer. I can design a material. And so this is what we've done in this particular case. We have designed materials actually by reasoning over a famous painting by Kandinsky and a graph on mycelium and sustainability to ultimately come up with this particular design. And because this model is multimodal, I can actually not only make a visual representation of what this looks like, but also the model will tell me, as a scientist, engineer, what the different components are in there. So I can actually have a real understanding of what this really means. And it is consistent in a sense that I can understand that these predictions of what the different components and constituents are make sense. As a material scientist, I can understand that. And it can go very detailed, of course, in the chemistry. So one of the really amazing things about this is that when you ask a lot of genai or AI generally about designs, it really just interpolates around things it has already known, seen before. This technology allows you to go far outside and really be innovative and creative and discover new things. And these new things are very detailed. That's the other dimension to it. So not only are they new, they're very detailed. So I just highlighted a couple of things here. So we talk about specific chemical functionalization, very specific length scales, very specific mechanisms, including manufacturing processes. So this really sort of goes the whole way. The other thing we can do, and here is something that is not in the paper yet, but that's something you're working on right now, is I can build a random discovery engine. What I mean by that is I can take this graph and maybe I don't know what I want, maybe I want to explore. So I can actually use this graph based reasoning extremely effectively to create a very large number of research ideas or projected material behavior ideas by combining, again, things that the model has learned with high fidelity, like predicting chemical properties with a knowledge graph, which provides more contextual relationships between ideas and concepts and make predictions. And so again, you can look at one of these, I won't go into all the details here, but you can generate some extremely interesting ideas, which in a multi agent system, then you can evaluate what I've done right now, for now, at least for the presentation, actually, I put one of those in there and I looked into what's predicted. So this predicts a new type of way of combining DNA with heteroxylpatite in creating a biomaterial that has applications of use as supercapacitor by applying this patterning technology to creating mesoporous carbon. So that's something that is a very specific idea. So I looked into the literature and saw, you know, kind of wanted to see is has been done. And so to what I could find in the last day or so when I looked into this, it hasn't been done right. So there are, there's literature on DNA for biomimetrization, there's literature on, you know, creating supercapacitors, obviously from nanomaterials, but those two ideas have not been combined. And so this specific idea, for example, gives me, the model has learned, obviously through graph reasoning, build a path, connect these ideas and say something intelligent about how this can be done. So it's not just an idea. It goes into great detail on how this can be manufactured, what properties you'd have. And you can do many of these. Again, you can do 10,000, 10,0000 I did, I think something like 100,000 or so. Over a few days, and which can then be the basis for additional evaluation. So, I mean, this is sort of a little advertisement for my, my courses. If you're interested in this, you know, some of you might be interested coming to MIT's. I teach two courses at MIT in the summer. One is on campus, this one on the left in June, and the other one's live, virtual, and they talk about some of the things I've presented here. And it's, you know, a fun way to learn and be involved, if you, if you're interested. So happy to discuss more of you, if you're interested in coming so with that, thank you very much. And hopefully we have some time for questions. Thank you, Marcus. That was a lot of material that you provided, and I'm sure there are lots of questions that we need to kind of figure out because we have maybe about ten minutes or so at most. Now, you're going to Ravi, how are you going to do the moderation around here? Yes, I have myself three, four questions, but I will hold them back and ask you the most important one to begin with and then request John Soa. Mike DeBalis and others are already asking questions on the chat, so I will go through some of the chat questions. My biggest question is, your technique is a ideal candidate, and I think you touched on it at the last minute, is for integrating bio and material development so the material that can coexist in a living system most effectively can use all the things that you described to us today. So that, to me, is a low hanging fruit. I hope you are working on that. Low hanging fruit and developing prosthetics, integrated medical devices and things that can benefit humanity immensely. Second thing is, I think you opened our eyes to visual dynamic language of the future, starting with knowledge, graphs and reasoning that you did and obviously changed a lot of our notions about how to integrate logic and statistics in the process. Thank you so much. But I will open now and request Phil Jackson, if you have a quick one. I think John John Soha has his hand up, please. John John has like ten questions in the chat. John, kindly go ahead. Okay. One question that I have is about the difference between DNA and most of these other kinds of materials, and that is that DNA can just, one tiny difference can be an amazing, huge difference in terms of the results, whereas in most of these other materials, it's an average of a large, large number of particles. And when you have an average, just a small difference between one particle and another won't make a big difference. But when you get to DNA, just one tiny change can just reverse everything, right? Yeah, exactly. I mean, that's exactly the point. So this is why this type of modeling of hierarchical structures is really critical, because that's exactly right. You can't average out. And so building graph representations of the way these systems behave is absolutely essential. And you want to learn what's important, what's not important, in what context. Yeah, exactly. So that's the basis to all the modeling I showed today is exactly that. Yeah, I totally agree. And that's the really big fallacy of conventional modeling. A lot of times, whether it's in biology or engineering and science, other areas, is that we think we know and we ignore things because we don't see the importance of that in the next level up. And we say, yeah, let's just average it out. But actually, you can't do that, I think. Who else has a question here? But I have one question, Markus, in terms of you are generating this knowledge using the llms. That's one of. One way of generating the knowledge, I guess. The knowledge graphs. Yes, yes. So the knowledge graphs are created exactly. Using llms, essentially from the sources that we like, literature, for example. And we're using the capabilities of those to be able to create nodes and relationships. And they're then assembled. There's a couple of steps involved in how we, as a mechanics involved in how we do this, of course, and how we use the LLM and what kind of LLM and how they must be trained, and how we ultimately make the graph using embeddings and so on. But essentially, yes. And of course, you can add new knowledge to the graphs through special purpose AI, like an AI that understands how to fold a protein or how to make predictions, or you can, of course, add knowledge from physics simulation, or experiments. So the beautiful thing about graphs is, of course, you can always grow them. You can make them bigger. You can add new graphs, you can put contextual relationships between graphs through graph path analysis or isomorphic mappings. And there's a lot of different ways. So there's not a single way, I would say, you know, not, we don't have to get the graphs using M and Ms, but there has to be a step involved in how to extract knowledge, which is the connection between different features. And that's a big field to make it better, you know, I think there's a lot of room there to make this process better. Fantastic. Yeah. You have a quick question that. Because you. Thank you. I have looked at your work because we also have been working with David Spivak for a long time. My question is that once you generate the olog structures from the text, are you going back and making sure it follows all the rules of the conceptual frame olog provides manually or through evaluation from different perspectives? Yeah, great, great question. So in the work we've done in this paper that I just, up in the beginning, we didn't do a manual check, you know, but, but in smaller cases, we've done that. In the earlier work we've done the last few years, we've done like 1000 papers, one paper too. We can do that manually, but what we have done in the, so we're looking at large data formats and you can do it manually, but there are actually ways. This is what I talked about earlier a little bit, the mechanics by which we construct these knowledge gaps. And so one way is sort of you can, you can create an initial draft. This is what we do of notes and relationships. And then we, we provide a set of these and then we ask the model, okay, now look at all the ones you've made from maybe a set of text, you know, maybe the whole paper, maybe sets of papers, and try to unify the terminologies between them. And we do that step. So that already for that you give a seed graph into the LLM. Yes, exactly. Well, so I provide the, I provide the LLM with an example and it's all in the paper in detail, but. Yeah, and that's, you know, I say here's a text or sentence and this is how a graph would look like. And that's where you need to figure out, you know, what, what model you want to use. And that has a huge impact on that, of course. So, but you have to find the model that does a good job of this. Yeah, I think the more examples you give, the better. You can train models to do this. And I've also worked with some of those. Exactly. And then, but then there's also the embedding strategies. One of the things you can do, you can use lms, but it's very expensive. And even though we have long context lengths, there are a lot of limitations. But with embedding models, you can add on to this. So instead of just saying, here's all the text, I can use embedding vectors of notes, the edges, and I would like to know more about the details. I'll come. Yeah, absolutely. Yeah. Happy to talk. I mean, it's all in the, it's all on the paper, but happy to meet you. I saw the paper, but I also need more details. Absolutely. Yeah. Happy to read. Yeah, yeah. And I. And I released the code as well. I haven't released it yet, just because I have to clean it up. Okay. But it's all there. It's already on my GitHub, so they'll be available. You can also take a look at this. Yeah, that's great. I think we need to kind of come to a closure here. Marcus, thanks a lot. Thank you very much for this excellent talk. Also, I think you can, if it's possible, you can send your slides over so that we can put it on the web along with your presentation. It's all recorded, if you don't mind. So you can either send it to me or Ravi, and then we'll put it on the web. And I think actually, your talk kind of must have given lots of people lots of questions to ponder. So they might come back to you later on. They can read your papers and see how this thing can be synthesized at some point. So, any last word, Ravi or Ken, before we just saying that, we generally put abstracts from the chat to the session page, and Ken does that wonderfully. So. But in this particular case, I also had the idea that if we could send you the chat questions, because they are very thought provoking and very useful, maybe they can be useful for your work as well. Yes. Yeah, I'm looking at it right now and, yeah, be great if you can send them, please, and, you know, and we can continue the discussion. I'm sorry I went to the end. I would like to request our organizers, if they can have you come back again this year itself. Yeah, no, I'll be happy to do that. And that may be the next event. You know, I can. I don't have to show a lot. I can just maybe remind everybody what I show, maybe have some updates, but then have most of the time for discussion, because I would love to hear. Absolutely. Wonderful. Thank you so much. Be open to that. I would love to do that. Absolutely. I think that's a great idea. So, anyway, thanks a lot. So, again, you're going to bring a meeting to a close, and. Thanks. We're going to hang around after some time. Ken, you have a minute? I will be. But I'd like to remind everyone that next week we're having our first synthesis session. Oh, yes. And so bring your ideas to try to synthesize all the enormous amount of material that has been covered in our summit so far. And with that, I wish to adjourn the meeting once again. Thanks, Marcus. Thank you, Ken. Really great talk. Thank you so much. Thanks, everyone. Well done. Thank you. Thank you. Bye. So, let's see.